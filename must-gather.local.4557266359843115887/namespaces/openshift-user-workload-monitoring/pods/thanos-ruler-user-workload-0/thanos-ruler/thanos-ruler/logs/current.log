2022-05-24T18:38:42.847503469Z ts=2022-05-24T18:38:42.847441632Z caller=log.go:168 component=tsdb level=info msg="Replaying on-disk memory mappable chunks if any"
2022-05-24T18:38:42.847503469Z ts=2022-05-24T18:38:42.8474767Z caller=log.go:168 component=tsdb level=info msg="On-disk memory mappable chunks replay completed" duration=1.516µs
2022-05-24T18:38:42.847503469Z ts=2022-05-24T18:38:42.847484752Z caller=log.go:168 component=tsdb level=info msg="Replaying WAL, this may take a while"
2022-05-24T18:38:42.847742300Z ts=2022-05-24T18:38:42.84772247Z caller=log.go:168 component=tsdb level=info msg="WAL segment loaded" segment=0 maxSegment=3
2022-05-24T18:38:42.848356048Z ts=2022-05-24T18:38:42.848335243Z caller=log.go:168 component=tsdb level=info msg="WAL segment loaded" segment=1 maxSegment=3
2022-05-24T18:38:42.848539216Z ts=2022-05-24T18:38:42.848493567Z caller=log.go:168 component=tsdb level=info msg="WAL segment loaded" segment=2 maxSegment=3
2022-05-24T18:38:42.849558185Z ts=2022-05-24T18:38:42.849200415Z caller=log.go:168 component=tsdb level=info msg="WAL segment loaded" segment=3 maxSegment=3
2022-05-24T18:38:42.849558185Z ts=2022-05-24T18:38:42.849262524Z caller=log.go:168 component=tsdb level=info msg="WAL replay completed" checkpoint_replay_duration=40.371µs wal_replay_duration=1.725698ms total_replay_duration=1.77922ms
2022-05-24T18:38:42.850467608Z level=info ts=2022-05-24T18:38:42.850440367Z caller=rule.go:658 msg="a leftover lockfile found and removed"
2022-05-24T18:38:42.851224302Z level=info ts=2022-05-24T18:38:42.851180952Z caller=options.go:31 component=rules protocol=gRPC msg="enabling server side TLS"
2022-05-24T18:38:42.851283202Z level=info ts=2022-05-24T18:38:42.851261576Z caller=options.go:61 component=rules protocol=gRPC msg="server TLS client verification enabled"
2022-05-24T18:38:42.852548245Z level=info ts=2022-05-24T18:38:42.852511322Z caller=rule.go:640 component=rules msg="no supported bucket was configured, uploads will be disabled"
2022-05-24T18:38:42.852568936Z level=info ts=2022-05-24T18:38:42.852534319Z caller=rule.go:643 component=rules msg="starting rule node"
2022-05-24T18:38:42.852738057Z level=info ts=2022-05-24T18:38:42.852698585Z caller=rule.go:811 component=rules msg="reload rule files" numFiles=1
2022-05-24T18:38:42.852965203Z level=info ts=2022-05-24T18:38:42.852901867Z caller=intrumentation.go:60 component=rules msg="changing probe status" status=healthy
2022-05-24T18:38:42.852977209Z level=info ts=2022-05-24T18:38:42.852965951Z caller=http.go:63 component=rules service=http/server component=rule msg="listening for requests and metrics" address=localhost:10902
2022-05-24T18:38:42.853277617Z level=info ts=2022-05-24T18:38:42.8531476Z caller=intrumentation.go:48 component=rules msg="changing probe status" status=ready
2022-05-24T18:38:42.853645574Z level=info ts=2022-05-24T18:38:42.853615862Z caller=grpc.go:127 component=rules service=gRPC/server component=rule msg="listening for serving gRPC" address=0.0.0.0:10901
2022-05-24T18:38:42.853678468Z ts=2022-05-24T18:38:42.853649322Z caller=log.go:168 component=rules service=http/server component=rule level=info msg="TLS is disabled." http2=false
2022-05-24T18:41:17.845867417Z level=info ts=2022-05-24T18:41:17.845815693Z caller=rule.go:811 component=rules msg="reload rule files" numFiles=2
2022-05-24T18:41:18.824638787Z level=error ts=2022-05-24T18:41:18.824563368Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: receive series from Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unknown desc = query Prometheus: request failed with code 503 Service Unavailable; msg Service Unavailable\"}\n" query="sum by(namespace) (ceph_mds_metadata{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"} == 1) < 2"
2022-05-24T18:41:18.824722197Z ts=2022-05-24T18:41:18.824621932Z caller=log.go:168 component=rules level=warn group=ceph-mds-status msg="Evaluating rule failed" rule="alert: CephMdsMissingReplicas\nexpr: sum by(namespace) (ceph_mds_metadata{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"}\n  == 1) < 2\nfor: 5m\nlabels:\n  namespace: openshift-storage\n  severity: warning\nannotations:\n  description: Minimum required replicas for storage metadata service not available.\n    Might affect the working of storage cluster.\n  message: Insufficient replicas for storage metadata service.\n  severity_level: warning\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:41:19.158377357Z level=error ts=2022-05-24T18:41:19.158324422Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: receive series from Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unknown desc = query Prometheus: request failed with code 503 Service Unavailable; msg Service Unavailable\"}\n" query="label_replace((up{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"} == 0 or absent(up{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"})), \"namespace\", \"openshift-storage\", \"\", \"\")"
2022-05-24T18:41:19.158480328Z ts=2022-05-24T18:41:19.158369127Z caller=log.go:168 component=rules level=warn group=ceph-mgr-status msg="Evaluating rule failed" rule="alert: CephMgrIsAbsent\nexpr: label_replace((up{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"} == 0 or\n  absent(up{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"})), \"namespace\", \"openshift-storage\",\n  \"\", \"\")\nfor: 5m\nlabels:\n  namespace: openshift-storage\n  severity: critical\nannotations:\n  description: Ceph Manager has disappeared from Prometheus target discovery.\n  message: Storage metrics collector service not available anymore.\n  severity_level: critical\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:41:19.161535042Z level=error ts=2022-05-24T18:41:19.16149277Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: receive series from Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unknown desc = query Prometheus: request failed with code 503 Service Unavailable; msg Service Unavailable\"}\n" query="sum by(namespace) (kube_deployment_spec_replicas{deployment=~\"rook-ceph-mgr-.*\",namespace=\"openshift-storage\"}) < 1"
2022-05-24T18:41:19.161636914Z ts=2022-05-24T18:41:19.161527929Z caller=log.go:168 component=rules level=warn group=ceph-mgr-status msg="Evaluating rule failed" rule="alert: CephMgrIsMissingReplicas\nexpr: sum by(namespace) (kube_deployment_spec_replicas{deployment=~\"rook-ceph-mgr-.*\",namespace=\"openshift-storage\"})\n  < 1\nfor: 5m\nlabels:\n  namespace: openshift-storage\n  severity: warning\nannotations:\n  description: Ceph Manager is missing replicas.\n  message: Storage metrics collector service doesn't have required no of replicas.\n  severity_level: warning\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:41:20.852474548Z level=error ts=2022-05-24T18:41:20.852418862Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: receive series from Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unknown desc = query Prometheus: request failed with code 503 Service Unavailable; msg Service Unavailable\"}\n" query="(kubelet_volume_stats_used_bytes{namespace=\"openshift-storage\"} * on(namespace, persistentvolumeclaim) group_left(storageclass, provisioner) (kube_persistentvolumeclaim_info{namespace=\"openshift-storage\"} * on(storageclass) group_left(provisioner) kube_storageclass_info{namespace=\"openshift-storage\",provisioner=~\"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)\"})) / (kubelet_volume_stats_capacity_bytes{namespace=\"openshift-storage\"} * on(namespace, persistentvolumeclaim) group_left(storageclass, provisioner) (kube_persistentvolumeclaim_info{namespace=\"openshift-storage\"} * on(storageclass) group_left(provisioner) kube_storageclass_info{namespace=\"openshift-storage\",provisioner=~\"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)\"})) > 0.75"
2022-05-24T18:41:20.852641585Z ts=2022-05-24T18:41:20.852466072Z caller=log.go:168 component=rules level=warn group=persistent-volume-alert.rules msg="Evaluating rule failed" rule="alert: PersistentVolumeUsageNearFull\nexpr: (kubelet_volume_stats_used_bytes{namespace=\"openshift-storage\"} * on(namespace,\n  persistentvolumeclaim) group_left(storageclass, provisioner) (kube_persistentvolumeclaim_info{namespace=\"openshift-storage\"}\n  * on(storageclass) group_left(provisioner) kube_storageclass_info{namespace=\"openshift-storage\",provisioner=~\"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)\"}))\n  / (kubelet_volume_stats_capacity_bytes{namespace=\"openshift-storage\"} * on(namespace,\n  persistentvolumeclaim) group_left(storageclass, provisioner) (kube_persistentvolumeclaim_info{namespace=\"openshift-storage\"}\n  * on(storageclass) group_left(provisioner) kube_storageclass_info{namespace=\"openshift-storage\",provisioner=~\"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)\"}))\n  > 0.75\nfor: 5s\nlabels:\n  namespace: openshift-storage\n  severity: warning\nannotations:\n  description: PVC {{ $labels.persistentvolumeclaim }} utilization has crossed 75%.\n    Free up some space or expand the PVC.\n  message: PVC {{ $labels.persistentvolumeclaim }} is nearing full. Data deletion\n    or PVC expansion is required.\n  severity_level: warning\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:41:20.865754713Z level=error ts=2022-05-24T18:41:20.865685071Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: receive series from Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unknown desc = query Prometheus: request failed with code 503 Service Unavailable; msg Service Unavailable\"}\n" query="(kubelet_volume_stats_used_bytes{namespace=\"openshift-storage\"} * on(namespace, persistentvolumeclaim) group_left(storageclass, provisioner) (kube_persistentvolumeclaim_info{namespace=\"openshift-storage\"} * on(storageclass) group_left(provisioner) kube_storageclass_info{namespace=\"openshift-storage\",provisioner=~\"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)\"})) / (kubelet_volume_stats_capacity_bytes{namespace=\"openshift-storage\"} * on(namespace, persistentvolumeclaim) group_left(storageclass, provisioner) (kube_persistentvolumeclaim_info{namespace=\"openshift-storage\"} * on(storageclass) group_left(provisioner) kube_storageclass_info{namespace=\"openshift-storage\",provisioner=~\"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)\"})) > 0.85"
2022-05-24T18:41:20.865928662Z ts=2022-05-24T18:41:20.865760892Z caller=log.go:168 component=rules level=warn group=persistent-volume-alert.rules msg="Evaluating rule failed" rule="alert: PersistentVolumeUsageCritical\nexpr: (kubelet_volume_stats_used_bytes{namespace=\"openshift-storage\"} * on(namespace,\n  persistentvolumeclaim) group_left(storageclass, provisioner) (kube_persistentvolumeclaim_info{namespace=\"openshift-storage\"}\n  * on(storageclass) group_left(provisioner) kube_storageclass_info{namespace=\"openshift-storage\",provisioner=~\"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)\"}))\n  / (kubelet_volume_stats_capacity_bytes{namespace=\"openshift-storage\"} * on(namespace,\n  persistentvolumeclaim) group_left(storageclass, provisioner) (kube_persistentvolumeclaim_info{namespace=\"openshift-storage\"}\n  * on(storageclass) group_left(provisioner) kube_storageclass_info{namespace=\"openshift-storage\",provisioner=~\"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)\"}))\n  > 0.85\nfor: 5s\nlabels:\n  namespace: openshift-storage\n  severity: critical\nannotations:\n  description: PVC {{ $labels.persistentvolumeclaim }} utilization has crossed 85%.\n    Free up some space or expand the PVC immediately.\n  message: PVC {{ $labels.persistentvolumeclaim }} is critically full. Data deletion\n    or PVC expansion is required.\n  severity_level: error\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:41:20.938670018Z level=error ts=2022-05-24T18:41:20.938552296Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: receive series from Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unknown desc = query Prometheus: request failed with code 503 Service Unavailable; msg Service Unavailable\"}\n" query="(ceph_osd_metadata{namespace=\"openshift-storage\"} * on(ceph_daemon) group_right(device_class, hostname) (ceph_osd_stat_bytes_used{namespace=\"openshift-storage\"} / ceph_osd_stat_bytes{namespace=\"openshift-storage\"})) >= 0.8"
2022-05-24T18:41:20.938816301Z ts=2022-05-24T18:41:20.938659105Z caller=log.go:168 component=rules level=warn group=osd-alert.rules msg="Evaluating rule failed" rule="alert: CephOSDCriticallyFull\nexpr: (ceph_osd_metadata{namespace=\"openshift-storage\"} * on(ceph_daemon) group_right(device_class,\n  hostname) (ceph_osd_stat_bytes_used{namespace=\"openshift-storage\"} / ceph_osd_stat_bytes{namespace=\"openshift-storage\"}))\n  >= 0.8\nfor: 40s\nlabels:\n  namespace: openshift-storage\n  severity: critical\nannotations:\n  description: Utilization of storage device {{ $labels.ceph_daemon }} of device_class\n    type {{$labels.device_class}} has crossed 80% on host {{ $labels.hostname }}.\n    Immediately free up some space or add capacity of type {{$labels.device_class}}.\n  message: Back-end storage device is critically full.\n  severity_level: error\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:41:20.941360799Z level=error ts=2022-05-24T18:41:20.941323063Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: receive series from Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unknown desc = query Prometheus: request failed with code 503 Service Unavailable; msg Service Unavailable\"}\n" query="changes(ceph_osd_up{namespace=\"openshift-storage\"}[5m]) >= 10"
2022-05-24T18:41:20.941433081Z ts=2022-05-24T18:41:20.941348398Z caller=log.go:168 component=rules level=warn group=osd-alert.rules msg="Evaluating rule failed" rule="alert: CephOSDFlapping\nexpr: changes(ceph_osd_up{namespace=\"openshift-storage\"}[5m]) >= 10\nlabels:\n  namespace: openshift-storage\n  severity: critical\nannotations:\n  description: Storage daemon {{ $labels.ceph_daemon }} has restarted 5 times in last\n    5 minutes. Please check the pod events or ceph status to find out the cause.\n  message: Ceph storage osd flapping.\n  severity_level: error\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:41:20.944356977Z level=error ts=2022-05-24T18:41:20.944323694Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: receive series from Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unknown desc = query Prometheus: request failed with code 503 Service Unavailable; msg Service Unavailable\"}\n" query="(ceph_osd_metadata{namespace=\"openshift-storage\"} * on(ceph_daemon) group_right(device_class, hostname) (ceph_osd_stat_bytes_used{namespace=\"openshift-storage\"} / ceph_osd_stat_bytes{namespace=\"openshift-storage\"})) >= 0.75"
2022-05-24T18:41:20.944444780Z ts=2022-05-24T18:41:20.944353614Z caller=log.go:168 component=rules level=warn group=osd-alert.rules msg="Evaluating rule failed" rule="alert: CephOSDNearFull\nexpr: (ceph_osd_metadata{namespace=\"openshift-storage\"} * on(ceph_daemon) group_right(device_class,\n  hostname) (ceph_osd_stat_bytes_used{namespace=\"openshift-storage\"} / ceph_osd_stat_bytes{namespace=\"openshift-storage\"}))\n  >= 0.75\nfor: 40s\nlabels:\n  namespace: openshift-storage\n  severity: warning\nannotations:\n  description: Utilization of storage device {{ $labels.ceph_daemon }} of device_class\n    type {{$labels.device_class}} has crossed 75% on host {{ $labels.hostname }}.\n    Immediately free up some space or add capacity of type {{$labels.device_class}}.\n  message: Back-end storage device is nearing full.\n  severity_level: warning\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:41:20.947993843Z level=error ts=2022-05-24T18:41:20.947962999Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: receive series from Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unknown desc = query Prometheus: request failed with code 503 Service Unavailable; msg Service Unavailable\"}\n" query="label_replace((ceph_osd_in{namespace=\"openshift-storage\"} == 1 and ceph_osd_up{namespace=\"openshift-storage\"} == 0), \"disk\", \"$1\", \"ceph_daemon\", \"osd.(.*)\") + on(ceph_daemon) group_left(host, device) label_replace(ceph_disk_occupation{namespace=\"openshift-storage\"}, \"host\", \"$1\", \"exported_instance\", \"(.*)\")"
2022-05-24T18:41:20.948080349Z ts=2022-05-24T18:41:20.947988526Z caller=log.go:168 component=rules level=warn group=osd-alert.rules msg="Evaluating rule failed" rule="alert: CephOSDDiskNotResponding\nexpr: label_replace((ceph_osd_in{namespace=\"openshift-storage\"} == 1 and ceph_osd_up{namespace=\"openshift-storage\"}\n  == 0), \"disk\", \"$1\", \"ceph_daemon\", \"osd.(.*)\") + on(ceph_daemon) group_left(host,\n  device) label_replace(ceph_disk_occupation{namespace=\"openshift-storage\"}, \"host\",\n  \"$1\", \"exported_instance\", \"(.*)\")\nfor: 15m\nlabels:\n  namespace: openshift-storage\n  severity: critical\nannotations:\n  description: Disk device {{ $labels.device }} not responding, on host {{ $labels.host\n    }}.\n  message: Disk not responding\n  severity_level: error\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:41:20.951642354Z level=error ts=2022-05-24T18:41:20.951611557Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: receive series from Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unknown desc = query Prometheus: request failed with code 503 Service Unavailable; msg Service Unavailable\"}\n" query="label_replace((ceph_osd_in{namespace=\"openshift-storage\"} == 0 and ceph_osd_up{namespace=\"openshift-storage\"} == 0), \"disk\", \"$1\", \"ceph_daemon\", \"osd.(.*)\") + on(ceph_daemon) group_left(host, device) label_replace(ceph_disk_occupation{namespace=\"openshift-storage\"}, \"host\", \"$1\", \"exported_instance\", \"(.*)\")"
2022-05-24T18:41:20.951722004Z ts=2022-05-24T18:41:20.951637314Z caller=log.go:168 component=rules level=warn group=osd-alert.rules msg="Evaluating rule failed" rule="alert: CephOSDDiskUnavailable\nexpr: label_replace((ceph_osd_in{namespace=\"openshift-storage\"} == 0 and ceph_osd_up{namespace=\"openshift-storage\"}\n  == 0), \"disk\", \"$1\", \"ceph_daemon\", \"osd.(.*)\") + on(ceph_daemon) group_left(host,\n  device) label_replace(ceph_disk_occupation{namespace=\"openshift-storage\"}, \"host\",\n  \"$1\", \"exported_instance\", \"(.*)\")\nfor: 1m\nlabels:\n  namespace: openshift-storage\n  severity: critical\nannotations:\n  description: Disk device {{ $labels.device }} not accessible on host {{ $labels.host\n    }}.\n  message: Disk not accessible\n  severity_level: error\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:41:20.954101898Z level=error ts=2022-05-24T18:41:20.95407372Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: receive series from Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unknown desc = query Prometheus: request failed with code 503 Service Unavailable; msg Service Unavailable\"}\n" query="ceph_healthcheck_slow_ops{namespace=\"openshift-storage\"} > 0"
2022-05-24T18:41:20.954164056Z ts=2022-05-24T18:41:20.954097233Z caller=log.go:168 component=rules level=warn group=osd-alert.rules msg="Evaluating rule failed" rule="alert: CephOSDSlowOps\nexpr: ceph_healthcheck_slow_ops{namespace=\"openshift-storage\"} > 0\nfor: 30s\nlabels:\n  namespace: openshift-storage\n  severity: warning\nannotations:\n  description: '{{ $value }} Ceph OSD requests are taking too long to process. Please\n    check ceph status to find out the cause.'\n  message: OSD requests are taking too long to process.\n  severity_level: warning\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:41:20.956941941Z level=error ts=2022-05-24T18:41:20.956911934Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: receive series from Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unknown desc = query Prometheus: request failed with code 503 Service Unavailable; msg Service Unavailable\"}\n" query="ceph_pg_undersized{namespace=\"openshift-storage\"} > 0"
2022-05-24T18:41:20.956999490Z ts=2022-05-24T18:41:20.956936802Z caller=log.go:168 component=rules level=warn group=osd-alert.rules msg="Evaluating rule failed" rule="alert: CephDataRecoveryTakingTooLong\nexpr: ceph_pg_undersized{namespace=\"openshift-storage\"} > 0\nfor: 2h\nlabels:\n  namespace: openshift-storage\n  severity: warning\nannotations:\n  description: Data recovery has been active for too long. Contact Support.\n  message: Data recovery is slow\n  severity_level: warning\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:41:20.959097729Z level=error ts=2022-05-24T18:41:20.959060438Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: receive series from Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unknown desc = query Prometheus: request failed with code 503 Service Unavailable; msg Service Unavailable\"}\n" query="ceph_pg_inconsistent{namespace=\"openshift-storage\"} > 0"
2022-05-24T18:41:20.959175239Z ts=2022-05-24T18:41:20.959099775Z caller=log.go:168 component=rules level=warn group=osd-alert.rules msg="Evaluating rule failed" rule="alert: CephPGRepairTakingTooLong\nexpr: ceph_pg_inconsistent{namespace=\"openshift-storage\"} > 0\nfor: 1h\nlabels:\n  namespace: openshift-storage\n  severity: warning\nannotations:\n  description: Self heal operations taking too long. Contact Support.\n  message: Self heal problems detected\n  severity_level: warning\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:41:21.136608142Z level=error ts=2022-05-24T18:41:21.136545722Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: receive series from Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unknown desc = query Prometheus: request failed with code 503 Service Unavailable; msg Service Unavailable\"}\n" query="count by(namespace) (ceph_mon_quorum_status{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"} == 1) <= (floor(count by(namespace) (ceph_mon_metadata{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"}) / 2) + 1)"
2022-05-24T18:41:21.136693315Z ts=2022-05-24T18:41:21.13657858Z caller=log.go:168 component=rules level=warn group=quorum-alert.rules msg="Evaluating rule failed" rule="alert: CephMonQuorumAtRisk\nexpr: count by(namespace) (ceph_mon_quorum_status{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"}\n  == 1) <= (floor(count by(namespace) (ceph_mon_metadata{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"})\n  / 2) + 1)\nfor: 15m\nlabels:\n  namespace: openshift-storage\n  severity: critical\nannotations:\n  description: Storage cluster quorum is low. Contact Support.\n  message: Storage quorum at risk\n  severity_level: error\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:41:21.139457313Z level=error ts=2022-05-24T18:41:21.139423042Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: receive series from Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unknown desc = query Prometheus: request failed with code 503 Service Unavailable; msg Service Unavailable\"}\n" query="count by(namespace) (kube_pod_status_phase{namespace=\"openshift-storage\",phase=~\"Running|running\",pod=~\"rook-ceph-mon-.*\"} == 1) < 2"
2022-05-24T18:41:21.139519241Z ts=2022-05-24T18:41:21.139449745Z caller=log.go:168 component=rules level=warn group=quorum-alert.rules msg="Evaluating rule failed" rule="alert: CephMonQuorumLost\nexpr: count by(namespace) (kube_pod_status_phase{namespace=\"openshift-storage\",phase=~\"Running|running\",pod=~\"rook-ceph-mon-.*\"}\n  == 1) < 2\nfor: 5m\nlabels:\n  namespace: openshift-storage\n  severity: critical\nannotations:\n  description: Storage cluster quorum is lost. Contact Support.\n  message: Storage quorum is lost\n  severity_level: critical\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:41:21.142471955Z level=error ts=2022-05-24T18:41:21.142436828Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: receive series from Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unknown desc = query Prometheus: request failed with code 503 Service Unavailable; msg Service Unavailable\"}\n" query="(ceph_mon_metadata{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"} * on(ceph_daemon) group_left() (rate(ceph_mon_num_elections{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"}[5m]) * 60)) > 0.95"
2022-05-24T18:41:21.142559834Z ts=2022-05-24T18:41:21.142466548Z caller=log.go:168 component=rules level=warn group=quorum-alert.rules msg="Evaluating rule failed" rule="alert: CephMonHighNumberOfLeaderChanges\nexpr: (ceph_mon_metadata{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"} * on(ceph_daemon)\n  group_left() (rate(ceph_mon_num_elections{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"}[5m])\n  * 60)) > 0.95\nfor: 5m\nlabels:\n  namespace: openshift-storage\n  severity: warning\nannotations:\n  description: Ceph Monitor {{ $labels.ceph_daemon }} on host {{ $labels.hostname\n    }} has seen {{ $value | printf \"%.2f\" }} leader changes per minute recently.\n  message: Storage Cluster has seen many leader changes recently.\n  severity_level: warning\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:41:22.980529552Z level=error ts=2022-05-24T18:41:22.980485696Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: receive series from Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unknown desc = query Prometheus: request failed with code 503 Service Unavailable; msg Service Unavailable\"}\n" query="ceph_cluster_total_used_raw_bytes{namespace=\"openshift-storage\"} / ceph_cluster_total_bytes{namespace=\"openshift-storage\"} > 0.75"
2022-05-24T18:41:22.980634594Z ts=2022-05-24T18:41:22.980521032Z caller=log.go:168 component=rules level=warn group=cluster-utilization-alert.rules msg="Evaluating rule failed" rule="alert: CephClusterNearFull\nexpr: ceph_cluster_total_used_raw_bytes{namespace=\"openshift-storage\"} / ceph_cluster_total_bytes{namespace=\"openshift-storage\"}\n  > 0.75\nfor: 5s\nlabels:\n  namespace: openshift-storage\n  severity: warning\nannotations:\n  description: Storage cluster utilization has crossed 75% and will become read-only\n    at 85%. Free up some space or expand the storage cluster.\n  message: Storage cluster is nearing full. Data deletion or cluster expansion is\n    required.\n  severity_level: warning\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:41:22.983802085Z level=error ts=2022-05-24T18:41:22.983766196Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: receive series from Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unknown desc = query Prometheus: request failed with code 503 Service Unavailable; msg Service Unavailable\"}\n" query="ceph_cluster_total_used_raw_bytes{namespace=\"openshift-storage\"} / ceph_cluster_total_bytes{namespace=\"openshift-storage\"} > 0.8"
2022-05-24T18:41:22.983872324Z ts=2022-05-24T18:41:22.983795365Z caller=log.go:168 component=rules level=warn group=cluster-utilization-alert.rules msg="Evaluating rule failed" rule="alert: CephClusterCriticallyFull\nexpr: ceph_cluster_total_used_raw_bytes{namespace=\"openshift-storage\"} / ceph_cluster_total_bytes{namespace=\"openshift-storage\"}\n  > 0.8\nfor: 5s\nlabels:\n  namespace: openshift-storage\n  severity: critical\nannotations:\n  description: Storage cluster utilization has crossed 80% and will become read-only\n    at 85%. Free up some space or expand the storage cluster immediately.\n  message: Storage cluster is critically full and needs immediate data deletion or\n    cluster expansion.\n  severity_level: error\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:41:22.990850355Z level=error ts=2022-05-24T18:41:22.990814373Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: receive series from Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unknown desc = query Prometheus: request failed with code 503 Service Unavailable; msg Service Unavailable\"}\n" query="ceph_cluster_total_used_raw_bytes{namespace=\"openshift-storage\"} / ceph_cluster_total_bytes{namespace=\"openshift-storage\"} >= 0.85"
2022-05-24T18:41:22.990911389Z ts=2022-05-24T18:41:22.990844033Z caller=log.go:168 component=rules level=warn group=cluster-utilization-alert.rules msg="Evaluating rule failed" rule="alert: CephClusterReadOnly\nexpr: ceph_cluster_total_used_raw_bytes{namespace=\"openshift-storage\"} / ceph_cluster_total_bytes{namespace=\"openshift-storage\"}\n  >= 0.85\nlabels:\n  namespace: openshift-storage\n  severity: critical\nannotations:\n  description: Storage cluster utilization has crossed 85% and will become read-only\n    now. Free up some space or expand the storage cluster immediately.\n  message: Storage cluster is read-only now and needs immediate data deletion or cluster\n    expansion.\n  severity_level: error\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:41:25.692939693Z level=error ts=2022-05-24T18:41:25.692891708Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: receive series from Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unknown desc = query Prometheus: request failed with code 503 Service Unavailable; msg Service Unavailable\"}\n" query="cluster:ceph_node_down:join_kube{namespace=\"openshift-storage\"} == 0"
2022-05-24T18:41:25.693006292Z ts=2022-05-24T18:41:25.692926774Z caller=log.go:168 component=rules level=warn group=ceph-node-alert.rules msg="Evaluating rule failed" rule="alert: CephNodeDown\nexpr: cluster:ceph_node_down:join_kube{namespace=\"openshift-storage\"} == 0\nfor: 30s\nlabels:\n  namespace: openshift-storage\n  severity: critical\nannotations:\n  description: Storage node {{ $labels.node }} went down. Please check the node immediately.\n  message: Storage node {{ $labels.node }} went down\n  severity_level: error\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:41:27.985432617Z level=error ts=2022-05-24T18:41:27.985388323Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: receive series from Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unknown desc = query Prometheus: request failed with code 503 Service Unavailable; msg Service Unavailable\"}\n" query="count(ceph_osd_metadata{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"})"
2022-05-24T18:41:27.985487144Z ts=2022-05-24T18:41:27.985423898Z caller=log.go:168 component=rules level=warn group=telemeter.rules msg="Evaluating rule failed" rule="record: job:ceph_osd_metadata:count\nexpr: count(ceph_osd_metadata{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"})\nlabels:\n  namespace: openshift-storage\n" err="no query API server reachable"
2022-05-24T18:41:27.988661746Z level=error ts=2022-05-24T18:41:27.988616063Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: receive series from Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unknown desc = query Prometheus: request failed with code 503 Service Unavailable; msg Service Unavailable\"}\n" query="count(kube_persistentvolume_info{namespace=\"openshift-storage\"} * on(storageclass) group_left(provisioner) kube_storageclass_info{namespace=\"openshift-storage\",provisioner=~\"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)\"})"
2022-05-24T18:41:27.988711669Z ts=2022-05-24T18:41:27.988649867Z caller=log.go:168 component=rules level=warn group=telemeter.rules msg="Evaluating rule failed" rule="record: job:kube_pv:count\nexpr: count(kube_persistentvolume_info{namespace=\"openshift-storage\"} * on(storageclass)\n  group_left(provisioner) kube_storageclass_info{namespace=\"openshift-storage\",provisioner=~\"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)\"})\nlabels:\n  namespace: openshift-storage\n" err="no query API server reachable"
2022-05-24T18:41:27.991632282Z level=error ts=2022-05-24T18:41:27.991597892Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: receive series from Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unknown desc = query Prometheus: request failed with code 503 Service Unavailable; msg Service Unavailable\"}\n" query="sum(ceph_pool_rd{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"} + ceph_pool_wr{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"})"
2022-05-24T18:41:27.991679471Z ts=2022-05-24T18:41:27.991627597Z caller=log.go:168 component=rules level=warn group=telemeter.rules msg="Evaluating rule failed" rule="record: job:ceph_pools_iops:total\nexpr: sum(ceph_pool_rd{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"} + ceph_pool_wr{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"})\nlabels:\n  namespace: openshift-storage\n" err="no query API server reachable"
2022-05-24T18:41:27.994923581Z level=error ts=2022-05-24T18:41:27.994890651Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: receive series from Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unknown desc = query Prometheus: request failed with code 503 Service Unavailable; msg Service Unavailable\"}\n" query="sum(ceph_pool_rd_bytes{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"} + ceph_pool_wr_bytes{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"})"
2022-05-24T18:41:27.994976246Z ts=2022-05-24T18:41:27.994919258Z caller=log.go:168 component=rules level=warn group=telemeter.rules msg="Evaluating rule failed" rule="record: job:ceph_pools_iops_bytes:total\nexpr: sum(ceph_pool_rd_bytes{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"} +\n  ceph_pool_wr_bytes{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"})\nlabels:\n  namespace: openshift-storage\n" err="no query API server reachable"
2022-05-24T18:41:27.999218477Z level=error ts=2022-05-24T18:41:27.999184217Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: receive series from Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unknown desc = query Prometheus: request failed with code 503 Service Unavailable; msg Service Unavailable\"}\n" query="count(count by(ceph_version) (ceph_mon_metadata{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"} or ceph_osd_metadata{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"} or ceph_rgw_metadata{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"} or ceph_mds_metadata{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"} or ceph_mgr_metadata{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"}))"
2022-05-24T18:41:27.999288870Z ts=2022-05-24T18:41:27.999214409Z caller=log.go:168 component=rules level=warn group=telemeter.rules msg="Evaluating rule failed" rule="record: job:ceph_versions_running:count\nexpr: count(count by(ceph_version) (ceph_mon_metadata{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"}\n  or ceph_osd_metadata{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"} or ceph_rgw_metadata{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"}\n  or ceph_mds_metadata{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"} or ceph_mgr_metadata{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"}))\nlabels:\n  namespace: openshift-storage\n" err="no query API server reachable"
2022-05-24T18:41:29.308522795Z level=error ts=2022-05-24T18:41:29.308479173Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: receive series from Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unknown desc = query Prometheus: request failed with code 503 Service Unavailable; msg Service Unavailable\"}\n" query="(ceph_pool_stored_raw{namespace=\"openshift-storage\"} * on(pool_id) group_left(name) ceph_pool_metadata{namespace=\"openshift-storage\"}) / ((ceph_pool_quota_bytes{namespace=\"openshift-storage\"} * on(pool_id) group_left(name) ceph_pool_metadata{namespace=\"openshift-storage\"}) > 0) > 0.7"
2022-05-24T18:41:29.308638888Z ts=2022-05-24T18:41:29.308510056Z caller=log.go:168 component=rules level=warn group=pool-quota.rules msg="Evaluating rule failed" rule="alert: CephPoolQuotaBytesNearExhaustion\nexpr: (ceph_pool_stored_raw{namespace=\"openshift-storage\"} * on(pool_id) group_left(name)\n  ceph_pool_metadata{namespace=\"openshift-storage\"}) / ((ceph_pool_quota_bytes{namespace=\"openshift-storage\"}\n  * on(pool_id) group_left(name) ceph_pool_metadata{namespace=\"openshift-storage\"})\n  > 0) > 0.7\nfor: 1m\nlabels:\n  namespace: openshift-storage\n  severity: warning\nannotations:\n  description: Storage pool {{ $labels.name }} quota usage has crossed 70%.\n  message: Storage pool quota(bytes) is near exhaustion.\n  severity_level: warning\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:41:29.312796346Z level=error ts=2022-05-24T18:41:29.312757052Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: receive series from Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unknown desc = query Prometheus: request failed with code 503 Service Unavailable; msg Service Unavailable\"}\n" query="(ceph_pool_stored_raw{namespace=\"openshift-storage\"} * on(pool_id) group_left(name) ceph_pool_metadata{namespace=\"openshift-storage\"}) / ((ceph_pool_quota_bytes{namespace=\"openshift-storage\"} * on(pool_id) group_left(name) ceph_pool_metadata{namespace=\"openshift-storage\"}) > 0) > 0.9"
2022-05-24T18:41:29.312877624Z ts=2022-05-24T18:41:29.312787175Z caller=log.go:168 component=rules level=warn group=pool-quota.rules msg="Evaluating rule failed" rule="alert: CephPoolQuotaBytesCriticallyExhausted\nexpr: (ceph_pool_stored_raw{namespace=\"openshift-storage\"} * on(pool_id) group_left(name)\n  ceph_pool_metadata{namespace=\"openshift-storage\"}) / ((ceph_pool_quota_bytes{namespace=\"openshift-storage\"}\n  * on(pool_id) group_left(name) ceph_pool_metadata{namespace=\"openshift-storage\"})\n  > 0) > 0.9\nfor: 1m\nlabels:\n  namespace: openshift-storage\n  severity: critical\nannotations:\n  description: Storage pool {{ $labels.name }} quota usage has crossed 90%.\n  message: Storage pool quota(bytes) is critically exhausted.\n  severity_level: critical\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:41:30.146721450Z level=error ts=2022-05-24T18:41:30.14667226Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: receive series from Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unknown desc = query Prometheus: request failed with code 503 Service Unavailable; msg Service Unavailable\"}\n" query="ceph_health_status{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"} > 1"
2022-05-24T18:41:30.146798583Z ts=2022-05-24T18:41:30.146709816Z caller=log.go:168 component=rules level=warn group=cluster-state-alert.rules msg="Evaluating rule failed" rule="alert: CephClusterErrorState\nexpr: ceph_health_status{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"} > 1\nfor: 10m\nlabels:\n  namespace: openshift-storage\n  severity: critical\nannotations:\n  description: Storage cluster is in error state for more than 10m.\n  message: Storage cluster is in error state\n  severity_level: error\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:41:30.149434779Z level=error ts=2022-05-24T18:41:30.14939705Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: receive series from Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unknown desc = query Prometheus: request failed with code 503 Service Unavailable; msg Service Unavailable\"}\n" query="ceph_health_status{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"} == 1"
2022-05-24T18:41:30.149492649Z ts=2022-05-24T18:41:30.149423771Z caller=log.go:168 component=rules level=warn group=cluster-state-alert.rules msg="Evaluating rule failed" rule="alert: CephClusterWarningState\nexpr: ceph_health_status{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"} == 1\nfor: 15m\nlabels:\n  namespace: openshift-storage\n  severity: warning\nannotations:\n  description: Storage cluster is in warning state for more than 15m.\n  message: Storage cluster is in degraded state\n  severity_level: warning\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:41:30.151745362Z level=error ts=2022-05-24T18:41:30.151710195Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: receive series from Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unknown desc = query Prometheus: request failed with code 503 Service Unavailable; msg Service Unavailable\"}\n" query="count by(ceph_version, namespace) (count by(ceph_version, namespace) (ceph_osd_metadata{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"})) > 1"
2022-05-24T18:41:30.151808155Z ts=2022-05-24T18:41:30.151737422Z caller=log.go:168 component=rules level=warn group=cluster-state-alert.rules msg="Evaluating rule failed" rule="alert: CephOSDVersionMismatch\nexpr: count by(ceph_version, namespace) (count by(ceph_version, namespace) (ceph_osd_metadata{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"}))\n  > 1\nfor: 10m\nlabels:\n  namespace: openshift-storage\n  severity: warning\nannotations:\n  description: There are {{ $value }} different versions of Ceph OSD components running.\n  message: There are multiple versions of storage services running.\n  severity_level: warning\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:41:30.154335989Z level=error ts=2022-05-24T18:41:30.154301895Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: receive series from Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unknown desc = query Prometheus: request failed with code 503 Service Unavailable; msg Service Unavailable\"}\n" query="count(count by(ceph_version) (ceph_mon_metadata{ceph_version!=\"\",job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"})) > 1"
2022-05-24T18:41:30.154408393Z ts=2022-05-24T18:41:30.154327122Z caller=log.go:168 component=rules level=warn group=cluster-state-alert.rules msg="Evaluating rule failed" rule="alert: CephMonVersionMismatch\nexpr: count(count by(ceph_version) (ceph_mon_metadata{ceph_version!=\"\",job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"}))\n  > 1\nfor: 10m\nlabels:\n  namespace: openshift-storage\n  severity: warning\nannotations:\n  description: There are {{ $value }} different versions of Ceph Mon components running.\n  message: There are multiple versions of storage services running.\n  severity_level: warning\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:41:30.476901218Z level=error ts=2022-05-24T18:41:30.47685967Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: receive series from Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unknown desc = query Prometheus: request failed with code 503 Service Unavailable; msg Service Unavailable\"}\n" query="kube_node_status_condition{condition=\"Ready\",job=\"kube-state-metrics\",namespace=\"openshift-storage\",status=\"true\"} * on(node) group_right() max by(node, namespace) (label_replace(ceph_disk_occupation{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"}, \"node\", \"$1\", \"exported_instance\", \"(.*)\"))"
2022-05-24T18:41:30.476966655Z ts=2022-05-24T18:41:30.476891304Z caller=log.go:168 component=rules level=warn group=ceph.rules msg="Evaluating rule failed" rule="record: cluster:ceph_node_down:join_kube\nexpr: kube_node_status_condition{condition=\"Ready\",job=\"kube-state-metrics\",namespace=\"openshift-storage\",status=\"true\"}\n  * on(node) group_right() max by(node, namespace) (label_replace(ceph_disk_occupation{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"},\n  \"node\", \"$1\", \"exported_instance\", \"(.*)\"))\nlabels:\n  namespace: openshift-storage\n" err="no query API server reachable"
2022-05-24T18:41:30.485241249Z level=error ts=2022-05-24T18:41:30.48520195Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: receive series from Addr: 10.130.2.11:10901 LabelSets:  Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unknown desc = query Prometheus: request failed with code 503 Service Unavailable; msg Service Unavailable\"}\n" query="avg(topk by(ceph_daemon) (1, label_replace(label_replace(ceph_disk_occupation{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"}, \"instance\", \"$1\", \"exported_instance\", \"(.*)\"), \"device\", \"$1\", \"device\", \"/dev/(.*)\")) * on(instance, device) group_right(ceph_daemon) topk by(instance, device) (1, (irate(node_disk_read_time_seconds_total{namespace=\"openshift-storage\"}[1m]) + irate(node_disk_write_time_seconds_total{namespace=\"openshift-storage\"}[1m]) / (clamp_min(irate(node_disk_reads_completed_total{namespace=\"openshift-storage\"}[1m]), 1) + irate(node_disk_writes_completed_total{namespace=\"openshift-storage\"}[1m])))))"
2022-05-24T18:41:30.485343925Z ts=2022-05-24T18:41:30.485232276Z caller=log.go:168 component=rules level=warn group=ceph.rules msg="Evaluating rule failed" rule="record: cluster:ceph_disk_latency:join_ceph_node_disk_irate1m\nexpr: avg(topk by(ceph_daemon) (1, label_replace(label_replace(ceph_disk_occupation{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"},\n  \"instance\", \"$1\", \"exported_instance\", \"(.*)\"), \"device\", \"$1\", \"device\", \"/dev/(.*)\"))\n  * on(instance, device) group_right(ceph_daemon) topk by(instance, device) (1, (irate(node_disk_read_time_seconds_total{namespace=\"openshift-storage\"}[1m])\n  + irate(node_disk_write_time_seconds_total{namespace=\"openshift-storage\"}[1m]) /\n  (clamp_min(irate(node_disk_reads_completed_total{namespace=\"openshift-storage\"}[1m]),\n  1) + irate(node_disk_writes_completed_total{namespace=\"openshift-storage\"}[1m])))))\nlabels:\n  namespace: openshift-storage\n" err="no query API server reachable"
2022-05-24T18:42:30.145031518Z level=error ts=2022-05-24T18:42:30.144978619Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): fetch series for {prometheus=\\\"openshift-user-workload-monitoring/user-workload\\\", prometheus_replica=\\\"prometheus-user-workload-1\\\"} Addr: 10.129.2.36:10901 LabelSets: {prometheus=\\\"openshift-user-workload-monitoring/user-workload\\\", prometheus_replica=\\\"prometheus-user-workload-1\\\"} Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial tcp 10.129.2.36:10901: connect: connection refused\\\"\"}\n" query="ceph_health_status{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"} > 1"
2022-05-24T18:42:30.145110778Z ts=2022-05-24T18:42:30.14502222Z caller=log.go:168 component=rules level=warn group=cluster-state-alert.rules msg="Evaluating rule failed" rule="alert: CephClusterErrorState\nexpr: ceph_health_status{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"} > 1\nfor: 10m\nlabels:\n  namespace: openshift-storage\n  severity: critical\nannotations:\n  description: Storage cluster is in error state for more than 10m.\n  message: Storage cluster is in error state\n  severity_level: error\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:42:30.146252772Z level=error ts=2022-05-24T18:42:30.146217785Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): fetch series for {prometheus=\\\"openshift-user-workload-monitoring/user-workload\\\", prometheus_replica=\\\"prometheus-user-workload-1\\\"} Addr: 10.129.2.36:10901 LabelSets: {prometheus=\\\"openshift-user-workload-monitoring/user-workload\\\", prometheus_replica=\\\"prometheus-user-workload-1\\\"} Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial tcp 10.129.2.36:10901: connect: connection refused\\\"\"}\n" query="ceph_health_status{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"} == 1"
2022-05-24T18:42:30.146342063Z ts=2022-05-24T18:42:30.146253724Z caller=log.go:168 component=rules level=warn group=cluster-state-alert.rules msg="Evaluating rule failed" rule="alert: CephClusterWarningState\nexpr: ceph_health_status{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"} == 1\nfor: 15m\nlabels:\n  namespace: openshift-storage\n  severity: warning\nannotations:\n  description: Storage cluster is in warning state for more than 15m.\n  message: Storage cluster is in degraded state\n  severity_level: warning\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:42:30.147331738Z level=error ts=2022-05-24T18:42:30.147302031Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): fetch series for {prometheus=\\\"openshift-user-workload-monitoring/user-workload\\\", prometheus_replica=\\\"prometheus-user-workload-1\\\"} Addr: 10.129.2.36:10901 LabelSets: {prometheus=\\\"openshift-user-workload-monitoring/user-workload\\\", prometheus_replica=\\\"prometheus-user-workload-1\\\"} Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial tcp 10.129.2.36:10901: connect: connection refused\\\"\"}\n" query="count by(ceph_version, namespace) (count by(ceph_version, namespace) (ceph_osd_metadata{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"})) > 1"
2022-05-24T18:42:30.147412236Z ts=2022-05-24T18:42:30.147333526Z caller=log.go:168 component=rules level=warn group=cluster-state-alert.rules msg="Evaluating rule failed" rule="alert: CephOSDVersionMismatch\nexpr: count by(ceph_version, namespace) (count by(ceph_version, namespace) (ceph_osd_metadata{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"}))\n  > 1\nfor: 10m\nlabels:\n  namespace: openshift-storage\n  severity: warning\nannotations:\n  description: There are {{ $value }} different versions of Ceph OSD components running.\n  message: There are multiple versions of storage services running.\n  severity_level: warning\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:42:30.148463925Z level=error ts=2022-05-24T18:42:30.148434443Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): fetch series for {prometheus=\\\"openshift-user-workload-monitoring/user-workload\\\", prometheus_replica=\\\"prometheus-user-workload-1\\\"} Addr: 10.129.2.36:10901 LabelSets: {prometheus=\\\"openshift-user-workload-monitoring/user-workload\\\", prometheus_replica=\\\"prometheus-user-workload-1\\\"} Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial tcp 10.129.2.36:10901: connect: connection refused\\\"\"}\n" query="count(count by(ceph_version) (ceph_mon_metadata{ceph_version!=\"\",job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"})) > 1"
2022-05-24T18:42:30.148535887Z ts=2022-05-24T18:42:30.148461225Z caller=log.go:168 component=rules level=warn group=cluster-state-alert.rules msg="Evaluating rule failed" rule="alert: CephMonVersionMismatch\nexpr: count(count by(ceph_version) (ceph_mon_metadata{ceph_version!=\"\",job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"}))\n  > 1\nfor: 10m\nlabels:\n  namespace: openshift-storage\n  severity: warning\nannotations:\n  description: There are {{ $value }} different versions of Ceph Mon components running.\n  message: There are multiple versions of storage services running.\n  severity_level: warning\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:42:30.473152317Z level=error ts=2022-05-24T18:42:30.473104232Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): fetch series for {prometheus=\\\"openshift-user-workload-monitoring/user-workload\\\", prometheus_replica=\\\"prometheus-user-workload-1\\\"} Addr: 10.129.2.36:10901 LabelSets: {prometheus=\\\"openshift-user-workload-monitoring/user-workload\\\", prometheus_replica=\\\"prometheus-user-workload-1\\\"} Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial tcp 10.129.2.36:10901: connect: connection refused\\\"\"}\n" query="kube_node_status_condition{condition=\"Ready\",job=\"kube-state-metrics\",namespace=\"openshift-storage\",status=\"true\"} * on(node) group_right() max by(node, namespace) (label_replace(ceph_disk_occupation{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"}, \"node\", \"$1\", \"exported_instance\", \"(.*)\"))"
2022-05-24T18:42:30.473219375Z ts=2022-05-24T18:42:30.473142947Z caller=log.go:168 component=rules level=warn group=ceph.rules msg="Evaluating rule failed" rule="record: cluster:ceph_node_down:join_kube\nexpr: kube_node_status_condition{condition=\"Ready\",job=\"kube-state-metrics\",namespace=\"openshift-storage\",status=\"true\"}\n  * on(node) group_right() max by(node, namespace) (label_replace(ceph_disk_occupation{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"},\n  \"node\", \"$1\", \"exported_instance\", \"(.*)\"))\nlabels:\n  namespace: openshift-storage\n" err="no query API server reachable"
2022-05-24T18:42:30.474912844Z level=error ts=2022-05-24T18:42:30.474872371Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): fetch series for {prometheus=\\\"openshift-user-workload-monitoring/user-workload\\\", prometheus_replica=\\\"prometheus-user-workload-1\\\"} Addr: 10.129.2.36:10901 LabelSets: {prometheus=\\\"openshift-user-workload-monitoring/user-workload\\\", prometheus_replica=\\\"prometheus-user-workload-1\\\"} Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial tcp 10.129.2.36:10901: connect: connection refused\\\"\"}\n" query="avg(topk by(ceph_daemon) (1, label_replace(label_replace(ceph_disk_occupation{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"}, \"instance\", \"$1\", \"exported_instance\", \"(.*)\"), \"device\", \"$1\", \"device\", \"/dev/(.*)\")) * on(instance, device) group_right(ceph_daemon) topk by(instance, device) (1, (irate(node_disk_read_time_seconds_total{namespace=\"openshift-storage\"}[1m]) + irate(node_disk_write_time_seconds_total{namespace=\"openshift-storage\"}[1m]) / (clamp_min(irate(node_disk_reads_completed_total{namespace=\"openshift-storage\"}[1m]), 1) + irate(node_disk_writes_completed_total{namespace=\"openshift-storage\"}[1m])))))"
2022-05-24T18:42:30.475017998Z ts=2022-05-24T18:42:30.47489886Z caller=log.go:168 component=rules level=warn group=ceph.rules msg="Evaluating rule failed" rule="record: cluster:ceph_disk_latency:join_ceph_node_disk_irate1m\nexpr: avg(topk by(ceph_daemon) (1, label_replace(label_replace(ceph_disk_occupation{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"},\n  \"instance\", \"$1\", \"exported_instance\", \"(.*)\"), \"device\", \"$1\", \"device\", \"/dev/(.*)\"))\n  * on(instance, device) group_right(ceph_daemon) topk by(instance, device) (1, (irate(node_disk_read_time_seconds_total{namespace=\"openshift-storage\"}[1m])\n  + irate(node_disk_write_time_seconds_total{namespace=\"openshift-storage\"}[1m]) /\n  (clamp_min(irate(node_disk_reads_completed_total{namespace=\"openshift-storage\"}[1m]),\n  1) + irate(node_disk_writes_completed_total{namespace=\"openshift-storage\"}[1m])))))\nlabels:\n  namespace: openshift-storage\n" err="no query API server reachable"
2022-05-24T18:42:38.345788018Z level=error ts=2022-05-24T18:42:38.345738214Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): fetch series for {prometheus=\\\"openshift-user-workload-monitoring/user-workload\\\", prometheus_replica=\\\"prometheus-user-workload-1\\\"} Addr: 10.129.2.36:10901 LabelSets: {prometheus=\\\"openshift-user-workload-monitoring/user-workload\\\", prometheus_replica=\\\"prometheus-user-workload-1\\\"} Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Canceled desc = grpc: the client connection is closing\"}\n" query="ceph_cluster_total_used_raw_bytes{namespace=\"openshift-storage\"} / ceph_cluster_total_bytes{namespace=\"openshift-storage\"} > 0.75"
2022-05-24T18:42:38.345881278Z ts=2022-05-24T18:42:38.345776343Z caller=log.go:168 component=rules level=warn group=cluster-utilization-alert.rules msg="Evaluating rule failed" rule="alert: CephClusterNearFull\nexpr: ceph_cluster_total_used_raw_bytes{namespace=\"openshift-storage\"} / ceph_cluster_total_bytes{namespace=\"openshift-storage\"}\n  > 0.75\nfor: 5s\nlabels:\n  namespace: openshift-storage\n  severity: warning\nannotations:\n  description: Storage cluster utilization has crossed 75% and will become read-only\n    at 85%. Free up some space or expand the storage cluster.\n  message: Storage cluster is nearing full. Data deletion or cluster expansion is\n    required.\n  severity_level: warning\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:42:38.346211702Z level=error ts=2022-05-24T18:42:38.34616844Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): fetch series for {prometheus=\\\"openshift-user-workload-monitoring/user-workload\\\", prometheus_replica=\\\"prometheus-user-workload-1\\\"} Addr: 10.129.2.36:10901 LabelSets: {prometheus=\\\"openshift-user-workload-monitoring/user-workload\\\", prometheus_replica=\\\"prometheus-user-workload-1\\\"} Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Canceled desc = grpc: the client connection is closing\"}\n" query="(ceph_osd_metadata{namespace=\"openshift-storage\"} * on(ceph_daemon) group_right(device_class, hostname) (ceph_osd_stat_bytes_used{namespace=\"openshift-storage\"} / ceph_osd_stat_bytes{namespace=\"openshift-storage\"})) >= 0.8"
2022-05-24T18:42:38.346325115Z ts=2022-05-24T18:42:38.346204831Z caller=log.go:168 component=rules level=warn group=osd-alert.rules msg="Evaluating rule failed" rule="alert: CephOSDCriticallyFull\nexpr: (ceph_osd_metadata{namespace=\"openshift-storage\"} * on(ceph_daemon) group_right(device_class,\n  hostname) (ceph_osd_stat_bytes_used{namespace=\"openshift-storage\"} / ceph_osd_stat_bytes{namespace=\"openshift-storage\"}))\n  >= 0.8\nfor: 40s\nlabels:\n  namespace: openshift-storage\n  severity: critical\nannotations:\n  description: Utilization of storage device {{ $labels.ceph_daemon }} of device_class\n    type {{$labels.device_class}} has crossed 80% on host {{ $labels.hostname }}.\n    Immediately free up some space or add capacity of type {{$labels.device_class}}.\n  message: Back-end storage device is critically full.\n  severity_level: error\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:42:38.347360343Z level=error ts=2022-05-24T18:42:38.347307269Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): fetch series for {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Addr: 10.129.2.37:10901 LabelSets: {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Mint: 1653417294716 Maxt: 9223372036854775807: rpc error: code = Canceled desc = grpc: the client connection is closing\"}\n" query="label_replace((up{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"} == 0 or absent(up{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"})), \"namespace\", \"openshift-storage\", \"\", \"\")"
2022-05-24T18:42:38.347440295Z ts=2022-05-24T18:42:38.347350539Z caller=log.go:168 component=rules level=warn group=ceph-mgr-status msg="Evaluating rule failed" rule="alert: CephMgrIsAbsent\nexpr: label_replace((up{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"} == 0 or\n  absent(up{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"})), \"namespace\", \"openshift-storage\",\n  \"\", \"\")\nfor: 5m\nlabels:\n  namespace: openshift-storage\n  severity: critical\nannotations:\n  description: Ceph Manager has disappeared from Prometheus target discovery.\n  message: Storage metrics collector service not available anymore.\n  severity_level: critical\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:42:38.347750910Z level=error ts=2022-05-24T18:42:38.347711016Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): fetch series for {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Addr: 10.129.2.37:10901 LabelSets: {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Mint: 1653417294716 Maxt: 9223372036854775807: rpc error: code = Canceled desc = grpc: the client connection is closing\"}\n" query="count by(namespace) (ceph_mon_quorum_status{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"} == 1) <= (floor(count by(namespace) (ceph_mon_metadata{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"}) / 2) + 1)"
2022-05-24T18:42:38.347865193Z ts=2022-05-24T18:42:38.347746211Z caller=log.go:168 component=rules level=warn group=quorum-alert.rules msg="Evaluating rule failed" rule="alert: CephMonQuorumAtRisk\nexpr: count by(namespace) (ceph_mon_quorum_status{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"}\n  == 1) <= (floor(count by(namespace) (ceph_mon_metadata{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"})\n  / 2) + 1)\nfor: 15m\nlabels:\n  namespace: openshift-storage\n  severity: critical\nannotations:\n  description: Storage cluster quorum is low. Contact Support.\n  message: Storage quorum at risk\n  severity_level: error\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:42:38.348106589Z level=error ts=2022-05-24T18:42:38.348049963Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): fetch series for {prometheus=\\\"openshift-user-workload-monitoring/user-workload\\\", prometheus_replica=\\\"prometheus-user-workload-1\\\"} Addr: 10.129.2.36:10901 LabelSets: {prometheus=\\\"openshift-user-workload-monitoring/user-workload\\\", prometheus_replica=\\\"prometheus-user-workload-1\\\"} Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Canceled desc = grpc: the client connection is closing\"}\n" query="(kubelet_volume_stats_used_bytes{namespace=\"openshift-storage\"} * on(namespace, persistentvolumeclaim) group_left(storageclass, provisioner) (kube_persistentvolumeclaim_info{namespace=\"openshift-storage\"} * on(storageclass) group_left(provisioner) kube_storageclass_info{namespace=\"openshift-storage\",provisioner=~\"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)\"})) / (kubelet_volume_stats_capacity_bytes{namespace=\"openshift-storage\"} * on(namespace, persistentvolumeclaim) group_left(storageclass, provisioner) (kube_persistentvolumeclaim_info{namespace=\"openshift-storage\"} * on(storageclass) group_left(provisioner) kube_storageclass_info{namespace=\"openshift-storage\",provisioner=~\"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)\"})) > 0.75"
2022-05-24T18:42:38.348158865Z level=error ts=2022-05-24T18:42:38.348111686Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): fetch series for {prometheus=\\\"openshift-user-workload-monitoring/user-workload\\\", prometheus_replica=\\\"prometheus-user-workload-1\\\"} Addr: 10.129.2.36:10901 LabelSets: {prometheus=\\\"openshift-user-workload-monitoring/user-workload\\\", prometheus_replica=\\\"prometheus-user-workload-1\\\"} Mint: -62167219200000 Maxt: 9223372036854775807: rpc error: code = Canceled desc = grpc: the client connection is closing\"}\n" query="sum by(namespace) (ceph_mds_metadata{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"} == 1) < 2"
2022-05-24T18:42:38.348247416Z ts=2022-05-24T18:42:38.348155498Z caller=log.go:168 component=rules level=warn group=ceph-mds-status msg="Evaluating rule failed" rule="alert: CephMdsMissingReplicas\nexpr: sum by(namespace) (ceph_mds_metadata{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"}\n  == 1) < 2\nfor: 5m\nlabels:\n  namespace: openshift-storage\n  severity: warning\nannotations:\n  description: Minimum required replicas for storage metadata service not available.\n    Might affect the working of storage cluster.\n  message: Insufficient replicas for storage metadata service.\n  severity_level: warning\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:42:38.348261504Z ts=2022-05-24T18:42:38.348092623Z caller=log.go:168 component=rules level=warn group=persistent-volume-alert.rules msg="Evaluating rule failed" rule="alert: PersistentVolumeUsageNearFull\nexpr: (kubelet_volume_stats_used_bytes{namespace=\"openshift-storage\"} * on(namespace,\n  persistentvolumeclaim) group_left(storageclass, provisioner) (kube_persistentvolumeclaim_info{namespace=\"openshift-storage\"}\n  * on(storageclass) group_left(provisioner) kube_storageclass_info{namespace=\"openshift-storage\",provisioner=~\"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)\"}))\n  / (kubelet_volume_stats_capacity_bytes{namespace=\"openshift-storage\"} * on(namespace,\n  persistentvolumeclaim) group_left(storageclass, provisioner) (kube_persistentvolumeclaim_info{namespace=\"openshift-storage\"}\n  * on(storageclass) group_left(provisioner) kube_storageclass_info{namespace=\"openshift-storage\",provisioner=~\"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)\"}))\n  > 0.75\nfor: 5s\nlabels:\n  namespace: openshift-storage\n  severity: warning\nannotations:\n  description: PVC {{ $labels.persistentvolumeclaim }} utilization has crossed 75%.\n    Free up some space or expand the PVC.\n  message: PVC {{ $labels.persistentvolumeclaim }} is nearing full. Data deletion\n    or PVC expansion is required.\n  severity_level: warning\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:43:33.823422113Z level=error ts=2022-05-24T18:43:33.82336299Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): fetch series for {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Addr: 10.128.2.15:10901 LabelSets: {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Mint: 1653417762981 Maxt: 9223372036854775807: rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial tcp 10.128.2.15:10901: connect: connection refused\\\"\"}\n" query="sum by(namespace) (ceph_mds_metadata{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"} == 1) < 2"
2022-05-24T18:43:33.823500249Z ts=2022-05-24T18:43:33.823407924Z caller=log.go:168 component=rules level=warn group=ceph-mds-status msg="Evaluating rule failed" rule="alert: CephMdsMissingReplicas\nexpr: sum by(namespace) (ceph_mds_metadata{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"}\n  == 1) < 2\nfor: 5m\nlabels:\n  namespace: openshift-storage\n  severity: warning\nannotations:\n  description: Minimum required replicas for storage metadata service not available.\n    Might affect the working of storage cluster.\n  message: Insufficient replicas for storage metadata service.\n  severity_level: warning\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:43:34.157260015Z level=error ts=2022-05-24T18:43:34.157201959Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): fetch series for {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Addr: 10.128.2.15:10901 LabelSets: {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Mint: 1653417762981 Maxt: 9223372036854775807: rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial tcp 10.128.2.15:10901: connect: connection refused\\\"\"}\n" query="label_replace((up{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"} == 0 or absent(up{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"})), \"namespace\", \"openshift-storage\", \"\", \"\")"
2022-05-24T18:43:34.157341842Z ts=2022-05-24T18:43:34.157244035Z caller=log.go:168 component=rules level=warn group=ceph-mgr-status msg="Evaluating rule failed" rule="alert: CephMgrIsAbsent\nexpr: label_replace((up{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"} == 0 or\n  absent(up{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"})), \"namespace\", \"openshift-storage\",\n  \"\", \"\")\nfor: 5m\nlabels:\n  namespace: openshift-storage\n  severity: critical\nannotations:\n  description: Ceph Manager has disappeared from Prometheus target discovery.\n  message: Storage metrics collector service not available anymore.\n  severity_level: critical\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:43:34.158494459Z level=error ts=2022-05-24T18:43:34.158455568Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): fetch series for {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Addr: 10.128.2.15:10901 LabelSets: {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Mint: 1653417762981 Maxt: 9223372036854775807: rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial tcp 10.128.2.15:10901: connect: connection refused\\\"\"}\n" query="sum by(namespace) (kube_deployment_spec_replicas{deployment=~\"rook-ceph-mgr-.*\",namespace=\"openshift-storage\"}) < 1"
2022-05-24T18:43:34.158562499Z ts=2022-05-24T18:43:34.158485605Z caller=log.go:168 component=rules level=warn group=ceph-mgr-status msg="Evaluating rule failed" rule="alert: CephMgrIsMissingReplicas\nexpr: sum by(namespace) (kube_deployment_spec_replicas{deployment=~\"rook-ceph-mgr-.*\",namespace=\"openshift-storage\"})\n  < 1\nfor: 5m\nlabels:\n  namespace: openshift-storage\n  severity: warning\nannotations:\n  description: Ceph Manager is missing replicas.\n  message: Storage metrics collector service doesn't have required no of replicas.\n  severity_level: warning\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:43:35.850760375Z level=error ts=2022-05-24T18:43:35.850700761Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): fetch series for {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Addr: 10.128.2.15:10901 LabelSets: {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Mint: 1653417762981 Maxt: 9223372036854775807: rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial tcp 10.128.2.15:10901: connect: connection refused\\\"\"}\n" query="(kubelet_volume_stats_used_bytes{namespace=\"openshift-storage\"} * on(namespace, persistentvolumeclaim) group_left(storageclass, provisioner) (kube_persistentvolumeclaim_info{namespace=\"openshift-storage\"} * on(storageclass) group_left(provisioner) kube_storageclass_info{namespace=\"openshift-storage\",provisioner=~\"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)\"})) / (kubelet_volume_stats_capacity_bytes{namespace=\"openshift-storage\"} * on(namespace, persistentvolumeclaim) group_left(storageclass, provisioner) (kube_persistentvolumeclaim_info{namespace=\"openshift-storage\"} * on(storageclass) group_left(provisioner) kube_storageclass_info{namespace=\"openshift-storage\",provisioner=~\"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)\"})) > 0.75"
2022-05-24T18:43:35.850898094Z ts=2022-05-24T18:43:35.850746543Z caller=log.go:168 component=rules level=warn group=persistent-volume-alert.rules msg="Evaluating rule failed" rule="alert: PersistentVolumeUsageNearFull\nexpr: (kubelet_volume_stats_used_bytes{namespace=\"openshift-storage\"} * on(namespace,\n  persistentvolumeclaim) group_left(storageclass, provisioner) (kube_persistentvolumeclaim_info{namespace=\"openshift-storage\"}\n  * on(storageclass) group_left(provisioner) kube_storageclass_info{namespace=\"openshift-storage\",provisioner=~\"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)\"}))\n  / (kubelet_volume_stats_capacity_bytes{namespace=\"openshift-storage\"} * on(namespace,\n  persistentvolumeclaim) group_left(storageclass, provisioner) (kube_persistentvolumeclaim_info{namespace=\"openshift-storage\"}\n  * on(storageclass) group_left(provisioner) kube_storageclass_info{namespace=\"openshift-storage\",provisioner=~\"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)\"}))\n  > 0.75\nfor: 5s\nlabels:\n  namespace: openshift-storage\n  severity: warning\nannotations:\n  description: PVC {{ $labels.persistentvolumeclaim }} utilization has crossed 75%.\n    Free up some space or expand the PVC.\n  message: PVC {{ $labels.persistentvolumeclaim }} is nearing full. Data deletion\n    or PVC expansion is required.\n  severity_level: warning\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:43:35.852819171Z level=error ts=2022-05-24T18:43:35.852773358Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): fetch series for {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Addr: 10.128.2.15:10901 LabelSets: {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Mint: 1653417762981 Maxt: 9223372036854775807: rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial tcp 10.128.2.15:10901: connect: connection refused\\\"\"}\n" query="(kubelet_volume_stats_used_bytes{namespace=\"openshift-storage\"} * on(namespace, persistentvolumeclaim) group_left(storageclass, provisioner) (kube_persistentvolumeclaim_info{namespace=\"openshift-storage\"} * on(storageclass) group_left(provisioner) kube_storageclass_info{namespace=\"openshift-storage\",provisioner=~\"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)\"})) / (kubelet_volume_stats_capacity_bytes{namespace=\"openshift-storage\"} * on(namespace, persistentvolumeclaim) group_left(storageclass, provisioner) (kube_persistentvolumeclaim_info{namespace=\"openshift-storage\"} * on(storageclass) group_left(provisioner) kube_storageclass_info{namespace=\"openshift-storage\",provisioner=~\"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)\"})) > 0.85"
2022-05-24T18:43:35.852946038Z ts=2022-05-24T18:43:35.852817175Z caller=log.go:168 component=rules level=warn group=persistent-volume-alert.rules msg="Evaluating rule failed" rule="alert: PersistentVolumeUsageCritical\nexpr: (kubelet_volume_stats_used_bytes{namespace=\"openshift-storage\"} * on(namespace,\n  persistentvolumeclaim) group_left(storageclass, provisioner) (kube_persistentvolumeclaim_info{namespace=\"openshift-storage\"}\n  * on(storageclass) group_left(provisioner) kube_storageclass_info{namespace=\"openshift-storage\",provisioner=~\"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)\"}))\n  / (kubelet_volume_stats_capacity_bytes{namespace=\"openshift-storage\"} * on(namespace,\n  persistentvolumeclaim) group_left(storageclass, provisioner) (kube_persistentvolumeclaim_info{namespace=\"openshift-storage\"}\n  * on(storageclass) group_left(provisioner) kube_storageclass_info{namespace=\"openshift-storage\",provisioner=~\"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)\"}))\n  > 0.85\nfor: 5s\nlabels:\n  namespace: openshift-storage\n  severity: critical\nannotations:\n  description: PVC {{ $labels.persistentvolumeclaim }} utilization has crossed 85%.\n    Free up some space or expand the PVC immediately.\n  message: PVC {{ $labels.persistentvolumeclaim }} is critically full. Data deletion\n    or PVC expansion is required.\n  severity_level: error\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:43:35.937091864Z level=error ts=2022-05-24T18:43:35.937050613Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): fetch series for {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Addr: 10.128.2.15:10901 LabelSets: {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Mint: 1653417762981 Maxt: 9223372036854775807: rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial tcp 10.128.2.15:10901: connect: connection refused\\\"\"}\n" query="(ceph_osd_metadata{namespace=\"openshift-storage\"} * on(ceph_daemon) group_right(device_class, hostname) (ceph_osd_stat_bytes_used{namespace=\"openshift-storage\"} / ceph_osd_stat_bytes{namespace=\"openshift-storage\"})) >= 0.8"
2022-05-24T18:43:35.937175283Z ts=2022-05-24T18:43:35.937080732Z caller=log.go:168 component=rules level=warn group=osd-alert.rules msg="Evaluating rule failed" rule="alert: CephOSDCriticallyFull\nexpr: (ceph_osd_metadata{namespace=\"openshift-storage\"} * on(ceph_daemon) group_right(device_class,\n  hostname) (ceph_osd_stat_bytes_used{namespace=\"openshift-storage\"} / ceph_osd_stat_bytes{namespace=\"openshift-storage\"}))\n  >= 0.8\nfor: 40s\nlabels:\n  namespace: openshift-storage\n  severity: critical\nannotations:\n  description: Utilization of storage device {{ $labels.ceph_daemon }} of device_class\n    type {{$labels.device_class}} has crossed 80% on host {{ $labels.hostname }}.\n    Immediately free up some space or add capacity of type {{$labels.device_class}}.\n  message: Back-end storage device is critically full.\n  severity_level: error\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:43:35.938056464Z level=error ts=2022-05-24T18:43:35.938028221Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): fetch series for {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Addr: 10.128.2.15:10901 LabelSets: {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Mint: 1653417762981 Maxt: 9223372036854775807: rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial tcp 10.128.2.15:10901: connect: connection refused\\\"\"}\n" query="changes(ceph_osd_up{namespace=\"openshift-storage\"}[5m]) >= 10"
2022-05-24T18:43:35.938124239Z ts=2022-05-24T18:43:35.93805608Z caller=log.go:168 component=rules level=warn group=osd-alert.rules msg="Evaluating rule failed" rule="alert: CephOSDFlapping\nexpr: changes(ceph_osd_up{namespace=\"openshift-storage\"}[5m]) >= 10\nlabels:\n  namespace: openshift-storage\n  severity: critical\nannotations:\n  description: Storage daemon {{ $labels.ceph_daemon }} has restarted 5 times in last\n    5 minutes. Please check the pod events or ceph status to find out the cause.\n  message: Ceph storage osd flapping.\n  severity_level: error\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:43:35.939406361Z level=error ts=2022-05-24T18:43:35.939379692Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): fetch series for {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Addr: 10.128.2.15:10901 LabelSets: {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Mint: 1653417762981 Maxt: 9223372036854775807: rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial tcp 10.128.2.15:10901: connect: connection refused\\\"\"}\n" query="(ceph_osd_metadata{namespace=\"openshift-storage\"} * on(ceph_daemon) group_right(device_class, hostname) (ceph_osd_stat_bytes_used{namespace=\"openshift-storage\"} / ceph_osd_stat_bytes{namespace=\"openshift-storage\"})) >= 0.75"
2022-05-24T18:43:35.939488005Z ts=2022-05-24T18:43:35.939406324Z caller=log.go:168 component=rules level=warn group=osd-alert.rules msg="Evaluating rule failed" rule="alert: CephOSDNearFull\nexpr: (ceph_osd_metadata{namespace=\"openshift-storage\"} * on(ceph_daemon) group_right(device_class,\n  hostname) (ceph_osd_stat_bytes_used{namespace=\"openshift-storage\"} / ceph_osd_stat_bytes{namespace=\"openshift-storage\"}))\n  >= 0.75\nfor: 40s\nlabels:\n  namespace: openshift-storage\n  severity: warning\nannotations:\n  description: Utilization of storage device {{ $labels.ceph_daemon }} of device_class\n    type {{$labels.device_class}} has crossed 75% on host {{ $labels.hostname }}.\n    Immediately free up some space or add capacity of type {{$labels.device_class}}.\n  message: Back-end storage device is nearing full.\n  severity_level: warning\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:43:35.940627852Z level=error ts=2022-05-24T18:43:35.940601066Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): fetch series for {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Addr: 10.128.2.15:10901 LabelSets: {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Mint: 1653417762981 Maxt: 9223372036854775807: rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial tcp 10.128.2.15:10901: connect: connection refused\\\"\"}\n" query="label_replace((ceph_osd_in{namespace=\"openshift-storage\"} == 1 and ceph_osd_up{namespace=\"openshift-storage\"} == 0), \"disk\", \"$1\", \"ceph_daemon\", \"osd.(.*)\") + on(ceph_daemon) group_left(host, device) label_replace(ceph_disk_occupation{namespace=\"openshift-storage\"}, \"host\", \"$1\", \"exported_instance\", \"(.*)\")"
2022-05-24T18:43:35.940720096Z ts=2022-05-24T18:43:35.940627664Z caller=log.go:168 component=rules level=warn group=osd-alert.rules msg="Evaluating rule failed" rule="alert: CephOSDDiskNotResponding\nexpr: label_replace((ceph_osd_in{namespace=\"openshift-storage\"} == 1 and ceph_osd_up{namespace=\"openshift-storage\"}\n  == 0), \"disk\", \"$1\", \"ceph_daemon\", \"osd.(.*)\") + on(ceph_daemon) group_left(host,\n  device) label_replace(ceph_disk_occupation{namespace=\"openshift-storage\"}, \"host\",\n  \"$1\", \"exported_instance\", \"(.*)\")\nfor: 15m\nlabels:\n  namespace: openshift-storage\n  severity: critical\nannotations:\n  description: Disk device {{ $labels.device }} not responding, on host {{ $labels.host\n    }}.\n  message: Disk not responding\n  severity_level: error\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:43:35.941791880Z level=error ts=2022-05-24T18:43:35.941764406Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): fetch series for {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Addr: 10.128.2.15:10901 LabelSets: {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Mint: 1653417762981 Maxt: 9223372036854775807: rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial tcp 10.128.2.15:10901: connect: connection refused\\\"\"}\n" query="label_replace((ceph_osd_in{namespace=\"openshift-storage\"} == 0 and ceph_osd_up{namespace=\"openshift-storage\"} == 0), \"disk\", \"$1\", \"ceph_daemon\", \"osd.(.*)\") + on(ceph_daemon) group_left(host, device) label_replace(ceph_disk_occupation{namespace=\"openshift-storage\"}, \"host\", \"$1\", \"exported_instance\", \"(.*)\")"
2022-05-24T18:43:35.941880095Z ts=2022-05-24T18:43:35.941792472Z caller=log.go:168 component=rules level=warn group=osd-alert.rules msg="Evaluating rule failed" rule="alert: CephOSDDiskUnavailable\nexpr: label_replace((ceph_osd_in{namespace=\"openshift-storage\"} == 0 and ceph_osd_up{namespace=\"openshift-storage\"}\n  == 0), \"disk\", \"$1\", \"ceph_daemon\", \"osd.(.*)\") + on(ceph_daemon) group_left(host,\n  device) label_replace(ceph_disk_occupation{namespace=\"openshift-storage\"}, \"host\",\n  \"$1\", \"exported_instance\", \"(.*)\")\nfor: 1m\nlabels:\n  namespace: openshift-storage\n  severity: critical\nannotations:\n  description: Disk device {{ $labels.device }} not accessible on host {{ $labels.host\n    }}.\n  message: Disk not accessible\n  severity_level: error\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:43:35.942623991Z level=error ts=2022-05-24T18:43:35.942598605Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): fetch series for {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Addr: 10.128.2.15:10901 LabelSets: {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Mint: 1653417762981 Maxt: 9223372036854775807: rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial tcp 10.128.2.15:10901: connect: connection refused\\\"\"}\n" query="ceph_healthcheck_slow_ops{namespace=\"openshift-storage\"} > 0"
2022-05-24T18:43:35.942683977Z ts=2022-05-24T18:43:35.942622761Z caller=log.go:168 component=rules level=warn group=osd-alert.rules msg="Evaluating rule failed" rule="alert: CephOSDSlowOps\nexpr: ceph_healthcheck_slow_ops{namespace=\"openshift-storage\"} > 0\nfor: 30s\nlabels:\n  namespace: openshift-storage\n  severity: warning\nannotations:\n  description: '{{ $value }} Ceph OSD requests are taking too long to process. Please\n    check ceph status to find out the cause.'\n  message: OSD requests are taking too long to process.\n  severity_level: warning\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:43:35.943500899Z level=error ts=2022-05-24T18:43:35.943477639Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): fetch series for {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Addr: 10.128.2.15:10901 LabelSets: {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Mint: 1653417762981 Maxt: 9223372036854775807: rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial tcp 10.128.2.15:10901: connect: connection refused\\\"\"}\n" query="ceph_pg_undersized{namespace=\"openshift-storage\"} > 0"
2022-05-24T18:43:35.943567145Z ts=2022-05-24T18:43:35.943503635Z caller=log.go:168 component=rules level=warn group=osd-alert.rules msg="Evaluating rule failed" rule="alert: CephDataRecoveryTakingTooLong\nexpr: ceph_pg_undersized{namespace=\"openshift-storage\"} > 0\nfor: 2h\nlabels:\n  namespace: openshift-storage\n  severity: warning\nannotations:\n  description: Data recovery has been active for too long. Contact Support.\n  message: Data recovery is slow\n  severity_level: warning\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:43:35.944619836Z level=error ts=2022-05-24T18:43:35.944575592Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): fetch series for {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Addr: 10.128.2.15:10901 LabelSets: {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Mint: 1653417762981 Maxt: 9223372036854775807: rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial tcp 10.128.2.15:10901: connect: connection refused\\\"\"}\n" query="ceph_pg_inconsistent{namespace=\"openshift-storage\"} > 0"
2022-05-24T18:43:35.944682001Z ts=2022-05-24T18:43:35.94461607Z caller=log.go:168 component=rules level=warn group=osd-alert.rules msg="Evaluating rule failed" rule="alert: CephPGRepairTakingTooLong\nexpr: ceph_pg_inconsistent{namespace=\"openshift-storage\"} > 0\nfor: 1h\nlabels:\n  namespace: openshift-storage\n  severity: warning\nannotations:\n  description: Self heal operations taking too long. Contact Support.\n  message: Self heal problems detected\n  severity_level: warning\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:43:36.134540969Z level=error ts=2022-05-24T18:43:36.134494672Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): fetch series for {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Addr: 10.128.2.15:10901 LabelSets: {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Mint: 1653417762981 Maxt: 9223372036854775807: rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial tcp 10.128.2.15:10901: connect: connection refused\\\"\"}\n" query="count by(namespace) (ceph_mon_quorum_status{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"} == 1) <= (floor(count by(namespace) (ceph_mon_metadata{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"}) / 2) + 1)"
2022-05-24T18:43:36.134637838Z ts=2022-05-24T18:43:36.134521913Z caller=log.go:168 component=rules level=warn group=quorum-alert.rules msg="Evaluating rule failed" rule="alert: CephMonQuorumAtRisk\nexpr: count by(namespace) (ceph_mon_quorum_status{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"}\n  == 1) <= (floor(count by(namespace) (ceph_mon_metadata{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"})\n  / 2) + 1)\nfor: 15m\nlabels:\n  namespace: openshift-storage\n  severity: critical\nannotations:\n  description: Storage cluster quorum is low. Contact Support.\n  message: Storage quorum at risk\n  severity_level: error\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:43:36.135640307Z level=error ts=2022-05-24T18:43:36.135612269Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): fetch series for {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Addr: 10.128.2.15:10901 LabelSets: {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Mint: 1653417762981 Maxt: 9223372036854775807: rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial tcp 10.128.2.15:10901: connect: connection refused\\\"\"}\n" query="count by(namespace) (kube_pod_status_phase{namespace=\"openshift-storage\",phase=~\"Running|running\",pod=~\"rook-ceph-mon-.*\"} == 1) < 2"
2022-05-24T18:43:36.135718232Z ts=2022-05-24T18:43:36.135639048Z caller=log.go:168 component=rules level=warn group=quorum-alert.rules msg="Evaluating rule failed" rule="alert: CephMonQuorumLost\nexpr: count by(namespace) (kube_pod_status_phase{namespace=\"openshift-storage\",phase=~\"Running|running\",pod=~\"rook-ceph-mon-.*\"}\n  == 1) < 2\nfor: 5m\nlabels:\n  namespace: openshift-storage\n  severity: critical\nannotations:\n  description: Storage cluster quorum is lost. Contact Support.\n  message: Storage quorum is lost\n  severity_level: critical\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:43:36.136922035Z level=error ts=2022-05-24T18:43:36.136883088Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): fetch series for {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Addr: 10.128.2.15:10901 LabelSets: {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Mint: 1653417762981 Maxt: 9223372036854775807: rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial tcp 10.128.2.15:10901: connect: connection refused\\\"\"}\n" query="(ceph_mon_metadata{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"} * on(ceph_daemon) group_left() (rate(ceph_mon_num_elections{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"}[5m]) * 60)) > 0.95"
2022-05-24T18:43:36.136999127Z ts=2022-05-24T18:43:36.136911853Z caller=log.go:168 component=rules level=warn group=quorum-alert.rules msg="Evaluating rule failed" rule="alert: CephMonHighNumberOfLeaderChanges\nexpr: (ceph_mon_metadata{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"} * on(ceph_daemon)\n  group_left() (rate(ceph_mon_num_elections{job=\"rook-ceph-mgr\",namespace=\"openshift-storage\"}[5m])\n  * 60)) > 0.95\nfor: 5m\nlabels:\n  namespace: openshift-storage\n  severity: warning\nannotations:\n  description: Ceph Monitor {{ $labels.ceph_daemon }} on host {{ $labels.hostname\n    }} has seen {{ $value | printf \"%.2f\" }} leader changes per minute recently.\n  message: Storage Cluster has seen many leader changes recently.\n  severity_level: warning\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:43:37.980356460Z level=error ts=2022-05-24T18:43:37.980308231Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): fetch series for {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Addr: 10.128.2.15:10901 LabelSets: {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Mint: 1653417762981 Maxt: 9223372036854775807: rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial tcp 10.128.2.15:10901: connect: connection refused\\\"\"}\n" query="ceph_cluster_total_used_raw_bytes{namespace=\"openshift-storage\"} / ceph_cluster_total_bytes{namespace=\"openshift-storage\"} > 0.75"
2022-05-24T18:43:37.980438786Z ts=2022-05-24T18:43:37.980346265Z caller=log.go:168 component=rules level=warn group=cluster-utilization-alert.rules msg="Evaluating rule failed" rule="alert: CephClusterNearFull\nexpr: ceph_cluster_total_used_raw_bytes{namespace=\"openshift-storage\"} / ceph_cluster_total_bytes{namespace=\"openshift-storage\"}\n  > 0.75\nfor: 5s\nlabels:\n  namespace: openshift-storage\n  severity: warning\nannotations:\n  description: Storage cluster utilization has crossed 75% and will become read-only\n    at 85%. Free up some space or expand the storage cluster.\n  message: Storage cluster is nearing full. Data deletion or cluster expansion is\n    required.\n  severity_level: warning\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:43:37.981962097Z level=error ts=2022-05-24T18:43:37.981913153Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): fetch series for {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Addr: 10.128.2.15:10901 LabelSets: {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Mint: 1653417762981 Maxt: 9223372036854775807: rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial tcp 10.128.2.15:10901: connect: connection refused\\\"\"}\n" query="ceph_cluster_total_used_raw_bytes{namespace=\"openshift-storage\"} / ceph_cluster_total_bytes{namespace=\"openshift-storage\"} > 0.8"
2022-05-24T18:43:37.982039678Z ts=2022-05-24T18:43:37.981947128Z caller=log.go:168 component=rules level=warn group=cluster-utilization-alert.rules msg="Evaluating rule failed" rule="alert: CephClusterCriticallyFull\nexpr: ceph_cluster_total_used_raw_bytes{namespace=\"openshift-storage\"} / ceph_cluster_total_bytes{namespace=\"openshift-storage\"}\n  > 0.8\nfor: 5s\nlabels:\n  namespace: openshift-storage\n  severity: critical\nannotations:\n  description: Storage cluster utilization has crossed 80% and will become read-only\n    at 85%. Free up some space or expand the storage cluster immediately.\n  message: Storage cluster is critically full and needs immediate data deletion or\n    cluster expansion.\n  severity_level: error\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T18:43:37.983087195Z level=error ts=2022-05-24T18:43:37.98304577Z caller=rule.go:749 component=rules err="read query instant response: expected 2xx response, got 422. Body: {\"status\":\"error\",\"errorType\":\"execution\",\"error\":\"expanding series: proxy Series(): fetch series for {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Addr: 10.128.2.15:10901 LabelSets: {thanos_ruler_replica=\\\"thanos-ruler-user-workload-1\\\"} Mint: 1653417762981 Maxt: 9223372036854775807: rpc error: code = Unavailable desc = connection error: desc = \\\"transport: Error while dialing dial tcp 10.128.2.15:10901: connect: connection refused\\\"\"}\n" query="ceph_cluster_total_used_raw_bytes{namespace=\"openshift-storage\"} / ceph_cluster_total_bytes{namespace=\"openshift-storage\"} >= 0.85"
2022-05-24T18:43:37.983149516Z ts=2022-05-24T18:43:37.983079114Z caller=log.go:168 component=rules level=warn group=cluster-utilization-alert.rules msg="Evaluating rule failed" rule="alert: CephClusterReadOnly\nexpr: ceph_cluster_total_used_raw_bytes{namespace=\"openshift-storage\"} / ceph_cluster_total_bytes{namespace=\"openshift-storage\"}\n  >= 0.85\nlabels:\n  namespace: openshift-storage\n  severity: critical\nannotations:\n  description: Storage cluster utilization has crossed 85% and will become read-only\n    now. Free up some space or expand the storage cluster immediately.\n  message: Storage cluster is read-only now and needs immediate data deletion or cluster\n    expansion.\n  severity_level: error\n  storage_type: ceph\n" err="no query API server reachable"
2022-05-24T21:34:28.006039726Z ts=2022-05-24T21:34:28.00598476Z caller=log.go:168 component=tsdb level=info msg="write block" mint=1653417264716 maxt=1653422400000 ulid=01G3VZW7MJ9NQ7H90TAVDWK8G2 duration=19.263065ms
2022-05-24T21:34:28.006987588Z ts=2022-05-24T21:34:28.006958503Z caller=log.go:168 component=tsdb level=info msg="Head GC completed" duration=761.868µs
2022-05-24T21:34:28.009539583Z ts=2022-05-24T21:34:28.009512812Z caller=log.go:168 component=tsdb level=info msg="Creating checkpoint" from_segment=0 to_segment=1 mint=1653422400000
2022-05-24T21:34:28.013526961Z ts=2022-05-24T21:34:28.013492871Z caller=log.go:168 component=tsdb level=info msg="WAL checkpoint complete" first=0 last=1 duration=4.051955ms
2022-05-24T23:00:09.745409967Z ts=2022-05-24T23:00:09.74534798Z caller=log.go:168 component=tsdb level=info msg="write block" mint=1653422400143 maxt=1653429600000 ulid=01G3W4S4VWCYFTBF3MZTE8R8E7 duration=20.930831ms
2022-05-24T23:00:09.746314813Z ts=2022-05-24T23:00:09.74628838Z caller=log.go:168 component=tsdb level=info msg="Head GC completed" duration=715.986µs
