2022-05-24T18:38:28.949924246Z ts=2022-05-24T18:38:28.949745981Z caller=log.go:168 component=tsdb level=info msg="Replaying on-disk memory mappable chunks if any"
2022-05-24T18:38:28.949998701Z ts=2022-05-24T18:38:28.949963321Z caller=log.go:168 component=tsdb level=info msg="On-disk memory mappable chunks replay completed" duration=25.46µs
2022-05-24T18:38:28.949998701Z ts=2022-05-24T18:38:28.949977393Z caller=log.go:168 component=tsdb level=info msg="Replaying WAL, this may take a while"
2022-05-24T18:38:28.950281222Z ts=2022-05-24T18:38:28.950258123Z caller=log.go:168 component=tsdb level=info msg="WAL segment loaded" segment=0 maxSegment=2
2022-05-24T18:38:28.950627681Z ts=2022-05-24T18:38:28.950577577Z caller=log.go:168 component=tsdb level=info msg="WAL segment loaded" segment=1 maxSegment=2
2022-05-24T18:38:28.950781979Z ts=2022-05-24T18:38:28.950761724Z caller=log.go:168 component=tsdb level=info msg="WAL segment loaded" segment=2 maxSegment=2
2022-05-24T18:38:28.950790965Z ts=2022-05-24T18:38:28.950778595Z caller=log.go:168 component=tsdb level=info msg="WAL replay completed" checkpoint_replay_duration=56.573µs wal_replay_duration=737.83µs total_replay_duration=842.259µs
2022-05-24T18:38:28.951556485Z level=info ts=2022-05-24T18:38:28.951527882Z caller=rule.go:658 msg="a leftover lockfile found and removed"
2022-05-24T18:38:28.952238070Z level=info ts=2022-05-24T18:38:28.952193365Z caller=options.go:31 component=rules protocol=gRPC msg="enabling server side TLS"
2022-05-24T18:38:28.952278318Z level=info ts=2022-05-24T18:38:28.952262234Z caller=options.go:61 component=rules protocol=gRPC msg="server TLS client verification enabled"
2022-05-24T18:38:28.953799812Z level=info ts=2022-05-24T18:38:28.953773132Z caller=rule.go:640 component=rules msg="no supported bucket was configured, uploads will be disabled"
2022-05-24T18:38:28.953808988Z level=info ts=2022-05-24T18:38:28.953795685Z caller=rule.go:643 component=rules msg="starting rule node"
2022-05-24T18:38:28.953902401Z level=info ts=2022-05-24T18:38:28.95386873Z caller=intrumentation.go:48 component=rules msg="changing probe status" status=ready
2022-05-24T18:38:28.953973701Z level=info ts=2022-05-24T18:38:28.953951356Z caller=rule.go:811 component=rules msg="reload rule files" numFiles=1
2022-05-24T18:38:28.954007390Z level=info ts=2022-05-24T18:38:28.953975139Z caller=intrumentation.go:60 component=rules msg="changing probe status" status=healthy
2022-05-24T18:38:28.954017648Z level=info ts=2022-05-24T18:38:28.954004678Z caller=http.go:63 component=rules service=http/server component=rule msg="listening for requests and metrics" address=localhost:10902
2022-05-24T18:38:28.954088761Z level=info ts=2022-05-24T18:38:28.954055803Z caller=grpc.go:127 component=rules service=gRPC/server component=rule msg="listening for serving gRPC" address=0.0.0.0:10901
2022-05-24T18:38:28.954359272Z ts=2022-05-24T18:38:28.954330485Z caller=log.go:168 component=rules service=http/server component=rule level=info msg="TLS is disabled." http2=false
2022-05-24T18:38:28.964438965Z ts=2022-05-24T18:38:28.964408064Z caller=log.go:168 component=rules level=info msg="Stopping rule manager..."
2022-05-24T18:38:28.964438965Z ts=2022-05-24T18:38:28.964427568Z caller=log.go:168 component=rules level=info msg="Rule manager stopped"
2022-05-24T18:38:28.964462625Z ts=2022-05-24T18:38:28.964433685Z caller=log.go:168 component=rules level=info msg="Stopping rule manager..."
2022-05-24T18:38:28.964462625Z ts=2022-05-24T18:38:28.964439197Z caller=log.go:168 component=rules level=info msg="Rule manager stopped"
2022-05-24T18:38:28.964462625Z level=warn ts=2022-05-24T18:38:28.964446029Z caller=intrumentation.go:54 component=rules msg="changing probe status" status=not-ready reason="lookup SRV records \"_web._tcp.alertmanager-operated.openshift-monitoring.svc\": could not resolve \"_web._tcp.alertmanager-operated.openshift-monitoring.svc\": all servers responded with errors to at least one search domain. Errs ;could not resolve _web._tcp.alertmanager-operated.openshift-monitoring.svc.cluster.local.: no servers returned a viable answer. Errs ;resolution against server 172.30.0.10 for _web._tcp.alertmanager-operated.openshift-monitoring.svc.cluster.local.: exchange: read udp 10.131.0.19:42475->172.30.0.10:53: read: connection refused"
2022-05-24T18:38:28.964483614Z level=info ts=2022-05-24T18:38:28.964460778Z caller=grpc.go:134 component=rules service=gRPC/server component=rule msg="internal server is shutting down" err="lookup SRV records \"_web._tcp.alertmanager-operated.openshift-monitoring.svc\": could not resolve \"_web._tcp.alertmanager-operated.openshift-monitoring.svc\": all servers responded with errors to at least one search domain. Errs ;could not resolve _web._tcp.alertmanager-operated.openshift-monitoring.svc.cluster.local.: no servers returned a viable answer. Errs ;resolution against server 172.30.0.10 for _web._tcp.alertmanager-operated.openshift-monitoring.svc.cluster.local.: exchange: read udp 10.131.0.19:42475->172.30.0.10:53: read: connection refused"
2022-05-24T18:38:28.964483614Z level=info ts=2022-05-24T18:38:28.964472407Z caller=grpc.go:147 component=rules service=gRPC/server component=rule msg="gracefully stopping internal server"
2022-05-24T18:38:28.964517943Z level=info ts=2022-05-24T18:38:28.964499439Z caller=grpc.go:160 component=rules service=gRPC/server component=rule msg="internal server is shutdown gracefully" err="lookup SRV records \"_web._tcp.alertmanager-operated.openshift-monitoring.svc\": could not resolve \"_web._tcp.alertmanager-operated.openshift-monitoring.svc\": all servers responded with errors to at least one search domain. Errs ;could not resolve _web._tcp.alertmanager-operated.openshift-monitoring.svc.cluster.local.: no servers returned a viable answer. Errs ;resolution against server 172.30.0.10 for _web._tcp.alertmanager-operated.openshift-monitoring.svc.cluster.local.: exchange: read udp 10.131.0.19:42475->172.30.0.10:53: read: connection refused"
2022-05-24T18:38:28.964526283Z level=warn ts=2022-05-24T18:38:28.964512158Z caller=intrumentation.go:54 component=rules msg="changing probe status" status=not-ready reason="lookup SRV records \"_web._tcp.alertmanager-operated.openshift-monitoring.svc\": could not resolve \"_web._tcp.alertmanager-operated.openshift-monitoring.svc\": all servers responded with errors to at least one search domain. Errs ;could not resolve _web._tcp.alertmanager-operated.openshift-monitoring.svc.cluster.local.: no servers returned a viable answer. Errs ;resolution against server 172.30.0.10 for _web._tcp.alertmanager-operated.openshift-monitoring.svc.cluster.local.: exchange: read udp 10.131.0.19:42475->172.30.0.10:53: read: connection refused"
2022-05-24T18:38:28.964534950Z level=info ts=2022-05-24T18:38:28.964518943Z caller=http.go:74 component=rules service=http/server component=rule msg="internal server is shutting down" err="lookup SRV records \"_web._tcp.alertmanager-operated.openshift-monitoring.svc\": could not resolve \"_web._tcp.alertmanager-operated.openshift-monitoring.svc\": all servers responded with errors to at least one search domain. Errs ;could not resolve _web._tcp.alertmanager-operated.openshift-monitoring.svc.cluster.local.: no servers returned a viable answer. Errs ;resolution against server 172.30.0.10 for _web._tcp.alertmanager-operated.openshift-monitoring.svc.cluster.local.: exchange: read udp 10.131.0.19:42475->172.30.0.10:53: read: connection refused"
2022-05-24T18:38:28.965703108Z level=info ts=2022-05-24T18:38:28.96567125Z caller=http.go:93 component=rules service=http/server component=rule msg="internal server is shutdown gracefully" err="lookup SRV records \"_web._tcp.alertmanager-operated.openshift-monitoring.svc\": could not resolve \"_web._tcp.alertmanager-operated.openshift-monitoring.svc\": all servers responded with errors to at least one search domain. Errs ;could not resolve _web._tcp.alertmanager-operated.openshift-monitoring.svc.cluster.local.: no servers returned a viable answer. Errs ;resolution against server 172.30.0.10 for _web._tcp.alertmanager-operated.openshift-monitoring.svc.cluster.local.: exchange: read udp 10.131.0.19:42475->172.30.0.10:53: read: connection refused"
2022-05-24T18:38:28.965718976Z level=info ts=2022-05-24T18:38:28.9657019Z caller=intrumentation.go:66 component=rules msg="changing probe status" status=not-healthy reason="lookup SRV records \"_web._tcp.alertmanager-operated.openshift-monitoring.svc\": could not resolve \"_web._tcp.alertmanager-operated.openshift-monitoring.svc\": all servers responded with errors to at least one search domain. Errs ;could not resolve _web._tcp.alertmanager-operated.openshift-monitoring.svc.cluster.local.: no servers returned a viable answer. Errs ;resolution against server 172.30.0.10 for _web._tcp.alertmanager-operated.openshift-monitoring.svc.cluster.local.: exchange: read udp 10.131.0.19:42475->172.30.0.10:53: read: connection refused"
2022-05-24T18:38:28.965882725Z level=error ts=2022-05-24T18:38:28.965839098Z caller=main.go:157 err="lookup SRV records \"_web._tcp.alertmanager-operated.openshift-monitoring.svc\": could not resolve \"_web._tcp.alertmanager-operated.openshift-monitoring.svc\": all servers responded with errors to at least one search domain. Errs ;could not resolve _web._tcp.alertmanager-operated.openshift-monitoring.svc.cluster.local.: no servers returned a viable answer. Errs ;resolution against server 172.30.0.10 for _web._tcp.alertmanager-operated.openshift-monitoring.svc.cluster.local.: exchange: read udp 10.131.0.19:42475->172.30.0.10:53: read: connection refused\nrule command failed\nmain.main\n\t/go/src/github.com/improbable-eng/thanos/cmd/thanos/main.go:157\nruntime.main\n\t/usr/lib/golang/src/runtime/proc.go:255\nruntime.goexit\n\t/usr/lib/golang/src/runtime/asm_amd64.s:1581"
