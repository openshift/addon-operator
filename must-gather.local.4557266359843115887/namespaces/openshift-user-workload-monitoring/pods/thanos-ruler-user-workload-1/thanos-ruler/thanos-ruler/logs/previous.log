2022-05-24T18:43:34.824602140Z ts=2022-05-24T18:43:34.824530007Z caller=log.go:168 component=tsdb level=info msg="Replaying on-disk memory mappable chunks if any"
2022-05-24T18:43:34.824602140Z ts=2022-05-24T18:43:34.824568809Z caller=log.go:168 component=tsdb level=info msg="On-disk memory mappable chunks replay completed" duration=2.228µs
2022-05-24T18:43:34.824602140Z ts=2022-05-24T18:43:34.824579694Z caller=log.go:168 component=tsdb level=info msg="Replaying WAL, this may take a while"
2022-05-24T18:43:34.825467486Z ts=2022-05-24T18:43:34.825428113Z caller=log.go:168 component=tsdb level=info msg="WAL segment loaded" segment=0 maxSegment=1
2022-05-24T18:43:34.825620403Z ts=2022-05-24T18:43:34.825600232Z caller=log.go:168 component=tsdb level=info msg="WAL segment loaded" segment=1 maxSegment=1
2022-05-24T18:43:34.825629862Z ts=2022-05-24T18:43:34.825617735Z caller=log.go:168 component=tsdb level=info msg="WAL replay completed" checkpoint_replay_duration=42.766µs wal_replay_duration=985.957µs total_replay_duration=1.049802ms
2022-05-24T18:43:34.826449352Z level=info ts=2022-05-24T18:43:34.82642442Z caller=rule.go:658 msg="a leftover lockfile found and removed"
2022-05-24T18:43:34.827167144Z level=info ts=2022-05-24T18:43:34.827133477Z caller=options.go:31 component=rules protocol=gRPC msg="enabling server side TLS"
2022-05-24T18:43:34.827221480Z level=info ts=2022-05-24T18:43:34.827205285Z caller=options.go:61 component=rules protocol=gRPC msg="server TLS client verification enabled"
2022-05-24T18:43:34.828561904Z level=info ts=2022-05-24T18:43:34.828531571Z caller=rule.go:640 component=rules msg="no supported bucket was configured, uploads will be disabled"
2022-05-24T18:43:34.828572124Z level=info ts=2022-05-24T18:43:34.828557539Z caller=rule.go:643 component=rules msg="starting rule node"
2022-05-24T18:43:34.828666396Z level=info ts=2022-05-24T18:43:34.828630042Z caller=intrumentation.go:48 component=rules msg="changing probe status" status=ready
2022-05-24T18:43:34.828725591Z level=info ts=2022-05-24T18:43:34.828701851Z caller=rule.go:811 component=rules msg="reload rule files" numFiles=2
2022-05-24T18:43:34.828805247Z level=info ts=2022-05-24T18:43:34.828750155Z caller=intrumentation.go:60 component=rules msg="changing probe status" status=healthy
2022-05-24T18:43:34.828817267Z level=info ts=2022-05-24T18:43:34.828803018Z caller=http.go:63 component=rules service=http/server component=rule msg="listening for requests and metrics" address=localhost:10902
2022-05-24T18:43:34.828885290Z level=info ts=2022-05-24T18:43:34.828858592Z caller=grpc.go:127 component=rules service=gRPC/server component=rule msg="listening for serving gRPC" address=0.0.0.0:10901
2022-05-24T18:43:34.829103785Z ts=2022-05-24T18:43:34.829077428Z caller=log.go:168 component=rules service=http/server component=rule level=info msg="TLS is disabled." http2=false
2022-05-24T18:43:34.835977875Z ts=2022-05-24T18:43:34.835875405Z caller=log.go:168 component=rules level=info msg="Stopping rule manager..."
2022-05-24T18:43:34.835977875Z ts=2022-05-24T18:43:34.835908861Z caller=log.go:168 component=rules level=info msg="Rule manager stopped"
2022-05-24T18:43:34.842335465Z ts=2022-05-24T18:43:34.842303852Z caller=log.go:168 component=rules level=info msg="Stopping rule manager..."
2022-05-24T18:43:34.842355852Z ts=2022-05-24T18:43:34.842337569Z caller=log.go:168 component=rules level=info msg="Rule manager stopped"
2022-05-24T18:43:34.842390340Z level=warn ts=2022-05-24T18:43:34.842347109Z caller=intrumentation.go:54 component=rules msg="changing probe status" status=not-ready reason="lookup IP addresses \"_web._tcp.alertmanager-operated.openshift-monitoring.svc\": could not resolve \"alertmanager-main-0.alertmanager-operated.openshift-monitoring.svc.cluster.local.\": all servers responded with errors to at least one search domain. Errs ;could not resolve alertmanager-main-0.alertmanager-operated.openshift-monitoring.svc.cluster.local.: no servers returned a viable answer. Errs ;resolution against server 172.30.0.10 for alertmanager-main-0.alertmanager-operated.openshift-monitoring.svc.cluster.local.: exchange: read udp 10.128.2.15:54873->172.30.0.10:53: read: connection refused"
2022-05-24T18:43:34.842408046Z level=info ts=2022-05-24T18:43:34.842371341Z caller=grpc.go:134 component=rules service=gRPC/server component=rule msg="internal server is shutting down" err="lookup IP addresses \"_web._tcp.alertmanager-operated.openshift-monitoring.svc\": could not resolve \"alertmanager-main-0.alertmanager-operated.openshift-monitoring.svc.cluster.local.\": all servers responded with errors to at least one search domain. Errs ;could not resolve alertmanager-main-0.alertmanager-operated.openshift-monitoring.svc.cluster.local.: no servers returned a viable answer. Errs ;resolution against server 172.30.0.10 for alertmanager-main-0.alertmanager-operated.openshift-monitoring.svc.cluster.local.: exchange: read udp 10.128.2.15:54873->172.30.0.10:53: read: connection refused"
2022-05-24T18:43:34.842415533Z level=info ts=2022-05-24T18:43:34.842401136Z caller=grpc.go:147 component=rules service=gRPC/server component=rule msg="gracefully stopping internal server"
2022-05-24T18:43:34.842445770Z level=info ts=2022-05-24T18:43:34.842429059Z caller=grpc.go:160 component=rules service=gRPC/server component=rule msg="internal server is shutdown gracefully" err="lookup IP addresses \"_web._tcp.alertmanager-operated.openshift-monitoring.svc\": could not resolve \"alertmanager-main-0.alertmanager-operated.openshift-monitoring.svc.cluster.local.\": all servers responded with errors to at least one search domain. Errs ;could not resolve alertmanager-main-0.alertmanager-operated.openshift-monitoring.svc.cluster.local.: no servers returned a viable answer. Errs ;resolution against server 172.30.0.10 for alertmanager-main-0.alertmanager-operated.openshift-monitoring.svc.cluster.local.: exchange: read udp 10.128.2.15:54873->172.30.0.10:53: read: connection refused"
2022-05-24T18:43:34.842453614Z level=warn ts=2022-05-24T18:43:34.842442295Z caller=intrumentation.go:54 component=rules msg="changing probe status" status=not-ready reason="lookup IP addresses \"_web._tcp.alertmanager-operated.openshift-monitoring.svc\": could not resolve \"alertmanager-main-0.alertmanager-operated.openshift-monitoring.svc.cluster.local.\": all servers responded with errors to at least one search domain. Errs ;could not resolve alertmanager-main-0.alertmanager-operated.openshift-monitoring.svc.cluster.local.: no servers returned a viable answer. Errs ;resolution against server 172.30.0.10 for alertmanager-main-0.alertmanager-operated.openshift-monitoring.svc.cluster.local.: exchange: read udp 10.128.2.15:54873->172.30.0.10:53: read: connection refused"
2022-05-24T18:43:34.842460625Z level=info ts=2022-05-24T18:43:34.842450889Z caller=http.go:74 component=rules service=http/server component=rule msg="internal server is shutting down" err="lookup IP addresses \"_web._tcp.alertmanager-operated.openshift-monitoring.svc\": could not resolve \"alertmanager-main-0.alertmanager-operated.openshift-monitoring.svc.cluster.local.\": all servers responded with errors to at least one search domain. Errs ;could not resolve alertmanager-main-0.alertmanager-operated.openshift-monitoring.svc.cluster.local.: no servers returned a viable answer. Errs ;resolution against server 172.30.0.10 for alertmanager-main-0.alertmanager-operated.openshift-monitoring.svc.cluster.local.: exchange: read udp 10.128.2.15:54873->172.30.0.10:53: read: connection refused"
2022-05-24T18:43:34.843619055Z level=info ts=2022-05-24T18:43:34.843577889Z caller=http.go:93 component=rules service=http/server component=rule msg="internal server is shutdown gracefully" err="lookup IP addresses \"_web._tcp.alertmanager-operated.openshift-monitoring.svc\": could not resolve \"alertmanager-main-0.alertmanager-operated.openshift-monitoring.svc.cluster.local.\": all servers responded with errors to at least one search domain. Errs ;could not resolve alertmanager-main-0.alertmanager-operated.openshift-monitoring.svc.cluster.local.: no servers returned a viable answer. Errs ;resolution against server 172.30.0.10 for alertmanager-main-0.alertmanager-operated.openshift-monitoring.svc.cluster.local.: exchange: read udp 10.128.2.15:54873->172.30.0.10:53: read: connection refused"
2022-05-24T18:43:34.843632077Z level=info ts=2022-05-24T18:43:34.843619666Z caller=intrumentation.go:66 component=rules msg="changing probe status" status=not-healthy reason="lookup IP addresses \"_web._tcp.alertmanager-operated.openshift-monitoring.svc\": could not resolve \"alertmanager-main-0.alertmanager-operated.openshift-monitoring.svc.cluster.local.\": all servers responded with errors to at least one search domain. Errs ;could not resolve alertmanager-main-0.alertmanager-operated.openshift-monitoring.svc.cluster.local.: no servers returned a viable answer. Errs ;resolution against server 172.30.0.10 for alertmanager-main-0.alertmanager-operated.openshift-monitoring.svc.cluster.local.: exchange: read udp 10.128.2.15:54873->172.30.0.10:53: read: connection refused"
2022-05-24T18:43:34.843697647Z level=error ts=2022-05-24T18:43:34.84367727Z caller=main.go:157 err="lookup IP addresses \"_web._tcp.alertmanager-operated.openshift-monitoring.svc\": could not resolve \"alertmanager-main-0.alertmanager-operated.openshift-monitoring.svc.cluster.local.\": all servers responded with errors to at least one search domain. Errs ;could not resolve alertmanager-main-0.alertmanager-operated.openshift-monitoring.svc.cluster.local.: no servers returned a viable answer. Errs ;resolution against server 172.30.0.10 for alertmanager-main-0.alertmanager-operated.openshift-monitoring.svc.cluster.local.: exchange: read udp 10.128.2.15:54873->172.30.0.10:53: read: connection refused\nrule command failed\nmain.main\n\t/go/src/github.com/improbable-eng/thanos/cmd/thanos/main.go:157\nruntime.main\n\t/usr/lib/golang/src/runtime/proc.go:255\nruntime.goexit\n\t/usr/lib/golang/src/runtime/asm_amd64.s:1581"
