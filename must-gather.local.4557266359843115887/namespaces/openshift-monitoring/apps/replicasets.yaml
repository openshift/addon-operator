---
apiVersion: apps/v1
items:
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      include.release.openshift.io/self-managed-high-availability: "true"
      include.release.openshift.io/single-node-developer: "true"
    creationTimestamp: "2022-05-24T18:06:07Z"
    generation: 1
    labels:
      app: cluster-monitoring-operator
      app.kubernetes.io/name: cluster-monitoring-operator
      pod-template-hash: d9db9df7c
    name: cluster-monitoring-operator-d9db9df7c
    namespace: openshift-monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cluster-monitoring-operator
      uid: 923eca00-584e-487f-ae37-61efab3d6e36
    resourceVersion: "63981"
    uid: 002361d0-e9a3-4085-b178-d2e038158cb2
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: cluster-monitoring-operator
        pod-template-hash: d9db9df7c
    template:
      metadata:
        annotations:
          target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
        creationTimestamp: null
        labels:
          app: cluster-monitoring-operator
          app.kubernetes.io/name: cluster-monitoring-operator
          pod-template-hash: d9db9df7c
      spec:
        containers:
        - args:
          - --logtostderr
          - --secure-listen-address=:8443
          - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305
          - --upstream=http://127.0.0.1:8080/
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:08e8b4004edaeeb125ced09ab2c4cd6d690afaf3a86309c91a994dec8e3ccbf3
          imagePullPolicy: IfNotPresent
          name: kube-rbac-proxy
          ports:
          - containerPort: 8443
            name: https
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/tls/private
            name: cluster-monitoring-operator-tls
        - args:
          - -namespace=openshift-monitoring
          - -namespace-user-workload=openshift-user-workload-monitoring
          - -configmap=cluster-monitoring-config
          - -release-version=$(RELEASE_VERSION)
          - -logtostderr=true
          - -v=2
          - -images=prometheus-operator=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6332f67ddb8aaba08639689ea91e5a0075e07dbfaea704a8a2b4cb7645281136
          - -images=prometheus-config-reloader=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b5d406a68f3fb45008977ea6f4d1cda50c8a878cf0055c11308b1785d5a06023
          - -images=configmap-reloader=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:29383a4e84dc026ecc9d313b83b8eef1280674a42e4447f5cf3b6f045a3a4f10
          - -images=prometheus=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:727fe0f3dcdad60f8beee58590b33815345e12d86f9526af81b45de17fb28f59
          - -images=alertmanager=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:21780af60acd852880a5b19eb1d3597e83fd2d4e3854c48f7763f04100756a36
          - -images=grafana=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:39594b0dd96744cb8fa31cf0485f7bee06159dc08e489edbacba3b3062efbb68
          - -images=oauth-proxy=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d787f47ee2a410f924ea00b2428f0cf2275eb059adac96ca1b69c71ad20ccb1d
          - -images=node-exporter=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:957a2dc20cc01a3f2fbaa836927aa6833699322657915846e9a84c191fa25bb7
          - -images=kube-state-metrics=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3b2f92612cf6a35edaade46270a961653eb53152a1217fa74be685ac3cfa8b71
          - -images=openshift-state-metrics=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e52032d4e2993366d3b3e6fc81837b2cd79f1a8be5638b9cf409a9aa7547495a
          - -images=kube-rbac-proxy=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:08e8b4004edaeeb125ced09ab2c4cd6d690afaf3a86309c91a994dec8e3ccbf3
          - -images=telemeter-client=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3cd352ab6f82a1f5f762b6283f9644e653d37dcdbab27e1fae3d5a82a76d2a9f
          - -images=prom-label-proxy=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:5c3e8963d0e5dbce2e30dc0341c7453c30a42395622905b718a8fe9974da41ab
          - -images=k8s-prometheus-adapter=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:750e142e941a9b9e1822c52bc671d01ebcb5bf479b42fab978b76c44b17685b5
          - -images=thanos=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8a5a316dec56c12072bd11525534dc01cad163fd44766be4e1fb3406796e662d
          env:
          - name: RELEASE_VERSION
            value: 4.10.13
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:5ae10a5aa60a664893a010afadb0d02833826980d0148f22497fc947c1fb142e
          imagePullPolicy: IfNotPresent
          name: cluster-monitoring-operator
          resources:
            requests:
              cpu: 10m
              memory: 75Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/cluster-monitoring-operator/telemetry
            name: telemetry-config
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
          node-role.kubernetes.io/master: ""
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: cluster-monitoring-operator
        serviceAccountName: cluster-monitoring-operator
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node.kubernetes.io/memory-pressure
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 120
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 120
        volumes:
        - configMap:
            defaultMode: 420
            name: telemetry-config
          name: telemetry-config
        - name: cluster-monitoring-operator-tls
          secret:
            defaultMode: 420
            secretName: cluster-monitoring-operator-tls
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2022-05-24T18:26:26Z"
    generation: 1
    labels:
      name: configure-alertmanager-operator
      pod-template-hash: 6bf57f7cbc
    name: configure-alertmanager-operator-6bf57f7cbc
    namespace: openshift-monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: configure-alertmanager-operator
      uid: 185a6a27-f3f4-4bee-a393-bbdc60aa5d71
    resourceVersion: "70391"
    uid: 89df1660-86d7-4a73-a017-a1c89270e062
  spec:
    replicas: 1
    selector:
      matchLabels:
        name: configure-alertmanager-operator
        pod-template-hash: 6bf57f7cbc
    template:
      metadata:
        annotations:
          categories: A list of comma separated categories that your operator falls
            under.
          certified: "false"
          containerImage: ""
          createdAt: "2022-03-25T05:30:08Z"
          description: OpenShift cluster provisioning and management at scale.
          olm.operatorGroup: openshift-cluster-monitoring
          olm.operatorNamespace: openshift-monitoring
          olm.targetNamespaces: openshift-addon-operator,openshift-apiserver,openshift-apiserver-operator,openshift-authentication,openshift-authentication-operator,openshift-build-test,openshift-cloud-controller-manager,openshift-cloud-controller-manager-operator,openshift-cloud-credential-operator,openshift-cloud-network-config-controller,openshift-cluster-csi-drivers,openshift-cluster-machine-approver,openshift-cluster-node-tuning-operator,openshift-cluster-samples-operator,openshift-cluster-storage-operator,openshift-cluster-version,openshift-codeready-workspaces,openshift-config-operator,openshift-console,openshift-console-operator,openshift-controller-manager,openshift-controller-manager-operator,openshift-custom-domains-operator,openshift-dns,openshift-dns-operator,openshift-etcd-operator,openshift-image-registry,openshift-ingress,openshift-ingress-operator,openshift-insights,openshift-kube-apiserver,openshift-kube-apiserver-operator,openshift-kube-controller-manager,openshift-kube-controller-manager-operator,openshift-kube-scheduler,openshift-kube-scheduler-operator,openshift-kube-storage-version-migrator,openshift-kube-storage-version-migrator-operator,openshift-machine-api,openshift-machine-config-operator,openshift-managed-node-metadata-operator,openshift-managed-upgrade-operator,openshift-marketplace,openshift-monitoring,openshift-multus,openshift-must-gather-operator,openshift-network-diagnostics,openshift-oauth-apiserver,openshift-ocm-agent-operator,openshift-operator-lifecycle-manager,openshift-osd-metrics,openshift-rbac-permissions,openshift-route-monitor-operator,openshift-sdn,openshift-service-ca-operator,openshift-user-workload-monitoring,openshift-validation-webhook,openshift-velero
          operatorframework.io/properties: '{"properties":[{"type":"olm.package","value":{"packageName":"configure-alertmanager-operator","version":"0.1.418-9e79f67"}}]}'
          support: OSD SRE
        creationTimestamp: null
        labels:
          name: configure-alertmanager-operator
          pod-template-hash: 6bf57f7cbc
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/infra
                  operator: Exists
              weight: 1
        containers:
        - command:
          - configure-alertmanager-operator
          env:
          - name: WATCH_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: OPERATOR_NAME
            value: configure-alertmanager-operator
          - name: FEDRAMP
            value: "false"
          - name: OPERATOR_IMAGE
            value: quay.io/app-sre/configure-alertmanager-operator@sha256:2253fe906430e59544adaa5a84ecfbfc18801ebc9856d759fb965ef9fbd46a4e
          - name: OPERATOR_CONDITION_NAME
            value: configure-alertmanager-operator.v0.1.418-9e79f67
          image: quay.io/app-sre/configure-alertmanager-operator@sha256:2253fe906430e59544adaa5a84ecfbfc18801ebc9856d759fb965ef9fbd46a4e
          imagePullPolicy: Always
          name: configure-alertmanager-operator
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: configure-alertmanager-operator
        serviceAccountName: configure-alertmanager-operator
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/infra
          operator: Exists
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2022-05-24T18:28:14Z"
    generation: 1
    labels:
      app.kubernetes.io/component: grafana
      app.kubernetes.io/managed-by: cluster-monitoring-operator
      app.kubernetes.io/name: grafana
      app.kubernetes.io/part-of: openshift-monitoring
      app.kubernetes.io/version: 8.3.4
      pod-template-hash: 56c5499d4d
    name: grafana-56c5499d4d
    namespace: openshift-monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: grafana
      uid: 24e60f35-fb0f-46a3-bbbb-ccadfeeb8966
    resourceVersion: "60597"
    uid: f6179ea5-e2d0-42ac-9f68-cee627677321
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: grafana
        app.kubernetes.io/name: grafana
        app.kubernetes.io/part-of: openshift-monitoring
        pod-template-hash: 56c5499d4d
    template:
      metadata:
        annotations:
          checksum/grafana-config: 28b9b36ac4d235c1c5f2f9ca81714d1c
          checksum/grafana-dashboardproviders: 9ac0e8fe144a3a59f7ab62b4e733f22d
          checksum/grafana-datasources: 58dfbb3df9951f2ac8acf4c625c7173c
          target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: grafana
          app.kubernetes.io/managed-by: cluster-monitoring-operator
          app.kubernetes.io/name: grafana
          app.kubernetes.io/part-of: openshift-monitoring
          app.kubernetes.io/version: 8.3.4
          pod-template-hash: 56c5499d4d
      spec:
        containers:
        - args:
          - -config=/etc/grafana/grafana.ini
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:39594b0dd96744cb8fa31cf0485f7bee06159dc08e489edbacba3b3062efbb68
          imagePullPolicy: IfNotPresent
          name: grafana
          ports:
          - containerPort: 3001
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 4m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/grafana
            name: grafana-storage
          - mountPath: /etc/grafana/provisioning/datasources
            name: grafana-datasources
          - mountPath: /etc/grafana/provisioning/dashboards
            name: grafana-dashboards
          - mountPath: /grafana-dashboard-definitions/0/cluster-total
            name: grafana-dashboard-cluster-total
          - mountPath: /grafana-dashboard-definitions/0/etcd
            name: grafana-dashboard-etcd
          - mountPath: /grafana-dashboard-definitions/0/k8s-resources-cluster
            name: grafana-dashboard-k8s-resources-cluster
          - mountPath: /grafana-dashboard-definitions/0/k8s-resources-namespace
            name: grafana-dashboard-k8s-resources-namespace
          - mountPath: /grafana-dashboard-definitions/0/k8s-resources-node
            name: grafana-dashboard-k8s-resources-node
          - mountPath: /grafana-dashboard-definitions/0/k8s-resources-pod
            name: grafana-dashboard-k8s-resources-pod
          - mountPath: /grafana-dashboard-definitions/0/k8s-resources-workload
            name: grafana-dashboard-k8s-resources-workload
          - mountPath: /grafana-dashboard-definitions/0/k8s-resources-workloads-namespace
            name: grafana-dashboard-k8s-resources-workloads-namespace
          - mountPath: /grafana-dashboard-definitions/0/namespace-by-pod
            name: grafana-dashboard-namespace-by-pod
          - mountPath: /grafana-dashboard-definitions/0/node-cluster-rsrc-use
            name: grafana-dashboard-node-cluster-rsrc-use
          - mountPath: /grafana-dashboard-definitions/0/node-rsrc-use
            name: grafana-dashboard-node-rsrc-use
          - mountPath: /grafana-dashboard-definitions/0/pod-total
            name: grafana-dashboard-pod-total
          - mountPath: /grafana-dashboard-definitions/0/prometheus
            name: grafana-dashboard-prometheus
          - mountPath: /etc/grafana
            name: grafana-config
        - args:
          - -provider=openshift
          - -https-address=:3000
          - -http-address=
          - -email-domain=*
          - -upstream=http://localhost:3001
          - '-openshift-sar={"resource": "namespaces", "verb": "get"}'
          - '-openshift-delegate-urls={"/": {"resource": "namespaces", "verb": "get"}}'
          - -tls-cert=/etc/tls/private/tls.crt
          - -tls-key=/etc/tls/private/tls.key
          - -client-secret-file=/var/run/secrets/kubernetes.io/serviceaccount/token
          - -cookie-secret-file=/etc/proxy/secrets/session_secret
          - -openshift-service-account=grafana
          - -openshift-ca=/etc/pki/tls/cert.pem
          - -openshift-ca=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          env:
          - name: HTTP_PROXY
          - name: HTTPS_PROXY
          - name: NO_PROXY
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d787f47ee2a410f924ea00b2428f0cf2275eb059adac96ca1b69c71ad20ccb1d
          imagePullPolicy: IfNotPresent
          name: grafana-proxy
          ports:
          - containerPort: 3000
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /oauth/healthz
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 1m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/tls/private
            name: secret-grafana-tls
          - mountPath: /etc/proxy/secrets
            name: secret-grafana-proxy
          - mountPath: /etc/pki/ca-trust/extracted/pem/
            name: grafana-trusted-ca-bundle
            readOnly: true
        - args:
          - --secure-listen-address=0.0.0.0:3002
          - --upstream=http://127.0.0.1:3001
          - --config-file=/etc/kube-rbac-proxy/config.yaml
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --client-ca-file=/etc/tls/client/client-ca.crt
          - --logtostderr=true
          - --allow-paths=/metrics
          - --tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:08e8b4004edaeeb125ced09ab2c4cd6d690afaf3a86309c91a994dec8e3ccbf3
          imagePullPolicy: IfNotPresent
          name: kube-rbac-proxy-metrics
          ports:
          - containerPort: 3002
            name: metrics
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 15Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/kube-rbac-proxy
            name: secret-grafana-kube-rbac-proxy-metric
            readOnly: true
          - mountPath: /etc/tls/private
            name: secret-grafana-tls
            readOnly: true
          - mountPath: /etc/tls/client
            name: metrics-client-ca
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          node-role.kubernetes.io/infra: ""
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: grafana
        serviceAccountName: grafana
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/infra
          operator: Exists
        volumes:
        - emptyDir: {}
          name: grafana-storage
        - name: grafana-datasources
          secret:
            defaultMode: 420
            secretName: grafana-datasources-v2
        - configMap:
            defaultMode: 420
            name: grafana-dashboards
          name: grafana-dashboards
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-cluster-total
          name: grafana-dashboard-cluster-total
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-etcd
          name: grafana-dashboard-etcd
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-k8s-resources-cluster
          name: grafana-dashboard-k8s-resources-cluster
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-k8s-resources-namespace
          name: grafana-dashboard-k8s-resources-namespace
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-k8s-resources-node
          name: grafana-dashboard-k8s-resources-node
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-k8s-resources-pod
          name: grafana-dashboard-k8s-resources-pod
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-k8s-resources-workload
          name: grafana-dashboard-k8s-resources-workload
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-k8s-resources-workloads-namespace
          name: grafana-dashboard-k8s-resources-workloads-namespace
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-namespace-by-pod
          name: grafana-dashboard-namespace-by-pod
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-node-cluster-rsrc-use
          name: grafana-dashboard-node-cluster-rsrc-use
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-node-rsrc-use
          name: grafana-dashboard-node-rsrc-use
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-pod-total
          name: grafana-dashboard-pod-total
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-prometheus
          name: grafana-dashboard-prometheus
        - name: grafana-config
          secret:
            defaultMode: 420
            secretName: grafana-config
        - name: secret-grafana-tls
          secret:
            defaultMode: 420
            secretName: grafana-tls
        - name: secret-grafana-kube-rbac-proxy-metric
          secret:
            defaultMode: 420
            secretName: grafana-kube-rbac-proxy-metric
        - name: secret-grafana-proxy
          secret:
            defaultMode: 420
            secretName: grafana-proxy
        - configMap:
            defaultMode: 420
            name: metrics-client-ca
          name: metrics-client-ca
        - configMap:
            defaultMode: 420
            items:
            - key: ca-bundle.crt
              path: tls-ca-bundle.pem
            name: grafana-trusted-ca-bundle-2rsonso43rc5p
            optional: true
          name: grafana-trusted-ca-bundle
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2022-05-24T18:13:22Z"
    generation: 2
    labels:
      app.kubernetes.io/component: grafana
      app.kubernetes.io/managed-by: cluster-monitoring-operator
      app.kubernetes.io/name: grafana
      app.kubernetes.io/part-of: openshift-monitoring
      app.kubernetes.io/version: 8.3.4
      pod-template-hash: 86b58bbc96
    name: grafana-86b58bbc96
    namespace: openshift-monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: grafana
      uid: 24e60f35-fb0f-46a3-bbbb-ccadfeeb8966
    resourceVersion: "42175"
    uid: a115092b-2623-4264-9659-f0aba1c51436
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: grafana
        app.kubernetes.io/name: grafana
        app.kubernetes.io/part-of: openshift-monitoring
        pod-template-hash: 86b58bbc96
    template:
      metadata:
        annotations:
          checksum/grafana-config: 28b9b36ac4d235c1c5f2f9ca81714d1c
          checksum/grafana-dashboardproviders: 9ac0e8fe144a3a59f7ab62b4e733f22d
          checksum/grafana-datasources: 58dfbb3df9951f2ac8acf4c625c7173c
          target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: grafana
          app.kubernetes.io/managed-by: cluster-monitoring-operator
          app.kubernetes.io/name: grafana
          app.kubernetes.io/part-of: openshift-monitoring
          app.kubernetes.io/version: 8.3.4
          pod-template-hash: 86b58bbc96
      spec:
        containers:
        - args:
          - -config=/etc/grafana/grafana.ini
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:39594b0dd96744cb8fa31cf0485f7bee06159dc08e489edbacba3b3062efbb68
          imagePullPolicy: IfNotPresent
          name: grafana
          ports:
          - containerPort: 3001
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 4m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/grafana
            name: grafana-storage
          - mountPath: /etc/grafana/provisioning/datasources
            name: grafana-datasources
          - mountPath: /etc/grafana/provisioning/dashboards
            name: grafana-dashboards
          - mountPath: /grafana-dashboard-definitions/0/cluster-total
            name: grafana-dashboard-cluster-total
          - mountPath: /grafana-dashboard-definitions/0/etcd
            name: grafana-dashboard-etcd
          - mountPath: /grafana-dashboard-definitions/0/k8s-resources-cluster
            name: grafana-dashboard-k8s-resources-cluster
          - mountPath: /grafana-dashboard-definitions/0/k8s-resources-namespace
            name: grafana-dashboard-k8s-resources-namespace
          - mountPath: /grafana-dashboard-definitions/0/k8s-resources-node
            name: grafana-dashboard-k8s-resources-node
          - mountPath: /grafana-dashboard-definitions/0/k8s-resources-pod
            name: grafana-dashboard-k8s-resources-pod
          - mountPath: /grafana-dashboard-definitions/0/k8s-resources-workload
            name: grafana-dashboard-k8s-resources-workload
          - mountPath: /grafana-dashboard-definitions/0/k8s-resources-workloads-namespace
            name: grafana-dashboard-k8s-resources-workloads-namespace
          - mountPath: /grafana-dashboard-definitions/0/namespace-by-pod
            name: grafana-dashboard-namespace-by-pod
          - mountPath: /grafana-dashboard-definitions/0/node-cluster-rsrc-use
            name: grafana-dashboard-node-cluster-rsrc-use
          - mountPath: /grafana-dashboard-definitions/0/node-rsrc-use
            name: grafana-dashboard-node-rsrc-use
          - mountPath: /grafana-dashboard-definitions/0/pod-total
            name: grafana-dashboard-pod-total
          - mountPath: /grafana-dashboard-definitions/0/prometheus
            name: grafana-dashboard-prometheus
          - mountPath: /etc/grafana
            name: grafana-config
        - args:
          - -provider=openshift
          - -https-address=:3000
          - -http-address=
          - -email-domain=*
          - -upstream=http://localhost:3001
          - '-openshift-sar={"resource": "namespaces", "verb": "get"}'
          - '-openshift-delegate-urls={"/": {"resource": "namespaces", "verb": "get"}}'
          - -tls-cert=/etc/tls/private/tls.crt
          - -tls-key=/etc/tls/private/tls.key
          - -client-secret-file=/var/run/secrets/kubernetes.io/serviceaccount/token
          - -cookie-secret-file=/etc/proxy/secrets/session_secret
          - -openshift-service-account=grafana
          - -openshift-ca=/etc/pki/tls/cert.pem
          - -openshift-ca=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          env:
          - name: HTTP_PROXY
          - name: HTTPS_PROXY
          - name: NO_PROXY
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d787f47ee2a410f924ea00b2428f0cf2275eb059adac96ca1b69c71ad20ccb1d
          imagePullPolicy: IfNotPresent
          name: grafana-proxy
          ports:
          - containerPort: 3000
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /oauth/healthz
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 1m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/tls/private
            name: secret-grafana-tls
          - mountPath: /etc/proxy/secrets
            name: secret-grafana-proxy
          - mountPath: /etc/pki/ca-trust/extracted/pem/
            name: grafana-trusted-ca-bundle
            readOnly: true
        - args:
          - --secure-listen-address=0.0.0.0:3002
          - --upstream=http://127.0.0.1:3001
          - --config-file=/etc/kube-rbac-proxy/config.yaml
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --client-ca-file=/etc/tls/client/client-ca.crt
          - --logtostderr=true
          - --allow-paths=/metrics
          - --tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:08e8b4004edaeeb125ced09ab2c4cd6d690afaf3a86309c91a994dec8e3ccbf3
          imagePullPolicy: IfNotPresent
          name: kube-rbac-proxy-metrics
          ports:
          - containerPort: 3002
            name: metrics
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 15Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/kube-rbac-proxy
            name: secret-grafana-kube-rbac-proxy-metric
            readOnly: true
          - mountPath: /etc/tls/private
            name: secret-grafana-tls
            readOnly: true
          - mountPath: /etc/tls/client
            name: metrics-client-ca
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: grafana
        serviceAccountName: grafana
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: grafana-storage
        - name: grafana-datasources
          secret:
            defaultMode: 420
            secretName: grafana-datasources-v2
        - configMap:
            defaultMode: 420
            name: grafana-dashboards
          name: grafana-dashboards
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-cluster-total
          name: grafana-dashboard-cluster-total
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-etcd
          name: grafana-dashboard-etcd
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-k8s-resources-cluster
          name: grafana-dashboard-k8s-resources-cluster
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-k8s-resources-namespace
          name: grafana-dashboard-k8s-resources-namespace
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-k8s-resources-node
          name: grafana-dashboard-k8s-resources-node
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-k8s-resources-pod
          name: grafana-dashboard-k8s-resources-pod
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-k8s-resources-workload
          name: grafana-dashboard-k8s-resources-workload
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-k8s-resources-workloads-namespace
          name: grafana-dashboard-k8s-resources-workloads-namespace
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-namespace-by-pod
          name: grafana-dashboard-namespace-by-pod
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-node-cluster-rsrc-use
          name: grafana-dashboard-node-cluster-rsrc-use
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-node-rsrc-use
          name: grafana-dashboard-node-rsrc-use
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-pod-total
          name: grafana-dashboard-pod-total
        - configMap:
            defaultMode: 420
            name: grafana-dashboard-prometheus
          name: grafana-dashboard-prometheus
        - name: grafana-config
          secret:
            defaultMode: 420
            secretName: grafana-config
        - name: secret-grafana-tls
          secret:
            defaultMode: 420
            secretName: grafana-tls
        - name: secret-grafana-kube-rbac-proxy-metric
          secret:
            defaultMode: 420
            secretName: grafana-kube-rbac-proxy-metric
        - name: secret-grafana-proxy
          secret:
            defaultMode: 420
            secretName: grafana-proxy
        - configMap:
            defaultMode: 420
            name: metrics-client-ca
          name: metrics-client-ca
        - configMap:
            defaultMode: 420
            items:
            - key: ca-bundle.crt
              path: tls-ca-bundle.pem
            name: grafana-trusted-ca-bundle-2rsonso43rc5p
            optional: true
          name: grafana-trusted-ca-bundle
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2022-05-24T18:28:13Z"
    generation: 1
    labels:
      app.kubernetes.io/component: exporter
      app.kubernetes.io/managed-by: cluster-monitoring-operator
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: openshift-monitoring
      app.kubernetes.io/version: 2.3.0
      pod-template-hash: 597568c675
    name: kube-state-metrics-597568c675
    namespace: openshift-monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-state-metrics
      uid: 058cc139-8ab1-4981-80d5-44b936ab99d8
    resourceVersion: "60511"
    uid: 4720d6f1-62d0-4db8-a3ae-d327ac9e11ee
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: exporter
        app.kubernetes.io/name: kube-state-metrics
        app.kubernetes.io/part-of: openshift-monitoring
        pod-template-hash: 597568c675
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/default-container: kube-state-metrics
          target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: exporter
          app.kubernetes.io/managed-by: cluster-monitoring-operator
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: openshift-monitoring
          app.kubernetes.io/version: 2.3.0
          pod-template-hash: 597568c675
      spec:
        containers:
        - args:
          - --host=127.0.0.1
          - --port=8081
          - --telemetry-host=127.0.0.1
          - --telemetry-port=8082
          - --metric-denylist=kube_secret_labels,kube_.*_annotations
          - --metric-labels-allowlist=pods=[*],nodes=[*],namespaces=[*],persistentvolumes=[*],persistentvolumeclaims=[*],poddisruptionbudgets=[*],poddisruptionbudget=[*]
          - |
            --metric-denylist=
            kube_.+_created,
            kube_.+_metadata_resource_version,
            kube_replicaset_metadata_generation,
            kube_replicaset_status_observed_generation,
            kube_pod_restart_policy,
            kube_pod_init_container_status_terminated,
            kube_pod_init_container_status_running,
            kube_pod_container_status_terminated,
            kube_pod_container_status_running,
            kube_pod_completion_time,
            kube_pod_status_scheduled
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3b2f92612cf6a35edaade46270a961653eb53152a1217fa74be685ac3cfa8b71
          imagePullPolicy: IfNotPresent
          name: kube-state-metrics
          resources:
            requests:
              cpu: 2m
              memory: 80Mi
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: volume-directive-shadow
        - args:
          - --logtostderr
          - --secure-listen-address=:8443
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --upstream=http://127.0.0.1:8081/
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          - --client-ca-file=/etc/tls/client/client-ca.crt
          - --config-file=/etc/kube-rbac-policy/config.yaml
          - --tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:08e8b4004edaeeb125ced09ab2c4cd6d690afaf3a86309c91a994dec8e3ccbf3
          imagePullPolicy: IfNotPresent
          name: kube-rbac-proxy-main
          ports:
          - containerPort: 8443
            name: https-main
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 15Mi
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/tls/private
            name: kube-state-metrics-tls
          - mountPath: /etc/tls/client
            name: metrics-client-ca
          - mountPath: /etc/kube-rbac-policy
            name: kube-state-metrics-kube-rbac-proxy-config
            readOnly: true
        - args:
          - --logtostderr
          - --secure-listen-address=:9443
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --upstream=http://127.0.0.1:8082/
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          - --client-ca-file=/etc/tls/client/client-ca.crt
          - --config-file=/etc/kube-rbac-policy/config.yaml
          - --tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:08e8b4004edaeeb125ced09ab2c4cd6d690afaf3a86309c91a994dec8e3ccbf3
          imagePullPolicy: IfNotPresent
          name: kube-rbac-proxy-self
          ports:
          - containerPort: 9443
            name: https-self
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 15Mi
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/tls/private
            name: kube-state-metrics-tls
          - mountPath: /etc/tls/client
            name: metrics-client-ca
          - mountPath: /etc/kube-rbac-policy
            name: kube-state-metrics-kube-rbac-proxy-config
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          node-role.kubernetes.io/infra: ""
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kube-state-metrics
        serviceAccountName: kube-state-metrics
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/infra
          operator: Exists
        volumes:
        - emptyDir: {}
          name: volume-directive-shadow
        - name: kube-state-metrics-tls
          secret:
            defaultMode: 420
            secretName: kube-state-metrics-tls
        - configMap:
            defaultMode: 420
            name: metrics-client-ca
          name: metrics-client-ca
        - name: kube-state-metrics-kube-rbac-proxy-config
          secret:
            defaultMode: 420
            secretName: kube-state-metrics-kube-rbac-proxy-config
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2022-05-24T18:08:56Z"
    generation: 2
    labels:
      app.kubernetes.io/component: exporter
      app.kubernetes.io/managed-by: cluster-monitoring-operator
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: openshift-monitoring
      app.kubernetes.io/version: 2.3.0
      pod-template-hash: 7d65ff6c55
    name: kube-state-metrics-7d65ff6c55
    namespace: openshift-monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-state-metrics
      uid: 058cc139-8ab1-4981-80d5-44b936ab99d8
    resourceVersion: "41400"
    uid: 74f0ef8d-bfb5-4e12-95cb-4b691ec529bd
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: exporter
        app.kubernetes.io/name: kube-state-metrics
        app.kubernetes.io/part-of: openshift-monitoring
        pod-template-hash: 7d65ff6c55
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/default-container: kube-state-metrics
          target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: exporter
          app.kubernetes.io/managed-by: cluster-monitoring-operator
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: openshift-monitoring
          app.kubernetes.io/version: 2.3.0
          pod-template-hash: 7d65ff6c55
      spec:
        containers:
        - args:
          - --host=127.0.0.1
          - --port=8081
          - --telemetry-host=127.0.0.1
          - --telemetry-port=8082
          - --metric-denylist=kube_secret_labels,kube_.*_annotations
          - --metric-labels-allowlist=pods=[*],nodes=[*],namespaces=[*],persistentvolumes=[*],persistentvolumeclaims=[*],poddisruptionbudgets=[*],poddisruptionbudget=[*]
          - |
            --metric-denylist=
            kube_.+_created,
            kube_.+_metadata_resource_version,
            kube_replicaset_metadata_generation,
            kube_replicaset_status_observed_generation,
            kube_pod_restart_policy,
            kube_pod_init_container_status_terminated,
            kube_pod_init_container_status_running,
            kube_pod_container_status_terminated,
            kube_pod_container_status_running,
            kube_pod_completion_time,
            kube_pod_status_scheduled
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3b2f92612cf6a35edaade46270a961653eb53152a1217fa74be685ac3cfa8b71
          imagePullPolicy: IfNotPresent
          name: kube-state-metrics
          resources:
            requests:
              cpu: 2m
              memory: 80Mi
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: volume-directive-shadow
        - args:
          - --logtostderr
          - --secure-listen-address=:8443
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --upstream=http://127.0.0.1:8081/
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          - --client-ca-file=/etc/tls/client/client-ca.crt
          - --config-file=/etc/kube-rbac-policy/config.yaml
          - --tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:08e8b4004edaeeb125ced09ab2c4cd6d690afaf3a86309c91a994dec8e3ccbf3
          imagePullPolicy: IfNotPresent
          name: kube-rbac-proxy-main
          ports:
          - containerPort: 8443
            name: https-main
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 15Mi
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/tls/private
            name: kube-state-metrics-tls
          - mountPath: /etc/tls/client
            name: metrics-client-ca
          - mountPath: /etc/kube-rbac-policy
            name: kube-state-metrics-kube-rbac-proxy-config
            readOnly: true
        - args:
          - --logtostderr
          - --secure-listen-address=:9443
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --upstream=http://127.0.0.1:8082/
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          - --client-ca-file=/etc/tls/client/client-ca.crt
          - --config-file=/etc/kube-rbac-policy/config.yaml
          - --tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:08e8b4004edaeeb125ced09ab2c4cd6d690afaf3a86309c91a994dec8e3ccbf3
          imagePullPolicy: IfNotPresent
          name: kube-rbac-proxy-self
          ports:
          - containerPort: 9443
            name: https-self
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 15Mi
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/tls/private
            name: kube-state-metrics-tls
          - mountPath: /etc/tls/client
            name: metrics-client-ca
          - mountPath: /etc/kube-rbac-policy
            name: kube-state-metrics-kube-rbac-proxy-config
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kube-state-metrics
        serviceAccountName: kube-state-metrics
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: volume-directive-shadow
        - name: kube-state-metrics-tls
          secret:
            defaultMode: 420
            secretName: kube-state-metrics-tls
        - configMap:
            defaultMode: 420
            name: metrics-client-ca
          name: metrics-client-ca
        - name: kube-state-metrics-kube-rbac-proxy-config
          secret:
            defaultMode: 420
            secretName: kube-state-metrics-kube-rbac-proxy-config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2022-05-24T18:28:13Z"
    generation: 1
    labels:
      app.kubernetes.io/component: exporter
      app.kubernetes.io/managed-by: cluster-monitoring-operator
      app.kubernetes.io/name: openshift-state-metrics
      app.kubernetes.io/part-of: openshift-monitoring
      pod-template-hash: 559868fdcc
    name: openshift-state-metrics-559868fdcc
    namespace: openshift-monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: openshift-state-metrics
      uid: 664bfe80-4f09-4001-9288-32752dcfcc29
    resourceVersion: "60527"
    uid: 7b87b2cf-d20c-4c0b-b57a-ed02045f6c85
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: exporter
        app.kubernetes.io/name: openshift-state-metrics
        pod-template-hash: 559868fdcc
    template:
      metadata:
        annotations:
          target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: exporter
          app.kubernetes.io/managed-by: cluster-monitoring-operator
          app.kubernetes.io/name: openshift-state-metrics
          app.kubernetes.io/part-of: openshift-monitoring
          pod-template-hash: 559868fdcc
      spec:
        containers:
        - args:
          - --logtostderr
          - --secure-listen-address=:8443
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --upstream=http://127.0.0.1:8081/
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          - --config-file=/etc/kube-rbac-policy/config.yaml
          - --tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:08e8b4004edaeeb125ced09ab2c4cd6d690afaf3a86309c91a994dec8e3ccbf3
          imagePullPolicy: IfNotPresent
          name: kube-rbac-proxy-main
          ports:
          - containerPort: 8443
            name: https-main
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/tls/private
            name: openshift-state-metrics-tls
          - mountPath: /etc/kube-rbac-policy
            name: openshift-state-metrics-kube-rbac-proxy-config
            readOnly: true
        - args:
          - --logtostderr
          - --secure-listen-address=:9443
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --upstream=http://127.0.0.1:8082/
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          - --config-file=/etc/kube-rbac-policy/config.yaml
          - --tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:08e8b4004edaeeb125ced09ab2c4cd6d690afaf3a86309c91a994dec8e3ccbf3
          imagePullPolicy: IfNotPresent
          name: kube-rbac-proxy-self
          ports:
          - containerPort: 9443
            name: https-self
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/tls/private
            name: openshift-state-metrics-tls
          - mountPath: /etc/kube-rbac-policy
            name: openshift-state-metrics-kube-rbac-proxy-config
            readOnly: true
        - args:
          - --host=127.0.0.1
          - --port=8081
          - --telemetry-host=127.0.0.1
          - --telemetry-port=8082
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e52032d4e2993366d3b3e6fc81837b2cd79f1a8be5638b9cf409a9aa7547495a
          imagePullPolicy: IfNotPresent
          name: openshift-state-metrics
          resources:
            requests:
              cpu: 1m
              memory: 32Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          node-role.kubernetes.io/infra: ""
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: openshift-state-metrics
        serviceAccountName: openshift-state-metrics
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/infra
          operator: Exists
        volumes:
        - name: openshift-state-metrics-tls
          secret:
            defaultMode: 420
            secretName: openshift-state-metrics-tls
        - name: openshift-state-metrics-kube-rbac-proxy-config
          secret:
            defaultMode: 420
            secretName: openshift-state-metrics-kube-rbac-proxy-config
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2022-05-24T18:08:56Z"
    generation: 2
    labels:
      app.kubernetes.io/component: exporter
      app.kubernetes.io/managed-by: cluster-monitoring-operator
      app.kubernetes.io/name: openshift-state-metrics
      app.kubernetes.io/part-of: openshift-monitoring
      pod-template-hash: 9cf54c74f
    name: openshift-state-metrics-9cf54c74f
    namespace: openshift-monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: openshift-state-metrics
      uid: 664bfe80-4f09-4001-9288-32752dcfcc29
    resourceVersion: "41225"
    uid: 71144b6a-e5f0-49bc-85d5-b28f92d341a0
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: exporter
        app.kubernetes.io/name: openshift-state-metrics
        pod-template-hash: 9cf54c74f
    template:
      metadata:
        annotations:
          target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: exporter
          app.kubernetes.io/managed-by: cluster-monitoring-operator
          app.kubernetes.io/name: openshift-state-metrics
          app.kubernetes.io/part-of: openshift-monitoring
          pod-template-hash: 9cf54c74f
      spec:
        containers:
        - args:
          - --logtostderr
          - --secure-listen-address=:8443
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --upstream=http://127.0.0.1:8081/
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          - --config-file=/etc/kube-rbac-policy/config.yaml
          - --tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:08e8b4004edaeeb125ced09ab2c4cd6d690afaf3a86309c91a994dec8e3ccbf3
          imagePullPolicy: IfNotPresent
          name: kube-rbac-proxy-main
          ports:
          - containerPort: 8443
            name: https-main
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/tls/private
            name: openshift-state-metrics-tls
          - mountPath: /etc/kube-rbac-policy
            name: openshift-state-metrics-kube-rbac-proxy-config
            readOnly: true
        - args:
          - --logtostderr
          - --secure-listen-address=:9443
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --upstream=http://127.0.0.1:8082/
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          - --config-file=/etc/kube-rbac-policy/config.yaml
          - --tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:08e8b4004edaeeb125ced09ab2c4cd6d690afaf3a86309c91a994dec8e3ccbf3
          imagePullPolicy: IfNotPresent
          name: kube-rbac-proxy-self
          ports:
          - containerPort: 9443
            name: https-self
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/tls/private
            name: openshift-state-metrics-tls
          - mountPath: /etc/kube-rbac-policy
            name: openshift-state-metrics-kube-rbac-proxy-config
            readOnly: true
        - args:
          - --host=127.0.0.1
          - --port=8081
          - --telemetry-host=127.0.0.1
          - --telemetry-port=8082
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e52032d4e2993366d3b3e6fc81837b2cd79f1a8be5638b9cf409a9aa7547495a
          imagePullPolicy: IfNotPresent
          name: openshift-state-metrics
          resources:
            requests:
              cpu: 1m
              memory: 32Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: openshift-state-metrics
        serviceAccountName: openshift-state-metrics
        terminationGracePeriodSeconds: 30
        volumes:
        - name: openshift-state-metrics-tls
          secret:
            defaultMode: 420
            secretName: openshift-state-metrics-tls
        - name: openshift-state-metrics-kube-rbac-proxy-config
          secret:
            defaultMode: 420
            secretName: openshift-state-metrics-kube-rbac-proxy-config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2022-05-24T18:13:09Z"
    generation: 3
    labels:
      app.kubernetes.io/component: metrics-adapter
      app.kubernetes.io/managed-by: cluster-monitoring-operator
      app.kubernetes.io/name: prometheus-adapter
      app.kubernetes.io/part-of: openshift-monitoring
      app.kubernetes.io/version: 0.9.1
      pod-template-hash: 566db5fdc7
    name: prometheus-adapter-566db5fdc7
    namespace: openshift-monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-adapter
      uid: d260c4f4-3016-4040-9cd3-48df3a86f9fc
    resourceVersion: "41718"
    uid: 70e8a366-899f-46f6-bd8e-be8be4997fdd
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: metrics-adapter
        app.kubernetes.io/name: prometheus-adapter
        app.kubernetes.io/part-of: openshift-monitoring
        pod-template-hash: 566db5fdc7
    template:
      metadata:
        annotations:
          target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics-adapter
          app.kubernetes.io/managed-by: cluster-monitoring-operator
          app.kubernetes.io/name: prometheus-adapter
          app.kubernetes.io/part-of: openshift-monitoring
          app.kubernetes.io/version: 0.9.1
          pod-template-hash: 566db5fdc7
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/component: metrics-adapter
                  app.kubernetes.io/name: prometheus-adapter
                  app.kubernetes.io/part-of: openshift-monitoring
              namespaces:
              - openshift-monitoring
              topologyKey: kubernetes.io/hostname
        containers:
        - args:
          - --prometheus-auth-config=/etc/prometheus-config/prometheus-config.yaml
          - --config=/etc/adapter/config.yaml
          - --logtostderr=true
          - --metrics-relist-interval=1m
          - --prometheus-url=https://thanos-querier.openshift-monitoring.svc:9091
          - --secure-port=6443
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --client-ca-file=/etc/tls/private/client-ca-file
          - --requestheader-client-ca-file=/etc/tls/private/requestheader-client-ca-file
          - --requestheader-allowed-names=kube-apiserver-proxy,system:kube-apiserver-proxy,system:openshift-aggregator
          - --requestheader-extra-headers-prefix=X-Remote-Extra-
          - --requestheader-group-headers=X-Remote-Group
          - --requestheader-username-headers=X-Remote-User
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          - --audit-policy-file=/etc/audit/metadata-profile.yaml
          - --audit-log-path=/var/log/adapter/audit.log
          - --audit-log-maxsize=100
          - --audit-log-maxbackup=5
          - --audit-log-compress=true
          - --tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:750e142e941a9b9e1822c52bc671d01ebcb5bf479b42fab978b76c44b17685b5
          imagePullPolicy: IfNotPresent
          name: prometheus-adapter
          ports:
          - containerPort: 6443
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 40Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /tmp
            name: tmpfs
          - mountPath: /etc/adapter
            name: config
          - mountPath: /etc/prometheus-config
            name: prometheus-adapter-prometheus-config
          - mountPath: /etc/ssl/certs
            name: serving-certs-ca-bundle
          - mountPath: /etc/audit
            name: prometheus-adapter-audit-profiles
            readOnly: true
          - mountPath: /var/log/adapter
            name: audit-log
          - mountPath: /etc/tls/private
            name: tls
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: prometheus-adapter
        serviceAccountName: prometheus-adapter
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmpfs
        - configMap:
            defaultMode: 420
            name: adapter-config
          name: config
        - configMap:
            defaultMode: 420
            name: prometheus-adapter-prometheus-config
          name: prometheus-adapter-prometheus-config
        - configMap:
            defaultMode: 420
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: serving-certs-ca-bundle
          name: serving-certs-ca-bundle
        - emptyDir: {}
          name: audit-log
        - configMap:
            defaultMode: 420
            name: prometheus-adapter-audit-profiles
          name: prometheus-adapter-audit-profiles
        - name: tls
          secret:
            defaultMode: 420
            secretName: prometheus-adapter-cggn4tgrdm7bp
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2022-05-24T18:28:13Z"
    generation: 2
    labels:
      app.kubernetes.io/component: metrics-adapter
      app.kubernetes.io/managed-by: cluster-monitoring-operator
      app.kubernetes.io/name: prometheus-adapter
      app.kubernetes.io/part-of: openshift-monitoring
      app.kubernetes.io/version: 0.9.1
      pod-template-hash: 58f5b7f7c
    name: prometheus-adapter-58f5b7f7c
    namespace: openshift-monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-adapter
      uid: d260c4f4-3016-4040-9cd3-48df3a86f9fc
    resourceVersion: "70598"
    uid: ce4afd94-9551-4818-b292-7da2676afaf4
  spec:
    replicas: 2
    selector:
      matchLabels:
        app.kubernetes.io/component: metrics-adapter
        app.kubernetes.io/name: prometheus-adapter
        app.kubernetes.io/part-of: openshift-monitoring
        pod-template-hash: 58f5b7f7c
    template:
      metadata:
        annotations:
          target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics-adapter
          app.kubernetes.io/managed-by: cluster-monitoring-operator
          app.kubernetes.io/name: prometheus-adapter
          app.kubernetes.io/part-of: openshift-monitoring
          app.kubernetes.io/version: 0.9.1
          pod-template-hash: 58f5b7f7c
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/component: metrics-adapter
                  app.kubernetes.io/name: prometheus-adapter
                  app.kubernetes.io/part-of: openshift-monitoring
              namespaces:
              - openshift-monitoring
              topologyKey: kubernetes.io/hostname
        containers:
        - args:
          - --prometheus-auth-config=/etc/prometheus-config/prometheus-config.yaml
          - --config=/etc/adapter/config.yaml
          - --logtostderr=true
          - --metrics-relist-interval=1m
          - --prometheus-url=https://thanos-querier.openshift-monitoring.svc:9091
          - --secure-port=6443
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --client-ca-file=/etc/tls/private/client-ca-file
          - --requestheader-client-ca-file=/etc/tls/private/requestheader-client-ca-file
          - --requestheader-allowed-names=kube-apiserver-proxy,system:kube-apiserver-proxy,system:openshift-aggregator
          - --requestheader-extra-headers-prefix=X-Remote-Extra-
          - --requestheader-group-headers=X-Remote-Group
          - --requestheader-username-headers=X-Remote-User
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          - --audit-policy-file=/etc/audit/metadata-profile.yaml
          - --audit-log-path=/var/log/adapter/audit.log
          - --audit-log-maxsize=100
          - --audit-log-maxbackup=5
          - --audit-log-compress=true
          - --tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:750e142e941a9b9e1822c52bc671d01ebcb5bf479b42fab978b76c44b17685b5
          imagePullPolicy: IfNotPresent
          name: prometheus-adapter
          ports:
          - containerPort: 6443
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 40Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /tmp
            name: tmpfs
          - mountPath: /etc/adapter
            name: config
          - mountPath: /etc/prometheus-config
            name: prometheus-adapter-prometheus-config
          - mountPath: /etc/ssl/certs
            name: serving-certs-ca-bundle
          - mountPath: /etc/audit
            name: prometheus-adapter-audit-profiles
            readOnly: true
          - mountPath: /var/log/adapter
            name: audit-log
          - mountPath: /etc/tls/private
            name: tls
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          node-role.kubernetes.io/infra: ""
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: prometheus-adapter
        serviceAccountName: prometheus-adapter
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/infra
          operator: Exists
        volumes:
        - emptyDir: {}
          name: tmpfs
        - configMap:
            defaultMode: 420
            name: adapter-config
          name: config
        - configMap:
            defaultMode: 420
            name: prometheus-adapter-prometheus-config
          name: prometheus-adapter-prometheus-config
        - configMap:
            defaultMode: 420
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: serving-certs-ca-bundle
          name: serving-certs-ca-bundle
        - emptyDir: {}
          name: audit-log
        - configMap:
            defaultMode: 420
            name: prometheus-adapter-audit-profiles
          name: prometheus-adapter-audit-profiles
        - name: tls
          secret:
            defaultMode: 420
            secretName: prometheus-adapter-cggn4tgrdm7bp
  status:
    availableReplicas: 2
    fullyLabeledReplicas: 2
    observedGeneration: 2
    readyReplicas: 2
    replicas: 2
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2022-05-24T18:25:30Z"
    generation: 1
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/managed-by: cluster-monitoring-operator
      app.kubernetes.io/name: prometheus-operator
      app.kubernetes.io/part-of: openshift-monitoring
      app.kubernetes.io/version: 0.53.1
      pod-template-hash: 599c4fbfc
    name: prometheus-operator-599c4fbfc
    namespace: openshift-monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-operator
      uid: 63a5a427-1f2f-4883-9a9e-818bc70210fa
    resourceVersion: "60522"
    uid: e7d3ed62-b9fb-413d-b3de-edd4e8f41bca
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/name: prometheus-operator
        app.kubernetes.io/part-of: openshift-monitoring
        pod-template-hash: 599c4fbfc
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/default-container: prometheus-operator
          target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: controller
          app.kubernetes.io/managed-by: cluster-monitoring-operator
          app.kubernetes.io/name: prometheus-operator
          app.kubernetes.io/part-of: openshift-monitoring
          app.kubernetes.io/version: 0.53.1
          pod-template-hash: 599c4fbfc
      spec:
        containers:
        - args:
          - --kubelet-service=kube-system/kubelet
          - --prometheus-config-reloader=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b5d406a68f3fb45008977ea6f4d1cda50c8a878cf0055c11308b1785d5a06023
          - --prometheus-instance-namespaces=openshift-monitoring
          - --thanos-ruler-instance-namespaces=openshift-monitoring
          - --alertmanager-instance-namespaces=openshift-monitoring
          - --config-reloader-cpu-limit=0
          - --config-reloader-memory-limit=0
          - --web.enable-tls=true
          - --web.tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --web.tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6332f67ddb8aaba08639689ea91e5a0075e07dbfaea704a8a2b4cb7645281136
          imagePullPolicy: IfNotPresent
          name: prometheus-operator
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 5m
              memory: 150Mi
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/tls/private
            name: prometheus-operator-tls
        - args:
          - --logtostderr
          - --secure-listen-address=:8443
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --upstream=https://prometheus-operator.openshift-monitoring.svc:8080/
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          - --client-ca-file=/etc/tls/client/client-ca.crt
          - --upstream-ca-file=/etc/configmaps/operator-cert-ca-bundle/service-ca.crt
          - --config-file=/etc/kube-rbac-policy/config.yaml
          - --tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:08e8b4004edaeeb125ced09ab2c4cd6d690afaf3a86309c91a994dec8e3ccbf3
          imagePullPolicy: IfNotPresent
          name: kube-rbac-proxy
          ports:
          - containerPort: 8443
            name: https
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 15Mi
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/tls/private
            name: prometheus-operator-tls
          - mountPath: /etc/configmaps/operator-cert-ca-bundle
            name: operator-certs-ca-bundle
          - mountPath: /etc/tls/client
            name: metrics-client-ca
          - mountPath: /etc/kube-rbac-policy
            name: prometheus-operator-kube-rbac-proxy-config
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          node-role.kubernetes.io/infra: ""
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: prometheus-operator
        serviceAccountName: prometheus-operator
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/infra
          operator: Exists
        volumes:
        - name: prometheus-operator-tls
          secret:
            defaultMode: 420
            secretName: prometheus-operator-tls
        - configMap:
            defaultMode: 420
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: operator-certs-ca-bundle
          name: operator-certs-ca-bundle
        - name: prometheus-operator-kube-rbac-proxy-config
          secret:
            defaultMode: 420
            secretName: prometheus-operator-kube-rbac-proxy-config
        - configMap:
            defaultMode: 420
            name: metrics-client-ca
          name: metrics-client-ca
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2022-05-24T18:08:35Z"
    generation: 2
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/managed-by: cluster-monitoring-operator
      app.kubernetes.io/name: prometheus-operator
      app.kubernetes.io/part-of: openshift-monitoring
      app.kubernetes.io/version: 0.53.1
      pod-template-hash: 5d6bd547b8
    name: prometheus-operator-5d6bd547b8
    namespace: openshift-monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus-operator
      uid: 63a5a427-1f2f-4883-9a9e-818bc70210fa
    resourceVersion: "40231"
    uid: 47b55075-37f8-4666-90a5-8a6f3cbb3ce8
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/name: prometheus-operator
        app.kubernetes.io/part-of: openshift-monitoring
        pod-template-hash: 5d6bd547b8
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/default-container: prometheus-operator
          target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: controller
          app.kubernetes.io/managed-by: cluster-monitoring-operator
          app.kubernetes.io/name: prometheus-operator
          app.kubernetes.io/part-of: openshift-monitoring
          app.kubernetes.io/version: 0.53.1
          pod-template-hash: 5d6bd547b8
      spec:
        containers:
        - args:
          - --kubelet-service=kube-system/kubelet
          - --prometheus-config-reloader=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b5d406a68f3fb45008977ea6f4d1cda50c8a878cf0055c11308b1785d5a06023
          - --prometheus-instance-namespaces=openshift-monitoring
          - --thanos-ruler-instance-namespaces=openshift-monitoring
          - --alertmanager-instance-namespaces=openshift-monitoring
          - --config-reloader-cpu-limit=0
          - --config-reloader-memory-limit=0
          - --web.enable-tls=true
          - --web.tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --web.tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6332f67ddb8aaba08639689ea91e5a0075e07dbfaea704a8a2b4cb7645281136
          imagePullPolicy: IfNotPresent
          name: prometheus-operator
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 5m
              memory: 150Mi
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/tls/private
            name: prometheus-operator-tls
        - args:
          - --logtostderr
          - --secure-listen-address=:8443
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --upstream=https://prometheus-operator.openshift-monitoring.svc:8080/
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          - --client-ca-file=/etc/tls/client/client-ca.crt
          - --upstream-ca-file=/etc/configmaps/operator-cert-ca-bundle/service-ca.crt
          - --config-file=/etc/kube-rbac-policy/config.yaml
          - --tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:08e8b4004edaeeb125ced09ab2c4cd6d690afaf3a86309c91a994dec8e3ccbf3
          imagePullPolicy: IfNotPresent
          name: kube-rbac-proxy
          ports:
          - containerPort: 8443
            name: https
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 15Mi
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/tls/private
            name: prometheus-operator-tls
          - mountPath: /etc/configmaps/operator-cert-ca-bundle
            name: operator-certs-ca-bundle
          - mountPath: /etc/tls/client
            name: metrics-client-ca
          - mountPath: /etc/kube-rbac-policy
            name: prometheus-operator-kube-rbac-proxy-config
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
          node-role.kubernetes.io/master: ""
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: prometheus-operator
        serviceAccountName: prometheus-operator
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        volumes:
        - name: prometheus-operator-tls
          secret:
            defaultMode: 420
            secretName: prometheus-operator-tls
        - configMap:
            defaultMode: 420
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: operator-certs-ca-bundle
          name: operator-certs-ca-bundle
        - name: prometheus-operator-kube-rbac-proxy-config
          secret:
            defaultMode: 420
            secretName: prometheus-operator-kube-rbac-proxy-config
        - configMap:
            defaultMode: 420
            name: metrics-client-ca
          name: metrics-client-ca
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2022-05-24T18:28:13Z"
    generation: 1
    labels:
      app.kubernetes.io/component: telemetry-metrics-collector
      app.kubernetes.io/managed-by: cluster-monitoring-operator
      app.kubernetes.io/name: telemeter-client
      app.kubernetes.io/part-of: openshift-monitoring
      pod-template-hash: 7589ddc4df
    name: telemeter-client-7589ddc4df
    namespace: openshift-monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: telemeter-client
      uid: bb023006-b697-4157-a1a7-49980adc581a
    resourceVersion: "60516"
    uid: b8496268-de30-42b8-a7c4-6ab06236877c
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: telemetry-metrics-collector
        app.kubernetes.io/name: telemeter-client
        pod-template-hash: 7589ddc4df
    template:
      metadata:
        annotations:
          target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: telemetry-metrics-collector
          app.kubernetes.io/managed-by: cluster-monitoring-operator
          app.kubernetes.io/name: telemeter-client
          app.kubernetes.io/part-of: openshift-monitoring
          pod-template-hash: 7589ddc4df
      spec:
        containers:
        - command:
          - /usr/bin/telemeter-client
          - --id=$(ID)
          - --from=$(FROM)
          - --from-ca-file=/etc/serving-certs-ca-bundle/service-ca.crt
          - --from-token-file=/var/run/secrets/kubernetes.io/serviceaccount/token
          - --to=$(TO)
          - --to-token-file=/etc/telemeter/token
          - --listen=localhost:8080
          - --anonymize-salt-file=/etc/telemeter/salt
          - --anonymize-labels=$(ANONYMIZE_LABELS)
          - --match={__name__=~"cluster:usage:.*"}
          - --match={__name__="count:up0"}
          - --match={__name__="count:up1"}
          - --match={__name__="cluster_version"}
          - --match={__name__="cluster_version_available_updates"}
          - --match={__name__="cluster_operator_up"}
          - --match={__name__="cluster_operator_conditions"}
          - --match={__name__="cluster_version_payload"}
          - --match={__name__="cluster_installer"}
          - --match={__name__="cluster_infrastructure_provider"}
          - --match={__name__="cluster_feature_set"}
          - --match={__name__="instance:etcd_object_counts:sum"}
          - --match={__name__="ALERTS",alertstate="firing"}
          - --match={__name__="code:apiserver_request_total:rate:sum"}
          - --match={__name__="cluster:capacity_cpu_cores:sum"}
          - --match={__name__="cluster:capacity_memory_bytes:sum"}
          - --match={__name__="cluster:cpu_usage_cores:sum"}
          - --match={__name__="cluster:memory_usage_bytes:sum"}
          - --match={__name__="openshift:cpu_usage_cores:sum"}
          - --match={__name__="openshift:memory_usage_bytes:sum"}
          - --match={__name__="workload:cpu_usage_cores:sum"}
          - --match={__name__="workload:memory_usage_bytes:sum"}
          - --match={__name__="cluster:virt_platform_nodes:sum"}
          - --match={__name__="cluster:node_instance_type_count:sum"}
          - --match={__name__="cnv:vmi_status_running:count"}
          - --match={__name__="cluster:vmi_request_cpu_cores:sum"}
          - --match={__name__="node_role_os_version_machine:cpu_capacity_cores:sum"}
          - --match={__name__="node_role_os_version_machine:cpu_capacity_sockets:sum"}
          - --match={__name__="subscription_sync_total"}
          - --match={__name__="olm_resolution_duration_seconds"}
          - --match={__name__="csv_succeeded"}
          - --match={__name__="csv_abnormal"}
          - --match={__name__="cluster:kube_persistentvolumeclaim_resource_requests_storage_bytes:provisioner:sum"}
          - --match={__name__="cluster:kubelet_volume_stats_used_bytes:provisioner:sum"}
          - --match={__name__="ceph_cluster_total_bytes"}
          - --match={__name__="ceph_cluster_total_used_raw_bytes"}
          - --match={__name__="ceph_health_status"}
          - --match={__name__="job:ceph_osd_metadata:count"}
          - --match={__name__="job:kube_pv:count"}
          - --match={__name__="job:ceph_pools_iops:total"}
          - --match={__name__="job:ceph_pools_iops_bytes:total"}
          - --match={__name__="job:ceph_versions_running:count"}
          - --match={__name__="job:noobaa_total_unhealthy_buckets:sum"}
          - --match={__name__="job:noobaa_bucket_count:sum"}
          - --match={__name__="job:noobaa_total_object_count:sum"}
          - --match={__name__="noobaa_accounts_num"}
          - --match={__name__="noobaa_total_usage"}
          - --match={__name__="console_url"}
          - --match={__name__="cluster:network_attachment_definition_instances:max"}
          - --match={__name__="cluster:network_attachment_definition_enabled_instance_up:max"}
          - --match={__name__="cluster:ingress_controller_aws_nlb_active:sum"}
          - --match={__name__="insightsclient_request_send_total"}
          - --match={__name__="cam_app_workload_migrations"}
          - --match={__name__="cluster:apiserver_current_inflight_requests:sum:max_over_time:2m"}
          - --match={__name__="cluster:alertmanager_integrations:max"}
          - --match={__name__="cluster:telemetry_selected_series:count"}
          - --match={__name__="openshift:prometheus_tsdb_head_series:sum"}
          - --match={__name__="openshift:prometheus_tsdb_head_samples_appended_total:sum"}
          - --match={__name__="monitoring:container_memory_working_set_bytes:sum"}
          - --match={__name__="namespace_job:scrape_series_added:topk3_sum1h"}
          - --match={__name__="namespace_job:scrape_samples_post_metric_relabeling:topk3"}
          - --match={__name__="monitoring:haproxy_server_http_responses_total:sum"}
          - --match={__name__="rhmi_status"}
          - --match={__name__="cluster_legacy_scheduler_policy"}
          - --match={__name__="cluster_master_schedulable"}
          - --match={__name__="che_workspace_status"}
          - --match={__name__="che_workspace_started_total"}
          - --match={__name__="che_workspace_failure_total"}
          - --match={__name__="che_workspace_start_time_seconds_sum"}
          - --match={__name__="che_workspace_start_time_seconds_count"}
          - --match={__name__="cco_credentials_mode"}
          - --match={__name__="cluster:kube_persistentvolume_plugin_type_counts:sum"}
          - --match={__name__="visual_web_terminal_sessions_total"}
          - --match={__name__="acm_managed_cluster_info"}
          - --match={__name__="cluster:vsphere_vcenter_info:sum"}
          - --match={__name__="cluster:vsphere_esxi_version_total:sum"}
          - --match={__name__="cluster:vsphere_node_hw_version_total:sum"}
          - --match={__name__="openshift:build_by_strategy:sum"}
          - --match={__name__="rhods_aggregate_availability"}
          - --match={__name__="rhods_total_users"}
          - --match={__name__="instance:etcd_disk_wal_fsync_duration_seconds:histogram_quantile",quantile="0.99"}
          - --match={__name__="instance:etcd_mvcc_db_total_size_in_bytes:sum"}
          - --match={__name__="instance:etcd_network_peer_round_trip_time_seconds:histogram_quantile",quantile="0.99"}
          - --match={__name__="instance:etcd_mvcc_db_total_size_in_use_in_bytes:sum"}
          - --match={__name__="instance:etcd_disk_backend_commit_duration_seconds:histogram_quantile",quantile="0.99"}
          - --match={__name__="jaeger_operator_instances_storage_types"}
          - --match={__name__="jaeger_operator_instances_strategies"}
          - --match={__name__="jaeger_operator_instances_agent_strategies"}
          - --match={__name__="appsvcs:cores_by_product:sum"}
          - --match={__name__="nto_custom_profiles:count"}
          - --match={__name__="openshift_csi_share_configmap"}
          - --match={__name__="openshift_csi_share_secret"}
          - --match={__name__="openshift_csi_share_mount_failures_total"}
          - --match={__name__="openshift_csi_share_mount_requests_total"}
          - --limit-bytes=5242880
          env:
          - name: ANONYMIZE_LABELS
          - name: FROM
            value: https://prometheus-k8s.openshift-monitoring.svc:9091
          - name: ID
            value: 3ef9f2d5-498d-4a5e-a004-09e65633e1bb
          - name: TO
            value: https://infogw.api.openshift.com
          - name: HTTP_PROXY
          - name: HTTPS_PROXY
          - name: NO_PROXY
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3cd352ab6f82a1f5f762b6283f9644e653d37dcdbab27e1fae3d5a82a76d2a9f
          imagePullPolicy: IfNotPresent
          name: telemeter-client
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 40Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/serving-certs-ca-bundle
            name: serving-certs-ca-bundle
          - mountPath: /etc/telemeter
            name: secret-telemeter-client
          - mountPath: /etc/pki/ca-trust/extracted/pem/
            name: telemeter-trusted-ca-bundle
            readOnly: true
        - args:
          - --reload-url=http://localhost:8080/-/reload
          - --watched-dir=/etc/serving-certs-ca-bundle
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b5d406a68f3fb45008977ea6f4d1cda50c8a878cf0055c11308b1785d5a06023
          imagePullPolicy: IfNotPresent
          name: reload
          resources:
            requests:
              cpu: 1m
              memory: 10Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/serving-certs-ca-bundle
            name: serving-certs-ca-bundle
        - args:
          - --secure-listen-address=:8443
          - --upstream=http://127.0.0.1:8080/
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --config-file=/etc/kube-rbac-policy/config.yaml
          - --client-ca-file=/etc/tls/client/client-ca.crt
          - --tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:08e8b4004edaeeb125ced09ab2c4cd6d690afaf3a86309c91a994dec8e3ccbf3
          imagePullPolicy: IfNotPresent
          name: kube-rbac-proxy
          ports:
          - containerPort: 8443
            name: https
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/tls/private
            name: telemeter-client-tls
          - mountPath: /etc/kube-rbac-policy
            name: secret-telemeter-client-kube-rbac-proxy-config
            readOnly: true
          - mountPath: /etc/tls/client
            name: metrics-client-ca
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          node-role.kubernetes.io/infra: ""
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: telemeter-client
        serviceAccountName: telemeter-client
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/infra
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: telemeter-client-serving-certs-ca-bundle
          name: serving-certs-ca-bundle
        - name: secret-telemeter-client
          secret:
            defaultMode: 420
            secretName: telemeter-client
        - name: telemeter-client-tls
          secret:
            defaultMode: 420
            secretName: telemeter-client-tls
        - name: secret-telemeter-client-kube-rbac-proxy-config
          secret:
            defaultMode: 420
            secretName: telemeter-client-kube-rbac-proxy-config
        - configMap:
            defaultMode: 420
            name: metrics-client-ca
          name: metrics-client-ca
        - configMap:
            defaultMode: 420
            items:
            - key: ca-bundle.crt
              path: tls-ca-bundle.pem
            name: telemeter-trusted-ca-bundle-2rsonso43rc5p
            optional: true
          name: telemeter-trusted-ca-bundle
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2022-05-24T18:13:14Z"
    generation: 2
    labels:
      app.kubernetes.io/component: telemetry-metrics-collector
      app.kubernetes.io/managed-by: cluster-monitoring-operator
      app.kubernetes.io/name: telemeter-client
      app.kubernetes.io/part-of: openshift-monitoring
      pod-template-hash: f8cfc5d66
    name: telemeter-client-f8cfc5d66
    namespace: openshift-monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: telemeter-client
      uid: bb023006-b697-4157-a1a7-49980adc581a
    resourceVersion: "41726"
    uid: 7c067388-0c9d-49c0-aae9-da4c99ab883d
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: telemetry-metrics-collector
        app.kubernetes.io/name: telemeter-client
        pod-template-hash: f8cfc5d66
    template:
      metadata:
        annotations:
          target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: telemetry-metrics-collector
          app.kubernetes.io/managed-by: cluster-monitoring-operator
          app.kubernetes.io/name: telemeter-client
          app.kubernetes.io/part-of: openshift-monitoring
          pod-template-hash: f8cfc5d66
      spec:
        containers:
        - command:
          - /usr/bin/telemeter-client
          - --id=$(ID)
          - --from=$(FROM)
          - --from-ca-file=/etc/serving-certs-ca-bundle/service-ca.crt
          - --from-token-file=/var/run/secrets/kubernetes.io/serviceaccount/token
          - --to=$(TO)
          - --to-token-file=/etc/telemeter/token
          - --listen=localhost:8080
          - --anonymize-salt-file=/etc/telemeter/salt
          - --anonymize-labels=$(ANONYMIZE_LABELS)
          - --match={__name__=~"cluster:usage:.*"}
          - --match={__name__="count:up0"}
          - --match={__name__="count:up1"}
          - --match={__name__="cluster_version"}
          - --match={__name__="cluster_version_available_updates"}
          - --match={__name__="cluster_operator_up"}
          - --match={__name__="cluster_operator_conditions"}
          - --match={__name__="cluster_version_payload"}
          - --match={__name__="cluster_installer"}
          - --match={__name__="cluster_infrastructure_provider"}
          - --match={__name__="cluster_feature_set"}
          - --match={__name__="instance:etcd_object_counts:sum"}
          - --match={__name__="ALERTS",alertstate="firing"}
          - --match={__name__="code:apiserver_request_total:rate:sum"}
          - --match={__name__="cluster:capacity_cpu_cores:sum"}
          - --match={__name__="cluster:capacity_memory_bytes:sum"}
          - --match={__name__="cluster:cpu_usage_cores:sum"}
          - --match={__name__="cluster:memory_usage_bytes:sum"}
          - --match={__name__="openshift:cpu_usage_cores:sum"}
          - --match={__name__="openshift:memory_usage_bytes:sum"}
          - --match={__name__="workload:cpu_usage_cores:sum"}
          - --match={__name__="workload:memory_usage_bytes:sum"}
          - --match={__name__="cluster:virt_platform_nodes:sum"}
          - --match={__name__="cluster:node_instance_type_count:sum"}
          - --match={__name__="cnv:vmi_status_running:count"}
          - --match={__name__="cluster:vmi_request_cpu_cores:sum"}
          - --match={__name__="node_role_os_version_machine:cpu_capacity_cores:sum"}
          - --match={__name__="node_role_os_version_machine:cpu_capacity_sockets:sum"}
          - --match={__name__="subscription_sync_total"}
          - --match={__name__="olm_resolution_duration_seconds"}
          - --match={__name__="csv_succeeded"}
          - --match={__name__="csv_abnormal"}
          - --match={__name__="cluster:kube_persistentvolumeclaim_resource_requests_storage_bytes:provisioner:sum"}
          - --match={__name__="cluster:kubelet_volume_stats_used_bytes:provisioner:sum"}
          - --match={__name__="ceph_cluster_total_bytes"}
          - --match={__name__="ceph_cluster_total_used_raw_bytes"}
          - --match={__name__="ceph_health_status"}
          - --match={__name__="job:ceph_osd_metadata:count"}
          - --match={__name__="job:kube_pv:count"}
          - --match={__name__="job:ceph_pools_iops:total"}
          - --match={__name__="job:ceph_pools_iops_bytes:total"}
          - --match={__name__="job:ceph_versions_running:count"}
          - --match={__name__="job:noobaa_total_unhealthy_buckets:sum"}
          - --match={__name__="job:noobaa_bucket_count:sum"}
          - --match={__name__="job:noobaa_total_object_count:sum"}
          - --match={__name__="noobaa_accounts_num"}
          - --match={__name__="noobaa_total_usage"}
          - --match={__name__="console_url"}
          - --match={__name__="cluster:network_attachment_definition_instances:max"}
          - --match={__name__="cluster:network_attachment_definition_enabled_instance_up:max"}
          - --match={__name__="cluster:ingress_controller_aws_nlb_active:sum"}
          - --match={__name__="insightsclient_request_send_total"}
          - --match={__name__="cam_app_workload_migrations"}
          - --match={__name__="cluster:apiserver_current_inflight_requests:sum:max_over_time:2m"}
          - --match={__name__="cluster:alertmanager_integrations:max"}
          - --match={__name__="cluster:telemetry_selected_series:count"}
          - --match={__name__="openshift:prometheus_tsdb_head_series:sum"}
          - --match={__name__="openshift:prometheus_tsdb_head_samples_appended_total:sum"}
          - --match={__name__="monitoring:container_memory_working_set_bytes:sum"}
          - --match={__name__="namespace_job:scrape_series_added:topk3_sum1h"}
          - --match={__name__="namespace_job:scrape_samples_post_metric_relabeling:topk3"}
          - --match={__name__="monitoring:haproxy_server_http_responses_total:sum"}
          - --match={__name__="rhmi_status"}
          - --match={__name__="cluster_legacy_scheduler_policy"}
          - --match={__name__="cluster_master_schedulable"}
          - --match={__name__="che_workspace_status"}
          - --match={__name__="che_workspace_started_total"}
          - --match={__name__="che_workspace_failure_total"}
          - --match={__name__="che_workspace_start_time_seconds_sum"}
          - --match={__name__="che_workspace_start_time_seconds_count"}
          - --match={__name__="cco_credentials_mode"}
          - --match={__name__="cluster:kube_persistentvolume_plugin_type_counts:sum"}
          - --match={__name__="visual_web_terminal_sessions_total"}
          - --match={__name__="acm_managed_cluster_info"}
          - --match={__name__="cluster:vsphere_vcenter_info:sum"}
          - --match={__name__="cluster:vsphere_esxi_version_total:sum"}
          - --match={__name__="cluster:vsphere_node_hw_version_total:sum"}
          - --match={__name__="openshift:build_by_strategy:sum"}
          - --match={__name__="rhods_aggregate_availability"}
          - --match={__name__="rhods_total_users"}
          - --match={__name__="instance:etcd_disk_wal_fsync_duration_seconds:histogram_quantile",quantile="0.99"}
          - --match={__name__="instance:etcd_mvcc_db_total_size_in_bytes:sum"}
          - --match={__name__="instance:etcd_network_peer_round_trip_time_seconds:histogram_quantile",quantile="0.99"}
          - --match={__name__="instance:etcd_mvcc_db_total_size_in_use_in_bytes:sum"}
          - --match={__name__="instance:etcd_disk_backend_commit_duration_seconds:histogram_quantile",quantile="0.99"}
          - --match={__name__="jaeger_operator_instances_storage_types"}
          - --match={__name__="jaeger_operator_instances_strategies"}
          - --match={__name__="jaeger_operator_instances_agent_strategies"}
          - --match={__name__="appsvcs:cores_by_product:sum"}
          - --match={__name__="nto_custom_profiles:count"}
          - --match={__name__="openshift_csi_share_configmap"}
          - --match={__name__="openshift_csi_share_secret"}
          - --match={__name__="openshift_csi_share_mount_failures_total"}
          - --match={__name__="openshift_csi_share_mount_requests_total"}
          - --limit-bytes=5242880
          env:
          - name: ANONYMIZE_LABELS
          - name: FROM
            value: https://prometheus-k8s.openshift-monitoring.svc:9091
          - name: ID
            value: 3ef9f2d5-498d-4a5e-a004-09e65633e1bb
          - name: TO
            value: https://infogw.api.openshift.com/
          - name: HTTP_PROXY
          - name: HTTPS_PROXY
          - name: NO_PROXY
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3cd352ab6f82a1f5f762b6283f9644e653d37dcdbab27e1fae3d5a82a76d2a9f
          imagePullPolicy: IfNotPresent
          name: telemeter-client
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 40Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/serving-certs-ca-bundle
            name: serving-certs-ca-bundle
          - mountPath: /etc/telemeter
            name: secret-telemeter-client
          - mountPath: /etc/pki/ca-trust/extracted/pem/
            name: telemeter-trusted-ca-bundle
            readOnly: true
        - args:
          - --reload-url=http://localhost:8080/-/reload
          - --watched-dir=/etc/serving-certs-ca-bundle
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:b5d406a68f3fb45008977ea6f4d1cda50c8a878cf0055c11308b1785d5a06023
          imagePullPolicy: IfNotPresent
          name: reload
          resources:
            requests:
              cpu: 1m
              memory: 10Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/serving-certs-ca-bundle
            name: serving-certs-ca-bundle
        - args:
          - --secure-listen-address=:8443
          - --upstream=http://127.0.0.1:8080/
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --config-file=/etc/kube-rbac-policy/config.yaml
          - --client-ca-file=/etc/tls/client/client-ca.crt
          - --tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:08e8b4004edaeeb125ced09ab2c4cd6d690afaf3a86309c91a994dec8e3ccbf3
          imagePullPolicy: IfNotPresent
          name: kube-rbac-proxy
          ports:
          - containerPort: 8443
            name: https
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/tls/private
            name: telemeter-client-tls
          - mountPath: /etc/kube-rbac-policy
            name: secret-telemeter-client-kube-rbac-proxy-config
            readOnly: true
          - mountPath: /etc/tls/client
            name: metrics-client-ca
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: telemeter-client
        serviceAccountName: telemeter-client
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: telemeter-client-serving-certs-ca-bundle
          name: serving-certs-ca-bundle
        - name: secret-telemeter-client
          secret:
            defaultMode: 420
            secretName: telemeter-client
        - name: telemeter-client-tls
          secret:
            defaultMode: 420
            secretName: telemeter-client-tls
        - name: secret-telemeter-client-kube-rbac-proxy-config
          secret:
            defaultMode: 420
            secretName: telemeter-client-kube-rbac-proxy-config
        - configMap:
            defaultMode: 420
            name: metrics-client-ca
          name: metrics-client-ca
        - configMap:
            defaultMode: 420
            items:
            - key: ca-bundle.crt
              path: tls-ca-bundle.pem
            name: telemeter-trusted-ca-bundle-2rsonso43rc5p
            optional: true
          name: telemeter-trusted-ca-bundle
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2022-05-24T18:13:23Z"
    generation: 3
    labels:
      app.kubernetes.io/component: query-layer
      app.kubernetes.io/instance: thanos-querier
      app.kubernetes.io/managed-by: cluster-monitoring-operator
      app.kubernetes.io/name: thanos-query
      app.kubernetes.io/part-of: openshift-monitoring
      app.kubernetes.io/version: 0.23.1
      pod-template-hash: 6dbfbb6d4
    name: thanos-querier-6dbfbb6d4
    namespace: openshift-monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: thanos-querier
      uid: e5ad4f5b-8060-43c0-9d01-2ee765f0c772
    resourceVersion: "42365"
    uid: 1b09f7b6-64dd-4b25-9165-9a84c2acda73
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: query-layer
        app.kubernetes.io/instance: thanos-querier
        app.kubernetes.io/name: thanos-query
        app.kubernetes.io/part-of: openshift-monitoring
        pod-template-hash: 6dbfbb6d4
    template:
      metadata:
        annotations:
          target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: query-layer
          app.kubernetes.io/instance: thanos-querier
          app.kubernetes.io/managed-by: cluster-monitoring-operator
          app.kubernetes.io/name: thanos-query
          app.kubernetes.io/part-of: openshift-monitoring
          app.kubernetes.io/version: 0.23.1
          pod-template-hash: 6dbfbb6d4
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/component: query-layer
                  app.kubernetes.io/instance: thanos-querier
                  app.kubernetes.io/name: thanos-query
                  app.kubernetes.io/part-of: openshift-monitoring
              topologyKey: kubernetes.io/hostname
        containers:
        - args:
          - query
          - --grpc-address=127.0.0.1:10901
          - --http-address=127.0.0.1:9090
          - --log.format=logfmt
          - --query.replica-label=prometheus_replica
          - --query.replica-label=thanos_ruler_replica
          - --store=dnssrv+_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local
          - --query.auto-downsampling
          - --store.sd-dns-resolver=miekgdns
          - --grpc-client-tls-secure
          - --grpc-client-tls-cert=/etc/tls/grpc/client.crt
          - --grpc-client-tls-key=/etc/tls/grpc/client.key
          - --grpc-client-tls-ca=/etc/tls/grpc/ca.crt
          - --grpc-client-server-name=prometheus-grpc
          - --rule=dnssrv+_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local
          - --target=dnssrv+_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local
          env:
          - name: HOST_IP_ADDRESS
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.hostIP
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8a5a316dec56c12072bd11525534dc01cad163fd44766be4e1fb3406796e662d
          imagePullPolicy: IfNotPresent
          name: thanos-query
          ports:
          - containerPort: 9090
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 10m
              memory: 12Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/tls/grpc
            name: secret-grpc-tls
        - args:
          - -provider=openshift
          - -https-address=:9091
          - -http-address=
          - -email-domain=*
          - -upstream=http://localhost:9090
          - -openshift-service-account=thanos-querier
          - '-openshift-sar={"resource": "namespaces", "verb": "get"}'
          - '-openshift-delegate-urls={"/": {"resource": "namespaces", "verb": "get"}}'
          - -tls-cert=/etc/tls/private/tls.crt
          - -tls-key=/etc/tls/private/tls.key
          - -client-secret-file=/var/run/secrets/kubernetes.io/serviceaccount/token
          - -cookie-secret-file=/etc/proxy/secrets/session_secret
          - -openshift-ca=/etc/pki/tls/cert.pem
          - -openshift-ca=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          - -bypass-auth-for=^/-/(healthy|ready)$
          - -htpasswd-file=/etc/proxy/htpasswd/auth
          env:
          - name: HTTP_PROXY
          - name: HTTPS_PROXY
          - name: NO_PROXY
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d787f47ee2a410f924ea00b2428f0cf2275eb059adac96ca1b69c71ad20ccb1d
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 4
            httpGet:
              path: /-/healthy
              port: 9091
              scheme: HTTPS
            initialDelaySeconds: 5
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: oauth-proxy
          ports:
          - containerPort: 9091
            name: web
            protocol: TCP
          readinessProbe:
            failureThreshold: 20
            httpGet:
              path: /-/ready
              port: 9091
              scheme: HTTPS
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 1m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/tls/private
            name: secret-thanos-querier-tls
          - mountPath: /etc/proxy/secrets
            name: secret-thanos-querier-oauth-cookie
          - mountPath: /etc/pki/ca-trust/extracted/pem/
            name: thanos-querier-trusted-ca-bundle
            readOnly: true
          - mountPath: /etc/proxy/htpasswd
            name: secret-thanos-querier-oauth-htpasswd
        - args:
          - --secure-listen-address=0.0.0.0:9092
          - --upstream=http://127.0.0.1:9095
          - --config-file=/etc/kube-rbac-proxy/config.yaml
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --logtostderr=true
          - --allow-paths=/api/v1/query,/api/v1/query_range,/api/v1/labels,/api/v1/label/*/values,/api/v1/series
          - --tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:08e8b4004edaeeb125ced09ab2c4cd6d690afaf3a86309c91a994dec8e3ccbf3
          imagePullPolicy: IfNotPresent
          name: kube-rbac-proxy
          ports:
          - containerPort: 9092
            name: tenancy
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 15Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/tls/private
            name: secret-thanos-querier-tls
          - mountPath: /etc/kube-rbac-proxy
            name: secret-thanos-querier-kube-rbac-proxy
        - args:
          - --insecure-listen-address=127.0.0.1:9095
          - --upstream=http://127.0.0.1:9090
          - --label=namespace
          - --enable-label-apis
          - --error-on-replace
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:5c3e8963d0e5dbce2e30dc0341c7453c30a42395622905b718a8fe9974da41ab
          imagePullPolicy: IfNotPresent
          name: prom-label-proxy
          resources:
            requests:
              cpu: 1m
              memory: 15Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
        - args:
          - --secure-listen-address=0.0.0.0:9093
          - --upstream=http://127.0.0.1:9095
          - --config-file=/etc/kube-rbac-proxy/config.yaml
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --logtostderr=true
          - --allow-paths=/api/v1/rules
          - --tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:08e8b4004edaeeb125ced09ab2c4cd6d690afaf3a86309c91a994dec8e3ccbf3
          imagePullPolicy: IfNotPresent
          name: kube-rbac-proxy-rules
          ports:
          - containerPort: 9093
            name: tenancy-rules
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 15Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/tls/private
            name: secret-thanos-querier-tls
          - mountPath: /etc/kube-rbac-proxy
            name: secret-thanos-querier-kube-rbac-proxy-rules
        - args:
          - --secure-listen-address=0.0.0.0:9094
          - --upstream=http://127.0.0.1:9090
          - --config-file=/etc/kube-rbac-proxy/config.yaml
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --client-ca-file=/etc/tls/client/client-ca.crt
          - --logtostderr=true
          - --allow-paths=/metrics
          - --tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:08e8b4004edaeeb125ced09ab2c4cd6d690afaf3a86309c91a994dec8e3ccbf3
          imagePullPolicy: IfNotPresent
          name: kube-rbac-proxy-metrics
          ports:
          - containerPort: 9094
            name: metrics
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 15Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/tls/private
            name: secret-thanos-querier-tls
          - mountPath: /etc/kube-rbac-proxy
            name: secret-thanos-querier-kube-rbac-proxy-metrics
          - mountPath: /etc/tls/client
            name: metrics-client-ca
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: thanos-querier
        serviceAccountName: thanos-querier
        terminationGracePeriodSeconds: 120
        volumes:
        - name: secret-thanos-querier-tls
          secret:
            defaultMode: 420
            secretName: thanos-querier-tls
        - name: secret-thanos-querier-oauth-cookie
          secret:
            defaultMode: 420
            secretName: thanos-querier-oauth-cookie
        - name: secret-thanos-querier-kube-rbac-proxy
          secret:
            defaultMode: 420
            secretName: thanos-querier-kube-rbac-proxy
        - name: secret-thanos-querier-kube-rbac-proxy-rules
          secret:
            defaultMode: 420
            secretName: thanos-querier-kube-rbac-proxy-rules
        - name: secret-thanos-querier-kube-rbac-proxy-metrics
          secret:
            defaultMode: 420
            secretName: thanos-querier-kube-rbac-proxy-metrics
        - configMap:
            defaultMode: 420
            name: metrics-client-ca
          name: metrics-client-ca
        - configMap:
            defaultMode: 420
            items:
            - key: ca-bundle.crt
              path: tls-ca-bundle.pem
            name: thanos-querier-trusted-ca-bundle-2rsonso43rc5p
            optional: true
          name: thanos-querier-trusted-ca-bundle
        - name: secret-thanos-querier-oauth-htpasswd
          secret:
            defaultMode: 420
            secretName: thanos-querier-oauth-htpasswd
        - name: secret-grpc-tls
          secret:
            defaultMode: 420
            secretName: thanos-querier-grpc-tls-66fq25vslj8tg
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2022-05-24T18:28:16Z"
    generation: 2
    labels:
      app.kubernetes.io/component: query-layer
      app.kubernetes.io/instance: thanos-querier
      app.kubernetes.io/managed-by: cluster-monitoring-operator
      app.kubernetes.io/name: thanos-query
      app.kubernetes.io/part-of: openshift-monitoring
      app.kubernetes.io/version: 0.23.1
      pod-template-hash: 86c47bb76
    name: thanos-querier-86c47bb76
    namespace: openshift-monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: thanos-querier
      uid: e5ad4f5b-8060-43c0-9d01-2ee765f0c772
    resourceVersion: "76419"
    uid: 993907a8-c083-4df5-b19a-df958996c1ab
  spec:
    replicas: 2
    selector:
      matchLabels:
        app.kubernetes.io/component: query-layer
        app.kubernetes.io/instance: thanos-querier
        app.kubernetes.io/name: thanos-query
        app.kubernetes.io/part-of: openshift-monitoring
        pod-template-hash: 86c47bb76
    template:
      metadata:
        annotations:
          target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: query-layer
          app.kubernetes.io/instance: thanos-querier
          app.kubernetes.io/managed-by: cluster-monitoring-operator
          app.kubernetes.io/name: thanos-query
          app.kubernetes.io/part-of: openshift-monitoring
          app.kubernetes.io/version: 0.23.1
          pod-template-hash: 86c47bb76
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/component: query-layer
                  app.kubernetes.io/instance: thanos-querier
                  app.kubernetes.io/name: thanos-query
                  app.kubernetes.io/part-of: openshift-monitoring
              topologyKey: kubernetes.io/hostname
        containers:
        - args:
          - query
          - --grpc-address=127.0.0.1:10901
          - --http-address=127.0.0.1:9090
          - --log.format=logfmt
          - --query.replica-label=prometheus_replica
          - --query.replica-label=thanos_ruler_replica
          - --store=dnssrv+_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local
          - --query.auto-downsampling
          - --store.sd-dns-resolver=miekgdns
          - --grpc-client-tls-secure
          - --grpc-client-tls-cert=/etc/tls/grpc/client.crt
          - --grpc-client-tls-key=/etc/tls/grpc/client.key
          - --grpc-client-tls-ca=/etc/tls/grpc/ca.crt
          - --grpc-client-server-name=prometheus-grpc
          - --rule=dnssrv+_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local
          - --target=dnssrv+_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local
          - --store=dnssrv+_grpc._tcp.prometheus-operated.openshift-user-workload-monitoring.svc.cluster.local
          - --store=dnssrv+_grpc._tcp.thanos-ruler-operated.openshift-user-workload-monitoring.svc.cluster.local
          - --rule=dnssrv+_grpc._tcp.prometheus-operated.openshift-user-workload-monitoring.svc.cluster.local
          - --rule=dnssrv+_grpc._tcp.thanos-ruler-operated.openshift-user-workload-monitoring.svc.cluster.local
          - --target=dnssrv+_grpc._tcp.prometheus-operated.openshift-user-workload-monitoring.svc.cluster.local
          env:
          - name: HOST_IP_ADDRESS
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.hostIP
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8a5a316dec56c12072bd11525534dc01cad163fd44766be4e1fb3406796e662d
          imagePullPolicy: IfNotPresent
          name: thanos-query
          ports:
          - containerPort: 9090
            name: http
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 2Gi
            requests:
              cpu: 5m
              memory: 125Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/tls/grpc
            name: secret-grpc-tls
        - args:
          - -provider=openshift
          - -https-address=:9091
          - -http-address=
          - -email-domain=*
          - -upstream=http://localhost:9090
          - -openshift-service-account=thanos-querier
          - '-openshift-sar={"resource": "namespaces", "verb": "get"}'
          - '-openshift-delegate-urls={"/": {"resource": "namespaces", "verb": "get"}}'
          - -tls-cert=/etc/tls/private/tls.crt
          - -tls-key=/etc/tls/private/tls.key
          - -client-secret-file=/var/run/secrets/kubernetes.io/serviceaccount/token
          - -cookie-secret-file=/etc/proxy/secrets/session_secret
          - -openshift-ca=/etc/pki/tls/cert.pem
          - -openshift-ca=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          - -bypass-auth-for=^/-/(healthy|ready)$
          - -htpasswd-file=/etc/proxy/htpasswd/auth
          env:
          - name: HTTP_PROXY
          - name: HTTPS_PROXY
          - name: NO_PROXY
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d787f47ee2a410f924ea00b2428f0cf2275eb059adac96ca1b69c71ad20ccb1d
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 4
            httpGet:
              path: /-/healthy
              port: 9091
              scheme: HTTPS
            initialDelaySeconds: 5
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: oauth-proxy
          ports:
          - containerPort: 9091
            name: web
            protocol: TCP
          readinessProbe:
            failureThreshold: 20
            httpGet:
              path: /-/ready
              port: 9091
              scheme: HTTPS
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 1m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/tls/private
            name: secret-thanos-querier-tls
          - mountPath: /etc/proxy/secrets
            name: secret-thanos-querier-oauth-cookie
          - mountPath: /etc/pki/ca-trust/extracted/pem/
            name: thanos-querier-trusted-ca-bundle
            readOnly: true
          - mountPath: /etc/proxy/htpasswd
            name: secret-thanos-querier-oauth-htpasswd
        - args:
          - --secure-listen-address=0.0.0.0:9092
          - --upstream=http://127.0.0.1:9095
          - --config-file=/etc/kube-rbac-proxy/config.yaml
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --logtostderr=true
          - --allow-paths=/api/v1/query,/api/v1/query_range,/api/v1/labels,/api/v1/label/*/values,/api/v1/series
          - --tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:08e8b4004edaeeb125ced09ab2c4cd6d690afaf3a86309c91a994dec8e3ccbf3
          imagePullPolicy: IfNotPresent
          name: kube-rbac-proxy
          ports:
          - containerPort: 9092
            name: tenancy
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 15Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/tls/private
            name: secret-thanos-querier-tls
          - mountPath: /etc/kube-rbac-proxy
            name: secret-thanos-querier-kube-rbac-proxy
        - args:
          - --insecure-listen-address=127.0.0.1:9095
          - --upstream=http://127.0.0.1:9090
          - --label=namespace
          - --enable-label-apis
          - --error-on-replace
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:5c3e8963d0e5dbce2e30dc0341c7453c30a42395622905b718a8fe9974da41ab
          imagePullPolicy: IfNotPresent
          name: prom-label-proxy
          resources:
            requests:
              cpu: 1m
              memory: 15Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
        - args:
          - --secure-listen-address=0.0.0.0:9093
          - --upstream=http://127.0.0.1:9095
          - --config-file=/etc/kube-rbac-proxy/config.yaml
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --logtostderr=true
          - --allow-paths=/api/v1/rules
          - --tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:08e8b4004edaeeb125ced09ab2c4cd6d690afaf3a86309c91a994dec8e3ccbf3
          imagePullPolicy: IfNotPresent
          name: kube-rbac-proxy-rules
          ports:
          - containerPort: 9093
            name: tenancy-rules
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 15Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/tls/private
            name: secret-thanos-querier-tls
          - mountPath: /etc/kube-rbac-proxy
            name: secret-thanos-querier-kube-rbac-proxy-rules
        - args:
          - --secure-listen-address=0.0.0.0:9094
          - --upstream=http://127.0.0.1:9090
          - --config-file=/etc/kube-rbac-proxy/config.yaml
          - --tls-cert-file=/etc/tls/private/tls.crt
          - --tls-private-key-file=/etc/tls/private/tls.key
          - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          - --client-ca-file=/etc/tls/client/client-ca.crt
          - --logtostderr=true
          - --allow-paths=/metrics
          - --tls-min-version=VersionTLS12
          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:08e8b4004edaeeb125ced09ab2c4cd6d690afaf3a86309c91a994dec8e3ccbf3
          imagePullPolicy: IfNotPresent
          name: kube-rbac-proxy-metrics
          ports:
          - containerPort: 9094
            name: metrics
            protocol: TCP
          resources:
            requests:
              cpu: 1m
              memory: 15Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/tls/private
            name: secret-thanos-querier-tls
          - mountPath: /etc/kube-rbac-proxy
            name: secret-thanos-querier-kube-rbac-proxy-metrics
          - mountPath: /etc/tls/client
            name: metrics-client-ca
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: thanos-querier
        serviceAccountName: thanos-querier
        terminationGracePeriodSeconds: 120
        volumes:
        - name: secret-thanos-querier-tls
          secret:
            defaultMode: 420
            secretName: thanos-querier-tls
        - name: secret-thanos-querier-oauth-cookie
          secret:
            defaultMode: 420
            secretName: thanos-querier-oauth-cookie
        - name: secret-thanos-querier-kube-rbac-proxy
          secret:
            defaultMode: 420
            secretName: thanos-querier-kube-rbac-proxy
        - name: secret-thanos-querier-kube-rbac-proxy-rules
          secret:
            defaultMode: 420
            secretName: thanos-querier-kube-rbac-proxy-rules
        - name: secret-thanos-querier-kube-rbac-proxy-metrics
          secret:
            defaultMode: 420
            secretName: thanos-querier-kube-rbac-proxy-metrics
        - configMap:
            defaultMode: 420
            name: metrics-client-ca
          name: metrics-client-ca
        - configMap:
            defaultMode: 420
            items:
            - key: ca-bundle.crt
              path: tls-ca-bundle.pem
            name: thanos-querier-trusted-ca-bundle-2rsonso43rc5p
            optional: true
          name: thanos-querier-trusted-ca-bundle
        - name: secret-thanos-querier-oauth-htpasswd
          secret:
            defaultMode: 420
            secretName: thanos-querier-oauth-htpasswd
        - name: secret-grpc-tls
          secret:
            defaultMode: 420
            secretName: thanos-querier-grpc-tls-66fq25vslj8tg
  status:
    availableReplicas: 2
    fullyLabeledReplicas: 2
    observedGeneration: 2
    readyReplicas: 2
    replicas: 2
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2022-05-24T18:25:37Z"
    generation: 1
    labels:
      app.kubernetes.io/component: authentication-proxy
      app.kubernetes.io/name: token-refresher
      app.kubernetes.io/version: master-2021-02-24-1e01b9c
      pod-template-hash: 7f8c567555
    name: token-refresher-7f8c567555
    namespace: openshift-monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: token-refresher
      uid: 71be56f1-7bdf-4eef-93f3-5fdf191c6e9d
    resourceVersion: "76403"
    uid: 7f40ad39-8781-4a28-9afe-4b41ff7786c4
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: authentication-proxy
        app.kubernetes.io/name: token-refresher
        pod-template-hash: 7f8c567555
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: authentication-proxy
          app.kubernetes.io/name: token-refresher
          app.kubernetes.io/version: master-2021-02-24-1e01b9c
          pod-template-hash: 7f8c567555
      spec:
        affinity:
          nodeAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - preference:
                matchExpressions:
                - key: node-role.kubernetes.io/infra
                  operator: Exists
              weight: 1
        containers:
        - args:
          - --oidc.audience=observatorium-telemeter
          - --oidc.client-id=$(CLIENT_ID)
          - --oidc.client-secret=$(CLIENT_SECRET)
          - --oidc.issuer-url=$(ISSUER_URL)
          - --url=$(RECEIVER_URL)
          env:
          - name: CLIENT_ID
            valueFrom:
              secretKeyRef:
                key: client-id
                name: observatorium-credentials
          - name: CLIENT_SECRET
            valueFrom:
              secretKeyRef:
                key: client-secret
                name: observatorium-credentials
          - name: RECEIVER_URL
            valueFrom:
              secretKeyRef:
                key: receiver-url
                name: observatorium-credentials
          - name: ISSUER_URL
            value: https://sso.redhat.com/auth/realms/redhat-external
          image: quay.io/observatorium/token-refresher@sha256:6ce9b80cd1d907cb6c9ed2a18612f386f7503257772d1d88155a4a2e6773fd00
          imagePullPolicy: IfNotPresent
          name: token-refresher
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ssl/certs/
            name: token-refresher-trusted-ca-bundle
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/infra
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: ca-bundle.crt
              path: ca-certificates.crt
            name: token-refresher-trusted-ca-bundle
          name: token-refresher-trusted-ca-bundle
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
kind: ReplicaSetList
metadata:
  resourceVersion: "322405"
