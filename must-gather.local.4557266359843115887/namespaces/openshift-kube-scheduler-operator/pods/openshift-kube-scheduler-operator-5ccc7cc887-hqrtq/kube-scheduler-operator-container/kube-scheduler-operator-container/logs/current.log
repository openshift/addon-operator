2022-05-24T18:36:32.479046055Z I0524 18:36:32.478960       1 cmd.go:209] Using service-serving-cert provided certificates
2022-05-24T18:36:32.479396101Z I0524 18:36:32.479378       1 observer_polling.go:159] Starting file observer
2022-05-24T18:36:35.751389838Z I0524 18:36:35.751346       1 builder.go:262] openshift-cluster-kube-scheduler-operator version 4.10.0-202204211158.p0.g0c57d73.assembly.stream-0c57d73-0c57d73f94b50ab95fc2914a7615cc1b23e7678e
2022-05-24T18:36:35.760582669Z I0524 18:36:35.760531       1 dynamic_serving_content.go:112] "Loaded a new cert/key pair" name="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key"
2022-05-24T18:36:36.628144405Z I0524 18:36:36.627543       1 requestheader_controller.go:244] Loaded a new request header values for RequestHeaderAuthRequestController
2022-05-24T18:36:36.633923441Z W0524 18:36:36.633887       1 secure_serving.go:69] Use of insecure cipher 'TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256' detected.
2022-05-24T18:36:36.633923441Z W0524 18:36:36.633907       1 secure_serving.go:69] Use of insecure cipher 'TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' detected.
2022-05-24T18:36:36.634226949Z I0524 18:36:36.634204       1 genericapiserver.go:406] MuxAndDiscoveryComplete has all endpoints registered and discovery information is complete
2022-05-24T18:36:36.739977893Z I0524 18:36:36.739325       1 configmap_cafile_content.go:201] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
2022-05-24T18:36:36.741421063Z I0524 18:36:36.741389       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
2022-05-24T18:36:36.741447329Z I0524 18:36:36.739402       1 configmap_cafile_content.go:201] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
2022-05-24T18:36:36.741447329Z I0524 18:36:36.741433       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
2022-05-24T18:36:36.741887288Z I0524 18:36:36.741365       1 leaderelection.go:248] attempting to acquire leader lease openshift-kube-scheduler-operator/openshift-cluster-kube-scheduler-operator-lock...
2022-05-24T18:36:36.742261559Z I0524 18:36:36.742239       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
2022-05-24T18:36:36.742306921Z I0524 18:36:36.742291       1 shared_informer.go:240] Waiting for caches to sync for RequestHeaderAuthRequestController
2022-05-24T18:36:36.742746308Z I0524 18:36:36.742724       1 tlsconfig.go:200] "Loaded serving cert" certName="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key" certDetail="\"metrics.openshift-kube-scheduler-operator.svc\" [serving] validServingFor=[metrics.openshift-kube-scheduler-operator.svc,metrics.openshift-kube-scheduler-operator.svc.cluster.local] issuer=\"openshift-service-serving-signer@1653415662\" (2022-05-24 18:07:57 +0000 UTC to 2024-05-23 18:07:58 +0000 UTC (now=2022-05-24 18:36:36.742687627 +0000 UTC))"
2022-05-24T18:36:36.742890954Z I0524 18:36:36.742874       1 named_certificates.go:53] "Loaded SNI cert" index=0 certName="self-signed loopback" certDetail="\"apiserver-loopback-client@1653417396\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1653417396\" (2022-05-24 17:36:35 +0000 UTC to 2023-05-24 17:36:35 +0000 UTC (now=2022-05-24 18:36:36.742857774 +0000 UTC))"
2022-05-24T18:36:36.742920562Z I0524 18:36:36.742908       1 secure_serving.go:266] Serving securely on [::]:8443
2022-05-24T18:36:36.742993857Z I0524 18:36:36.742981       1 genericapiserver.go:462] [graceful-termination] waiting for shutdown to be initiated
2022-05-24T18:36:36.743028365Z I0524 18:36:36.743009       1 dynamic_serving_content.go:131] "Starting controller" name="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key"
2022-05-24T18:36:36.743103679Z I0524 18:36:36.743085       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
2022-05-24T18:36:36.841992677Z I0524 18:36:36.841955       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file 
2022-05-24T18:36:36.842111848Z I0524 18:36:36.842060       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file 
2022-05-24T18:36:36.842409725Z I0524 18:36:36.842386       1 tlsconfig.go:178] "Loaded client CA" index=0 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"aggregator-signer\" [] issuer=\"<self>\" (2022-05-24 17:58:10 +0000 UTC to 2022-05-25 17:58:10 +0000 UTC (now=2022-05-24 18:36:36.842353747 +0000 UTC))"
2022-05-24T18:36:36.842451239Z I0524 18:36:36.842435       1 shared_informer.go:247] Caches are synced for RequestHeaderAuthRequestController 
2022-05-24T18:36:36.842613595Z I0524 18:36:36.842574       1 tlsconfig.go:200] "Loaded serving cert" certName="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key" certDetail="\"metrics.openshift-kube-scheduler-operator.svc\" [serving] validServingFor=[metrics.openshift-kube-scheduler-operator.svc,metrics.openshift-kube-scheduler-operator.svc.cluster.local] issuer=\"openshift-service-serving-signer@1653415662\" (2022-05-24 18:07:57 +0000 UTC to 2024-05-23 18:07:58 +0000 UTC (now=2022-05-24 18:36:36.842546216 +0000 UTC))"
2022-05-24T18:36:36.842755943Z I0524 18:36:36.842731       1 named_certificates.go:53] "Loaded SNI cert" index=0 certName="self-signed loopback" certDetail="\"apiserver-loopback-client@1653417396\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1653417396\" (2022-05-24 17:36:35 +0000 UTC to 2023-05-24 17:36:35 +0000 UTC (now=2022-05-24 18:36:36.842706805 +0000 UTC))"
2022-05-24T18:36:36.843017611Z I0524 18:36:36.842985       1 tlsconfig.go:178] "Loaded client CA" index=0 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"admin-kubeconfig-signer\" [] issuer=\"<self>\" (2022-05-24 17:58:08 +0000 UTC to 2032-05-21 17:58:08 +0000 UTC (now=2022-05-24 18:36:36.842961729 +0000 UTC))"
2022-05-24T18:36:36.843043194Z I0524 18:36:36.843031       1 tlsconfig.go:178] "Loaded client CA" index=1 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kubelet-signer\" [] issuer=\"<self>\" (2022-05-24 17:58:11 +0000 UTC to 2022-05-25 17:58:11 +0000 UTC (now=2022-05-24 18:36:36.843016508 +0000 UTC))"
2022-05-24T18:36:36.843065816Z I0524 18:36:36.843053       1 tlsconfig.go:178] "Loaded client CA" index=2 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-control-plane-signer\" [] issuer=\"<self>\" (2022-05-24 17:58:12 +0000 UTC to 2023-05-24 17:58:12 +0000 UTC (now=2022-05-24 18:36:36.843040911 +0000 UTC))"
2022-05-24T18:36:36.843090584Z I0524 18:36:36.843077       1 tlsconfig.go:178] "Loaded client CA" index=3 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-apiserver-to-kubelet-signer\" [] issuer=\"<self>\" (2022-05-24 17:58:12 +0000 UTC to 2023-05-24 17:58:12 +0000 UTC (now=2022-05-24 18:36:36.843063808 +0000 UTC))"
2022-05-24T18:36:36.843134858Z I0524 18:36:36.843116       1 tlsconfig.go:178] "Loaded client CA" index=4 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kubelet-bootstrap-kubeconfig-signer\" [] issuer=\"<self>\" (2022-05-24 17:58:09 +0000 UTC to 2032-05-21 17:58:09 +0000 UTC (now=2022-05-24 18:36:36.843090563 +0000 UTC))"
2022-05-24T18:36:36.843145529Z I0524 18:36:36.843139       1 tlsconfig.go:178] "Loaded client CA" index=5 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-csr-signer_@1653415662\" [] issuer=\"kubelet-signer\" (2022-05-24 18:07:41 +0000 UTC to 2022-05-25 17:58:11 +0000 UTC (now=2022-05-24 18:36:36.843128185 +0000 UTC))"
2022-05-24T18:36:36.843200620Z I0524 18:36:36.843166       1 tlsconfig.go:178] "Loaded client CA" index=6 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"openshift-kube-apiserver-operator_node-system-admin-signer@1653415661\" [] issuer=\"<self>\" (2022-05-24 18:07:40 +0000 UTC to 2023-05-24 18:07:41 +0000 UTC (now=2022-05-24 18:36:36.843145281 +0000 UTC))"
2022-05-24T18:36:36.843271772Z I0524 18:36:36.843251       1 tlsconfig.go:178] "Loaded client CA" index=7 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"aggregator-signer\" [] issuer=\"<self>\" (2022-05-24 17:58:10 +0000 UTC to 2022-05-25 17:58:10 +0000 UTC (now=2022-05-24 18:36:36.843230955 +0000 UTC))"
2022-05-24T18:36:36.843400690Z I0524 18:36:36.843385       1 tlsconfig.go:200] "Loaded serving cert" certName="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key" certDetail="\"metrics.openshift-kube-scheduler-operator.svc\" [serving] validServingFor=[metrics.openshift-kube-scheduler-operator.svc,metrics.openshift-kube-scheduler-operator.svc.cluster.local] issuer=\"openshift-service-serving-signer@1653415662\" (2022-05-24 18:07:57 +0000 UTC to 2024-05-23 18:07:58 +0000 UTC (now=2022-05-24 18:36:36.843371059 +0000 UTC))"
2022-05-24T18:36:36.843537285Z I0524 18:36:36.843523       1 named_certificates.go:53] "Loaded SNI cert" index=0 certName="self-signed loopback" certDetail="\"apiserver-loopback-client@1653417396\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1653417396\" (2022-05-24 17:36:35 +0000 UTC to 2023-05-24 17:36:35 +0000 UTC (now=2022-05-24 18:36:36.843497729 +0000 UTC))"
2022-05-24T18:36:36.976679915Z I0524 18:36:36.975921       1 leaderelection.go:258] successfully acquired lease openshift-kube-scheduler-operator/openshift-cluster-kube-scheduler-operator-lock
2022-05-24T18:36:36.976679915Z I0524 18:36:36.976210       1 event.go:285] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-cluster-kube-scheduler-operator-lock", UID:"b03cbfe0-966d-4549-93da-8fa8e400e6bb", APIVersion:"v1", ResourceVersion:"63342", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' openshift-kube-scheduler-operator-5ccc7cc887-hqrtq_3e45bc4e-3a40-4032-8dac-97fb5e054f2d became leader
2022-05-24T18:36:36.976679915Z I0524 18:36:36.976243       1 event.go:285] Event(v1.ObjectReference{Kind:"Lease", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-cluster-kube-scheduler-operator-lock", UID:"93454602-98b4-43b5-bd3d-7645e7d9ae9d", APIVersion:"coordination.k8s.io/v1", ResourceVersion:"63346", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' openshift-kube-scheduler-operator-5ccc7cc887-hqrtq_3e45bc4e-3a40-4032-8dac-97fb5e054f2d became leader
2022-05-24T18:36:37.079769508Z I0524 18:36:37.079714       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "RevisionController" resync interval is set to 0s which might lead to client request throttling
2022-05-24T18:36:37.080851425Z I0524 18:36:37.080806       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "NodeController" resync interval is set to 0s which might lead to client request throttling
2022-05-24T18:36:37.080965311Z I0524 18:36:37.080930       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "UnsupportedConfigOverridesController" resync interval is set to 0s which might lead to client request throttling
2022-05-24T18:36:37.081032535Z I0524 18:36:37.080960       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "GuardController" resync interval is set to 0s which might lead to client request throttling
2022-05-24T18:36:37.081068632Z I0524 18:36:37.080984       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "LoggingSyncer" resync interval is set to 0s which might lead to client request throttling
2022-05-24T18:36:37.081569630Z I0524 18:36:37.081529       1 base_controller.go:67] Waiting for caches to sync for RemoveStaleConditionsController
2022-05-24T18:36:37.081659535Z I0524 18:36:37.081626       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "PruneController" resync interval is set to 0s which might lead to client request throttling
2022-05-24T18:36:37.084168026Z I0524 18:36:37.084141       1 base_controller.go:67] Waiting for caches to sync for MissingStaticPodController
2022-05-24T18:36:37.084299310Z I0524 18:36:37.084271       1 base_controller.go:67] Waiting for caches to sync for NodeController
2022-05-24T18:36:37.084328101Z I0524 18:36:37.084299       1 base_controller.go:67] Waiting for caches to sync for GuardController
2022-05-24T18:36:37.084328101Z I0524 18:36:37.084302       1 base_controller.go:67] Waiting for caches to sync for RevisionController
2022-05-24T18:36:37.084835013Z I0524 18:36:37.084813       1 base_controller.go:67] Waiting for caches to sync for StaticResourceController
2022-05-24T18:36:37.084902886Z I0524 18:36:37.084892       1 base_controller.go:67] Waiting for caches to sync for UnsupportedConfigOverridesController
2022-05-24T18:36:37.084962412Z I0524 18:36:37.084950       1 base_controller.go:67] Waiting for caches to sync for LoggingSyncer
2022-05-24T18:36:37.085020619Z I0524 18:36:37.084993       1 base_controller.go:67] Waiting for caches to sync for StaticResourceController
2022-05-24T18:36:37.085075159Z I0524 18:36:37.085047       1 base_controller.go:67] Waiting for caches to sync for InstallerStateController
2022-05-24T18:36:37.085075159Z I0524 18:36:37.085070       1 base_controller.go:67] Waiting for caches to sync for StaticPodStateController
2022-05-24T18:36:37.085134444Z I0524 18:36:37.085102       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "InstallerController" resync interval is set to 0s which might lead to client request throttling
2022-05-24T18:36:37.085134444Z I0524 18:36:37.085053       1 base_controller.go:67] Waiting for caches to sync for ResourceSyncController
2022-05-24T18:36:37.085162753Z I0524 18:36:37.085064       1 base_controller.go:67] Waiting for caches to sync for TargetConfigController
2022-05-24T18:36:37.085332044Z I0524 18:36:37.085028       1 base_controller.go:67] Waiting for caches to sync for InstallerController
2022-05-24T18:36:37.085374663Z I0524 18:36:37.085069       1 base_controller.go:67] Waiting for caches to sync for PruneController
2022-05-24T18:36:37.085419314Z I0524 18:36:37.085326       1 base_controller.go:67] Waiting for caches to sync for ConfigObserver
2022-05-24T18:36:37.085471568Z I0524 18:36:37.085454       1 base_controller.go:67] Waiting for caches to sync for StatusSyncer_kube-scheduler
2022-05-24T18:36:37.182345101Z I0524 18:36:37.182296       1 base_controller.go:73] Caches are synced for RemoveStaleConditionsController 
2022-05-24T18:36:37.182345101Z I0524 18:36:37.182316       1 base_controller.go:110] Starting #1 worker of RemoveStaleConditionsController controller ...
2022-05-24T18:36:37.184558775Z I0524 18:36:37.184514       1 base_controller.go:73] Caches are synced for RevisionController 
2022-05-24T18:36:37.184558775Z I0524 18:36:37.184521       1 base_controller.go:73] Caches are synced for MissingStaticPodController 
2022-05-24T18:36:37.184558775Z I0524 18:36:37.184536       1 base_controller.go:110] Starting #1 worker of MissingStaticPodController controller ...
2022-05-24T18:36:37.184558775Z I0524 18:36:37.184544       1 base_controller.go:110] Starting #1 worker of RevisionController controller ...
2022-05-24T18:36:37.185108279Z I0524 18:36:37.185079       1 base_controller.go:73] Caches are synced for LoggingSyncer 
2022-05-24T18:36:37.185108279Z I0524 18:36:37.185096       1 base_controller.go:110] Starting #1 worker of LoggingSyncer controller ...
2022-05-24T18:36:37.185128581Z I0524 18:36:37.185104       1 base_controller.go:73] Caches are synced for StaticPodStateController 
2022-05-24T18:36:37.185128581Z I0524 18:36:37.185121       1 base_controller.go:110] Starting #1 worker of StaticPodStateController controller ...
2022-05-24T18:36:37.185149605Z I0524 18:36:37.185123       1 base_controller.go:73] Caches are synced for UnsupportedConfigOverridesController 
2022-05-24T18:36:37.185149605Z I0524 18:36:37.185137       1 base_controller.go:110] Starting #1 worker of UnsupportedConfigOverridesController controller ...
2022-05-24T18:36:37.185149605Z I0524 18:36:37.185145       1 base_controller.go:73] Caches are synced for StaticResourceController 
2022-05-24T18:36:37.185161038Z I0524 18:36:37.185151       1 base_controller.go:110] Starting #1 worker of StaticResourceController controller ...
2022-05-24T18:36:37.185210109Z I0524 18:36:37.185110       1 base_controller.go:73] Caches are synced for InstallerStateController 
2022-05-24T18:36:37.185210109Z I0524 18:36:37.185170       1 base_controller.go:110] Starting #1 worker of InstallerStateController controller ...
2022-05-24T18:36:37.185282410Z I0524 18:36:37.185256       1 base_controller.go:73] Caches are synced for StaticResourceController 
2022-05-24T18:36:37.185317137Z I0524 18:36:37.185303       1 base_controller.go:110] Starting #1 worker of StaticResourceController controller ...
2022-05-24T18:36:37.185468976Z I0524 18:36:37.185446       1 base_controller.go:73] Caches are synced for PruneController 
2022-05-24T18:36:37.185508368Z I0524 18:36:37.185497       1 base_controller.go:110] Starting #1 worker of PruneController controller ...
2022-05-24T18:36:37.185643767Z I0524 18:36:37.185602       1 base_controller.go:73] Caches are synced for InstallerController 
2022-05-24T18:36:37.185643767Z I0524 18:36:37.185621       1 base_controller.go:110] Starting #1 worker of InstallerController controller ...
2022-05-24T18:36:37.185688817Z I0524 18:36:37.185662       1 base_controller.go:73] Caches are synced for StatusSyncer_kube-scheduler 
2022-05-24T18:36:37.185728527Z I0524 18:36:37.185716       1 base_controller.go:110] Starting #1 worker of StatusSyncer_kube-scheduler controller ...
2022-05-24T18:36:37.289224951Z I0524 18:36:37.285700       1 base_controller.go:73] Caches are synced for ConfigObserver 
2022-05-24T18:36:37.289224951Z I0524 18:36:37.285719       1 base_controller.go:110] Starting #1 worker of ConfigObserver controller ...
2022-05-24T18:36:37.786015687Z I0524 18:36:37.785977       1 base_controller.go:73] Caches are synced for NodeController 
2022-05-24T18:36:37.786123200Z I0524 18:36:37.786112       1 base_controller.go:110] Starting #1 worker of NodeController controller ...
2022-05-24T18:36:37.786707775Z I0524 18:36:37.786080       1 base_controller.go:73] Caches are synced for GuardController 
2022-05-24T18:36:37.786766478Z I0524 18:36:37.786753       1 base_controller.go:110] Starting #1 worker of GuardController controller ...
2022-05-24T18:36:37.884113562Z I0524 18:36:37.884071       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:36:37.985517030Z I0524 18:36:37.985474       1 base_controller.go:73] Caches are synced for ResourceSyncController 
2022-05-24T18:36:37.985517030Z I0524 18:36:37.985502       1 base_controller.go:110] Starting #1 worker of ResourceSyncController controller ...
2022-05-24T18:36:37.987283152Z I0524 18:36:37.987238       1 base_controller.go:73] Caches are synced for TargetConfigController 
2022-05-24T18:36:37.987348084Z I0524 18:36:37.987321       1 base_controller.go:110] Starting #1 worker of TargetConfigController controller ...
2022-05-24T18:36:38.282875444Z I0524 18:36:38.282836       1 request.go:665] Waited for 1.097325281s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-149-121.ec2.internal
2022-05-24T18:36:39.482886000Z I0524 18:36:39.482843       1 request.go:665] Waited for 1.494988455s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/config
2022-05-24T18:36:40.552484375Z I0524 18:36:40.552433       1 installer_controller.go:512] "ip-10-0-138-197.ec2.internal" is in transition to 10, but has not made progress because installer is not finished, but in Running phase
2022-05-24T18:36:40.682731121Z I0524 18:36:40.682700       1 request.go:665] Waited for 1.536415684s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods
2022-05-24T18:36:40.720869383Z I0524 18:36:40.720828       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-10-ip-10-0-149-121.ec2.internal -n openshift-kube-scheduler because it was missing
2022-05-24T18:36:41.682822545Z I0524 18:36:41.682778       1 request.go:665] Waited for 1.308285607s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa
2022-05-24T18:36:42.682880446Z I0524 18:36:42.682841       1 request.go:665] Waited for 1.384154077s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2022-05-24T18:36:43.299348663Z I0524 18:36:43.299299       1 installer_controller.go:512] "ip-10-0-138-197.ec2.internal" is in transition to 10, but has not made progress because installer is not finished, but in Running phase
2022-05-24T18:36:43.683034621Z I0524 18:36:43.682994       1 request.go:665] Waited for 1.379575864s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-169-205.ec2.internal
2022-05-24T18:36:43.697470875Z I0524 18:36:43.697430       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:36:45.490401688Z I0524 18:36:45.490342       1 installer_controller.go:512] "ip-10-0-138-197.ec2.internal" is in transition to 10, but has not made progress because installer is not finished, but in Running phase
2022-05-24T18:36:46.282708619Z I0524 18:36:46.282660       1 request.go:665] Waited for 1.16934421s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-149-121.ec2.internal
2022-05-24T18:36:47.482165304Z I0524 18:36:47.482129       1 request.go:665] Waited for 1.195516676s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-138-197.ec2.internal
2022-05-24T18:36:48.685342170Z I0524 18:36:48.685299       1 installer_controller.go:512] "ip-10-0-138-197.ec2.internal" is in transition to 10, but has not made progress because waiting for static pod of revision 10, found 7
2022-05-24T18:36:49.684725454Z I0524 18:36:49.684687       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:36:50.885588670Z I0524 18:36:50.885550       1 installer_controller.go:512] "ip-10-0-138-197.ec2.internal" is in transition to 10, but has not made progress because waiting for static pod of revision 10, found 7
2022-05-24T18:36:51.886886218Z I0524 18:36:51.886851       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:36:52.884980217Z I0524 18:36:52.884943       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:36:55.436205745Z I0524 18:36:55.436154       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:36:55.437052098Z I0524 18:36:55.437020       1 installer_controller.go:512] "ip-10-0-138-197.ec2.internal" is in transition to 10, but has not made progress because waiting for static pod of revision 10, found 7
2022-05-24T18:36:59.292392424Z I0524 18:36:59.292121       1 installer_controller.go:512] "ip-10-0-138-197.ec2.internal" is in transition to 10, but has not made progress because waiting for static pod of revision 10, found 7
2022-05-24T18:36:59.629286382Z I0524 18:36:59.629248       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:37:01.631501573Z I0524 18:37:01.631456       1 installer_controller.go:512] "ip-10-0-138-197.ec2.internal" is in transition to 10, but has not made progress because static pod is pending
2022-05-24T18:37:02.830360807Z I0524 18:37:02.830301       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:37:03.628454839Z I0524 18:37:03.628388       1 installer_controller.go:512] "ip-10-0-138-197.ec2.internal" is in transition to 10, but has not made progress because static pod is pending
2022-05-24T18:37:04.030657331Z I0524 18:37:04.030620       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:37:22.845063967Z I0524 18:37:22.845023       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:37:27.344107937Z I0524 18:37:27.344067       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:37:28.173001414Z I0524 18:37:28.172952       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:37:28.457841516Z I0524 18:37:28.457806       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:37:44.812993685Z I0524 18:37:44.812954       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:37:44.821930824Z I0524 18:37:44.821895       1 installer_controller.go:500] "ip-10-0-138-197.ec2.internal" moving to (v1.NodeStatus) {
2022-05-24T18:37:44.821930824Z  NodeName: (string) (len=28) "ip-10-0-138-197.ec2.internal",
2022-05-24T18:37:44.821930824Z  CurrentRevision: (int32) 10,
2022-05-24T18:37:44.821930824Z  TargetRevision: (int32) 0,
2022-05-24T18:37:44.821930824Z  LastFailedRevision: (int32) 10,
2022-05-24T18:37:44.821930824Z  LastFailedTime: (*v1.Time)(0xc0011eda40)(2022-05-24 18:35:53 +0000 UTC),
2022-05-24T18:37:44.821930824Z  LastFailedReason: (string) (len=15) "InstallerFailed",
2022-05-24T18:37:44.821930824Z  LastFailedCount: (int) 1,
2022-05-24T18:37:44.821930824Z  LastFallbackCount: (int) 0,
2022-05-24T18:37:44.821930824Z  LastFailedRevisionErrors: ([]string) (len=1 cap=1) {
2022-05-24T18:37:44.821930824Z   (string) (len=73) "installer: The container could not be located when the pod was terminated"
2022-05-24T18:37:44.821930824Z  }
2022-05-24T18:37:44.821930824Z }
2022-05-24T18:37:44.821930824Z  because static pod is ready
2022-05-24T18:37:44.832096482Z I0524 18:37:44.832057       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "ip-10-0-138-197.ec2.internal" from revision 7 to 10 because static pod is ready
2022-05-24T18:37:44.832639679Z I0524 18:37:44.832609       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:35:50Z","message":"NodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:27:28Z","message":"NodeInstallerProgressing: 1 nodes are at revision 7; 2 nodes are at revision 10","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:11:34Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 7; 2 nodes are at revision 10","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2022-05-24T18:37:44.839901278Z I0524 18:37:44.839869       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nNodeInstallerDegraded: 1 nodes are failing on revision 10:\nNodeInstallerDegraded: installer: The container could not be located when the pod was terminated" to "NodeControllerDegraded: All master nodes are ready",Progressing message changed from "NodeInstallerProgressing: 2 nodes are at revision 7; 1 nodes are at revision 10" to "NodeInstallerProgressing: 1 nodes are at revision 7; 2 nodes are at revision 10",Available message changed from "StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 7; 1 nodes are at revision 10" to "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 7; 2 nodes are at revision 10"
2022-05-24T18:37:45.990623684Z I0524 18:37:45.990581       1 request.go:665] Waited for 1.157149115s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/config
2022-05-24T18:37:49.395072273Z I0524 18:37:49.395035       1 installer_controller.go:524] node ip-10-0-169-205.ec2.internal with revision 7 is the oldest and needs new revision 10
2022-05-24T18:37:49.395101034Z I0524 18:37:49.395075       1 installer_controller.go:532] "ip-10-0-169-205.ec2.internal" moving to (v1.NodeStatus) {
2022-05-24T18:37:49.395101034Z  NodeName: (string) (len=28) "ip-10-0-169-205.ec2.internal",
2022-05-24T18:37:49.395101034Z  CurrentRevision: (int32) 7,
2022-05-24T18:37:49.395101034Z  TargetRevision: (int32) 10,
2022-05-24T18:37:49.395101034Z  LastFailedRevision: (int32) 0,
2022-05-24T18:37:49.395101034Z  LastFailedTime: (*v1.Time)(<nil>),
2022-05-24T18:37:49.395101034Z  LastFailedReason: (string) "",
2022-05-24T18:37:49.395101034Z  LastFailedCount: (int) 0,
2022-05-24T18:37:49.395101034Z  LastFallbackCount: (int) 0,
2022-05-24T18:37:49.395101034Z  LastFailedRevisionErrors: ([]string) <nil>
2022-05-24T18:37:49.395101034Z }
2022-05-24T18:37:49.404591573Z I0524 18:37:49.404534       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "ip-10-0-169-205.ec2.internal" from revision 7 to 10 because node ip-10-0-169-205.ec2.internal with revision 7 is the oldest
2022-05-24T18:37:50.591578996Z I0524 18:37:50.591543       1 request.go:665] Waited for 1.185360598s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-10-ip-10-0-169-205.ec2.internal
2022-05-24T18:37:51.607064331Z I0524 18:37:51.606509       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-10-ip-10-0-169-205.ec2.internal -n openshift-kube-scheduler because it was missing
2022-05-24T18:37:51.611405138Z I0524 18:37:51.611371       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:37:52.790774855Z I0524 18:37:52.790741       1 request.go:665] Waited for 1.184682624s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-10-ip-10-0-169-205.ec2.internal
2022-05-24T18:37:52.794639272Z I0524 18:37:52.794606       1 installer_controller.go:512] "ip-10-0-169-205.ec2.internal" is in transition to 10, but has not made progress because installer is not finished, but in Pending phase
2022-05-24T18:37:53.790823579Z I0524 18:37:53.790785       1 request.go:665] Waited for 1.195301238s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2022-05-24T18:37:54.994137020Z I0524 18:37:54.994098       1 installer_controller.go:512] "ip-10-0-169-205.ec2.internal" is in transition to 10, but has not made progress because installer is not finished, but in Running phase
2022-05-24T18:37:56.211504989Z I0524 18:37:56.211445       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:37:56.994151239Z I0524 18:37:56.994114       1 installer_controller.go:512] "ip-10-0-169-205.ec2.internal" is in transition to 10, but has not made progress because installer is not finished, but in Running phase
2022-05-24T18:38:00.596985574Z I0524 18:38:00.596948       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:38:02.307120606Z I0524 18:38:02.307080       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 11 triggered by "secret/localhost-recovery-client-token has changed"
2022-05-24T18:38:02.797216938Z I0524 18:38:02.796905       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/revision-status-11 -n openshift-kube-scheduler because it was missing
2022-05-24T18:38:03.399474658Z I0524 18:38:03.399416       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-scheduler-pod-11 -n openshift-kube-scheduler because it was missing
2022-05-24T18:38:03.997126066Z I0524 18:38:03.997081       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/config-11 -n openshift-kube-scheduler because it was missing
2022-05-24T18:38:04.610392056Z I0524 18:38:04.610336       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/serviceaccount-ca-11 -n openshift-kube-scheduler because it was missing
2022-05-24T18:38:05.797870097Z I0524 18:38:05.797814       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/scheduler-kubeconfig-11 -n openshift-kube-scheduler because it was missing
2022-05-24T18:38:06.395895098Z I0524 18:38:06.395847       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-scheduler-cert-syncer-kubeconfig-11 -n openshift-kube-scheduler because it was missing
2022-05-24T18:38:06.997730397Z I0524 18:38:06.997681       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/serving-cert-11 -n openshift-kube-scheduler because it was missing
2022-05-24T18:38:07.601574114Z I0524 18:38:07.601524       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/localhost-recovery-client-token-11 -n openshift-kube-scheduler because it was missing
2022-05-24T18:38:07.619262390Z I0524 18:38:07.617732       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionCreate' Revision 10 created because secret/localhost-recovery-client-token has changed
2022-05-24T18:38:08.790933679Z I0524 18:38:08.790891       1 request.go:665] Waited for 1.173539111s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/revision-pruner-11-ip-10-0-149-121.ec2.internal
2022-05-24T18:38:09.799916992Z I0524 18:38:09.799845       1 installer_controller.go:500] "ip-10-0-169-205.ec2.internal" moving to (v1.NodeStatus) {
2022-05-24T18:38:09.799916992Z  NodeName: (string) (len=28) "ip-10-0-169-205.ec2.internal",
2022-05-24T18:38:09.799916992Z  CurrentRevision: (int32) 7,
2022-05-24T18:38:09.799916992Z  TargetRevision: (int32) 11,
2022-05-24T18:38:09.799916992Z  LastFailedRevision: (int32) 0,
2022-05-24T18:38:09.799916992Z  LastFailedTime: (*v1.Time)(<nil>),
2022-05-24T18:38:09.799916992Z  LastFailedReason: (string) "",
2022-05-24T18:38:09.799916992Z  LastFailedCount: (int) 0,
2022-05-24T18:38:09.799916992Z  LastFallbackCount: (int) 0,
2022-05-24T18:38:09.799916992Z  LastFailedRevisionErrors: ([]string) <nil>
2022-05-24T18:38:09.799916992Z }
2022-05-24T18:38:09.799916992Z  because new revision pending
2022-05-24T18:38:09.810399850Z I0524 18:38:09.810358       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:35:50Z","message":"NodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:27:28Z","message":"NodeInstallerProgressing: 1 nodes are at revision 7; 2 nodes are at revision 10; 0 nodes have achieved new revision 11","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:11:34Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 7; 2 nodes are at revision 10; 0 nodes have achieved new revision 11","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2022-05-24T18:38:09.820280657Z I0524 18:38:09.820238       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Progressing message changed from "NodeInstallerProgressing: 1 nodes are at revision 7; 2 nodes are at revision 10" to "NodeInstallerProgressing: 1 nodes are at revision 7; 2 nodes are at revision 10; 0 nodes have achieved new revision 11",Available message changed from "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 7; 2 nodes are at revision 10" to "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 7; 2 nodes are at revision 10; 0 nodes have achieved new revision 11"
2022-05-24T18:38:09.990629344Z I0524 18:38:09.990596       1 request.go:665] Waited for 1.195192403s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods
2022-05-24T18:38:09.999136504Z I0524 18:38:09.999087       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-11-ip-10-0-149-121.ec2.internal -n openshift-kube-scheduler because it was missing
2022-05-24T18:38:10.595149958Z I0524 18:38:10.595105       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:38:10.991014199Z I0524 18:38:10.990978       1 request.go:665] Waited for 1.191270511s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2022-05-24T18:38:12.190369103Z I0524 18:38:12.190332       1 request.go:665] Waited for 1.59467504s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-138-197.ec2.internal
2022-05-24T18:38:12.798341239Z I0524 18:38:12.798289       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-11-ip-10-0-169-205.ec2.internal -n openshift-kube-scheduler because it was missing
2022-05-24T18:38:12.999317887Z I0524 18:38:12.999251       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-11-ip-10-0-138-197.ec2.internal -n openshift-kube-scheduler because it was missing
2022-05-24T18:38:13.190895265Z I0524 18:38:13.190851       1 request.go:665] Waited for 1.394891404s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/secrets/serving-cert
2022-05-24T18:38:13.598858878Z I0524 18:38:13.598827       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:38:14.191166801Z I0524 18:38:14.191134       1 request.go:665] Waited for 1.392883991s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-11-ip-10-0-169-205.ec2.internal
2022-05-24T18:38:14.195799058Z I0524 18:38:14.195772       1 installer_controller.go:512] "ip-10-0-169-205.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Pending phase
2022-05-24T18:38:15.390348893Z I0524 18:38:15.390302       1 request.go:665] Waited for 1.393652745s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2022-05-24T18:38:15.797904971Z I0524 18:38:15.797840       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-11-ip-10-0-169-205.ec2.internal -n openshift-kube-scheduler because it was missing
2022-05-24T18:38:16.391193435Z I0524 18:38:16.391143       1 request.go:665] Waited for 1.397409184s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-138-197.ec2.internal
2022-05-24T18:38:16.995097783Z I0524 18:38:16.995061       1 installer_controller.go:512] "ip-10-0-169-205.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2022-05-24T18:38:17.590767224Z I0524 18:38:17.590731       1 request.go:665] Waited for 1.38079747s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler
2022-05-24T18:38:18.791170694Z I0524 18:38:18.791131       1 request.go:665] Waited for 1.396493421s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client
2022-05-24T18:38:19.199790020Z I0524 18:38:19.199758       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:38:19.794522515Z I0524 18:38:19.794481       1 installer_controller.go:512] "ip-10-0-169-205.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2022-05-24T18:38:19.991124822Z I0524 18:38:19.991092       1 request.go:665] Waited for 1.395677702s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/revision-pruner-11-ip-10-0-169-205.ec2.internal
2022-05-24T18:38:21.190288263Z I0524 18:38:21.190244       1 request.go:665] Waited for 1.394878957s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-11-ip-10-0-169-205.ec2.internal
2022-05-24T18:38:22.195041667Z I0524 18:38:22.195007       1 installer_controller.go:512] "ip-10-0-169-205.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2022-05-24T18:38:23.190518151Z I0524 18:38:23.190484       1 request.go:665] Waited for 1.041804606s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler
2022-05-24T18:38:24.394770041Z I0524 18:38:24.394735       1 installer_controller.go:512] "ip-10-0-169-205.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2022-05-24T18:38:25.594803411Z I0524 18:38:25.594765       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:38:26.797339207Z I0524 18:38:26.797303       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:38:28.194339467Z I0524 18:38:28.194304       1 installer_controller.go:512] "ip-10-0-169-205.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2022-05-24T18:38:29.794917400Z I0524 18:38:29.794878       1 installer_controller.go:512] "ip-10-0-169-205.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2022-05-24T18:38:30.197300293Z I0524 18:38:30.197267       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:38:36.436869508Z I0524 18:38:36.436775       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:35:50Z","message":"NodeControllerDegraded: The master nodes not ready: node \"ip-10-0-149-121.ec2.internal\" not ready since 2022-05-24 18:38:36 +0000 UTC because NodeStatusUnknown (Kubelet stopped posting node status.)","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:27:28Z","message":"NodeInstallerProgressing: 1 nodes are at revision 7; 2 nodes are at revision 10; 0 nodes have achieved new revision 11","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:11:34Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 7; 2 nodes are at revision 10; 0 nodes have achieved new revision 11","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2022-05-24T18:38:36.481778941Z I0524 18:38:36.481740       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:38:36.496421581Z I0524 18:38:36.496379       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready" to "NodeControllerDegraded: The master nodes not ready: node \"ip-10-0-149-121.ec2.internal\" not ready since 2022-05-24 18:38:36 +0000 UTC because NodeStatusUnknown (Kubelet stopped posting node status.)"
2022-05-24T18:38:37.236619890Z I0524 18:38:37.236579       1 installer_controller.go:512] "ip-10-0-169-205.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2022-05-24T18:38:37.632450507Z I0524 18:38:37.632407       1 request.go:665] Waited for 1.112694774s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler
2022-05-24T18:38:38.241988890Z I0524 18:38:38.241956       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:38:38.632541911Z I0524 18:38:38.632503       1 request.go:665] Waited for 1.44636326s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2022-05-24T18:38:39.632881287Z I0524 18:38:39.632846       1 request.go:665] Waited for 1.390533554s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-138-197.ec2.internal
2022-05-24T18:38:40.035755694Z I0524 18:38:40.035719       1 installer_controller.go:512] "ip-10-0-169-205.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2022-05-24T18:38:43.835593818Z I0524 18:38:43.835551       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:38:45.436087895Z I0524 18:38:45.436053       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:38:48.386337338Z I0524 18:38:48.386298       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:39:00.331379452Z I0524 18:39:00.331342       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:39:00.353366854Z I0524 18:39:00.353327       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:39:08.107252580Z I0524 18:39:08.107215       1 installer_controller.go:512] "ip-10-0-169-205.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2022-05-24T18:39:08.712233394Z I0524 18:39:08.711193       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:39:09.884765201Z I0524 18:39:09.884710       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:39:10.901357918Z I0524 18:39:10.901319       1 request.go:665] Waited for 1.016217076s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-138-197.ec2.internal
2022-05-24T18:39:12.305208297Z I0524 18:39:12.305152       1 installer_controller.go:512] "ip-10-0-169-205.ec2.internal" is in transition to 11, but has not made progress because waiting for static pod of revision 11, found 7
2022-05-24T18:39:18.711082069Z I0524 18:39:18.711043       1 installer_controller.go:512] "ip-10-0-169-205.ec2.internal" is in transition to 11, but has not made progress because waiting for static pod of revision 11, found 7
2022-05-24T18:39:18.716943543Z I0524 18:39:18.716907       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:39:19.461889536Z I0524 18:39:19.461854       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:39:21.101876364Z I0524 18:39:21.101829       1 installer_controller.go:512] "ip-10-0-169-205.ec2.internal" is in transition to 11, but has not made progress because static pod is pending
2022-05-24T18:39:22.101201499Z I0524 18:39:22.101059       1 request.go:665] Waited for 1.005375361s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/config
2022-05-24T18:39:24.303213668Z I0524 18:39:24.303157       1 installer_controller.go:512] "ip-10-0-169-205.ec2.internal" is in transition to 11, but has not made progress because static pod is pending
2022-05-24T18:39:24.701704861Z I0524 18:39:24.701671       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:39:24.898164321Z I0524 18:39:24.898127       1 request.go:665] Waited for 1.194420976s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2022-05-24T18:39:25.898585834Z I0524 18:39:25.898550       1 request.go:665] Waited for 1.196494348s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-138-197.ec2.internal
2022-05-24T18:39:26.706350397Z I0524 18:39:26.706312       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:39:27.103197045Z I0524 18:39:27.103148       1 installer_controller.go:512] "ip-10-0-169-205.ec2.internal" is in transition to 11, but has not made progress because static pod is pending
2022-05-24T18:39:43.975563295Z I0524 18:39:43.975528       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:39:46.835378945Z I0524 18:39:46.835339       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:39:46.883726936Z I0524 18:39:46.883660       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:39:46.975235769Z I0524 18:39:46.970032       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:00.301618119Z I0524 18:40:00.301583       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:00.317990161Z I0524 18:40:00.317953       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:01.897504440Z I0524 18:40:01.897462       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:06.906600107Z I0524 18:40:06.906560       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:06.917956613Z I0524 18:40:06.917920       1 installer_controller.go:500] "ip-10-0-169-205.ec2.internal" moving to (v1.NodeStatus) {
2022-05-24T18:40:06.917956613Z  NodeName: (string) (len=28) "ip-10-0-169-205.ec2.internal",
2022-05-24T18:40:06.917956613Z  CurrentRevision: (int32) 11,
2022-05-24T18:40:06.917956613Z  TargetRevision: (int32) 0,
2022-05-24T18:40:06.917956613Z  LastFailedRevision: (int32) 0,
2022-05-24T18:40:06.917956613Z  LastFailedTime: (*v1.Time)(<nil>),
2022-05-24T18:40:06.917956613Z  LastFailedReason: (string) "",
2022-05-24T18:40:06.917956613Z  LastFailedCount: (int) 0,
2022-05-24T18:40:06.917956613Z  LastFallbackCount: (int) 0,
2022-05-24T18:40:06.917956613Z  LastFailedRevisionErrors: ([]string) <nil>
2022-05-24T18:40:06.917956613Z }
2022-05-24T18:40:06.917956613Z  because static pod is ready
2022-05-24T18:40:06.929391696Z I0524 18:40:06.929325       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:35:50Z","message":"NodeControllerDegraded: The master nodes not ready: node \"ip-10-0-149-121.ec2.internal\" not ready since 2022-05-24 18:38:36 +0000 UTC because NodeStatusUnknown (Kubelet stopped posting node status.)","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:27:28Z","message":"NodeInstallerProgressing: 2 nodes are at revision 10; 1 nodes are at revision 11","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:11:34Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 10; 1 nodes are at revision 11","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2022-05-24T18:40:06.933109495Z I0524 18:40:06.933068       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "ip-10-0-169-205.ec2.internal" from revision 7 to 11 because static pod is ready
2022-05-24T18:40:06.941040186Z I0524 18:40:06.940994       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Progressing message changed from "NodeInstallerProgressing: 1 nodes are at revision 7; 2 nodes are at revision 10; 0 nodes have achieved new revision 11" to "NodeInstallerProgressing: 2 nodes are at revision 10; 1 nodes are at revision 11",Available message changed from "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 7; 2 nodes are at revision 10; 0 nodes have achieved new revision 11" to "StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 10; 1 nodes are at revision 11"
2022-05-24T18:40:08.103847814Z I0524 18:40:08.103807       1 request.go:665] Waited for 1.170566558s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-149-121.ec2.internal
2022-05-24T18:40:09.104208564Z I0524 18:40:09.104166       1 request.go:665] Waited for 1.182377479s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler
2022-05-24T18:40:10.104940641Z I0524 18:40:10.104900       1 request.go:665] Waited for 1.195858747s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/revision-pruner-11-ip-10-0-169-205.ec2.internal
2022-05-24T18:40:10.508628379Z I0524 18:40:10.508594       1 installer_controller.go:524] node ip-10-0-149-121.ec2.internal with revision 10 is the oldest not ready and needs new revision 11
2022-05-24T18:40:10.508655788Z I0524 18:40:10.508633       1 installer_controller.go:532] "ip-10-0-149-121.ec2.internal" moving to (v1.NodeStatus) {
2022-05-24T18:40:10.508655788Z  NodeName: (string) (len=28) "ip-10-0-149-121.ec2.internal",
2022-05-24T18:40:10.508655788Z  CurrentRevision: (int32) 10,
2022-05-24T18:40:10.508655788Z  TargetRevision: (int32) 11,
2022-05-24T18:40:10.508655788Z  LastFailedRevision: (int32) 0,
2022-05-24T18:40:10.508655788Z  LastFailedTime: (*v1.Time)(<nil>),
2022-05-24T18:40:10.508655788Z  LastFailedReason: (string) "",
2022-05-24T18:40:10.508655788Z  LastFailedCount: (int) 0,
2022-05-24T18:40:10.508655788Z  LastFallbackCount: (int) 0,
2022-05-24T18:40:10.508655788Z  LastFailedRevisionErrors: ([]string) <nil>
2022-05-24T18:40:10.508655788Z }
2022-05-24T18:40:10.520141729Z I0524 18:40:10.520099       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "ip-10-0-149-121.ec2.internal" from revision 10 to 11 because node ip-10-0-149-121.ec2.internal with revision 10 is the oldest not ready
2022-05-24T18:40:10.907199909Z I0524 18:40:10.907140       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:11.703941485Z I0524 18:40:11.703908       1 request.go:665] Waited for 1.181491274s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-149-121.ec2.internal
2022-05-24T18:40:12.704493960Z I0524 18:40:12.704457       1 request.go:665] Waited for 1.395730641s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler
2022-05-24T18:40:13.121033000Z I0524 18:40:13.120977       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-11-ip-10-0-149-121.ec2.internal -n openshift-kube-scheduler because it was missing
2022-05-24T18:40:13.903813579Z I0524 18:40:13.903779       1 request.go:665] Waited for 1.185466533s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler
2022-05-24T18:40:14.306747387Z I0524 18:40:14.306707       1 installer_controller.go:512] "ip-10-0-149-121.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Pending phase
2022-05-24T18:40:14.904624651Z I0524 18:40:14.904591       1 request.go:665] Waited for 1.392291925s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-138-197.ec2.internal
2022-05-24T18:40:16.104483443Z I0524 18:40:16.104443       1 request.go:665] Waited for 1.196322932s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-138-197.ec2.internal
2022-05-24T18:40:16.108556077Z I0524 18:40:16.108524       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:16.908382950Z I0524 18:40:16.908341       1 installer_controller.go:512] "ip-10-0-149-121.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Pending phase
2022-05-24T18:40:17.303961402Z I0524 18:40:17.303922       1 request.go:665] Waited for 1.195033179s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-169-205.ec2.internal
2022-05-24T18:40:19.507032922Z I0524 18:40:19.506985       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:21.106745177Z I0524 18:40:21.106704       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:21.796882235Z I0524 18:40:21.794350       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:25.789635474Z I0524 18:40:25.789600       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:25.837606133Z I0524 18:40:25.837565       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:35:50Z","message":"NodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:27:28Z","message":"NodeInstallerProgressing: 2 nodes are at revision 10; 1 nodes are at revision 11","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:11:34Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 10; 1 nodes are at revision 11","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2022-05-24T18:40:25.871628070Z I0524 18:40:25.870717       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "NodeControllerDegraded: The master nodes not ready: node \"ip-10-0-149-121.ec2.internal\" not ready since 2022-05-24 18:38:36 +0000 UTC because NodeStatusUnknown (Kubelet stopped posting node status.)" to "NodeControllerDegraded: All master nodes are ready"
2022-05-24T18:40:26.193940959Z I0524 18:40:26.193892       1 installer_controller.go:512] "ip-10-0-149-121.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Pending phase
2022-05-24T18:40:26.991529910Z I0524 18:40:26.990450       1 request.go:665] Waited for 1.05152995s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler
2022-05-24T18:40:27.793481954Z I0524 18:40:27.793442       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:28.190714400Z I0524 18:40:28.190674       1 request.go:665] Waited for 1.120359117s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2022-05-24T18:40:29.190761732Z I0524 18:40:29.190723       1 request.go:665] Waited for 1.192921469s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client
2022-05-24T18:40:29.593881248Z I0524 18:40:29.593846       1 installer_controller.go:512] "ip-10-0-149-121.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Pending phase
2022-05-24T18:40:30.202232641Z I0524 18:40:30.202157       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:31.194451927Z I0524 18:40:31.194412       1 installer_controller.go:512] "ip-10-0-149-121.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Pending phase
2022-05-24T18:40:32.994456216Z I0524 18:40:32.994412       1 installer_controller.go:512] "ip-10-0-149-121.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Pending phase
2022-05-24T18:40:35.201783045Z I0524 18:40:35.201742       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:36.594093464Z I0524 18:40:36.594051       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:38.190843744Z I0524 18:40:38.190809       1 request.go:665] Waited for 1.003911237s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2022-05-24T18:40:41.202640835Z I0524 18:40:41.202596       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:41.205993885Z I0524 18:40:41.205963       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:41.990638589Z I0524 18:40:41.990602       1 request.go:665] Waited for 1.094521271s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-149-121.ec2.internal
2022-05-24T18:40:42.795280348Z I0524 18:40:42.795242       1 installer_controller.go:512] "ip-10-0-149-121.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2022-05-24T18:40:42.990879499Z I0524 18:40:42.990829       1 request.go:665] Waited for 1.188924191s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2022-05-24T18:40:44.590947639Z I0524 18:40:44.590915       1 request.go:665] Waited for 1.015041736s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2022-05-24T18:40:45.195525913Z I0524 18:40:45.195485       1 installer_controller.go:512] "ip-10-0-149-121.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2022-05-24T18:40:45.591281825Z I0524 18:40:45.591247       1 request.go:665] Waited for 1.197218147s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa
2022-05-24T18:40:45.799251740Z I0524 18:40:45.799211       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:46.995278906Z I0524 18:40:46.995222       1 installer_controller.go:512] "ip-10-0-149-121.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2022-05-24T18:40:48.905550986Z I0524 18:40:48.905511       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:48.994420341Z I0524 18:40:48.994372       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:50.610266131Z I0524 18:40:50.610165       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/openshift-kube-scheduler-guard-ip-10-0-149-121.ec2.internal -n openshift-kube-scheduler because it was missing
2022-05-24T18:40:50.993908810Z I0524 18:40:50.993872       1 installer_controller.go:512] "ip-10-0-149-121.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2022-05-24T18:40:52.598940457Z I0524 18:40:52.598894       1 installer_controller.go:512] "ip-10-0-149-121.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2022-05-24T18:40:53.992774734Z I0524 18:40:53.992734       1 installer_controller.go:512] "ip-10-0-149-121.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2022-05-24T18:40:55.597072413Z I0524 18:40:55.597000       1 installer_controller.go:512] "ip-10-0-149-121.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2022-05-24T18:41:13.576538796Z I0524 18:41:13.576503       1 installer_controller.go:512] "ip-10-0-149-121.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2022-05-24T18:41:19.705282798Z I0524 18:41:19.705239       1 installer_controller.go:512] "ip-10-0-149-121.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2022-05-24T18:41:21.733727805Z I0524 18:41:21.733671       1 installer_controller.go:512] "ip-10-0-149-121.ec2.internal" is in transition to 11, but has not made progress because waiting for static pod of revision 11, found 10
2022-05-24T18:41:30.640056952Z I0524 18:41:30.640019       1 installer_controller.go:512] "ip-10-0-149-121.ec2.internal" is in transition to 11, but has not made progress because waiting for static pod of revision 11, found 10
2022-05-24T18:41:32.428594007Z I0524 18:41:32.428532       1 request.go:665] Waited for 1.044975677s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-138-197.ec2.internal
2022-05-24T18:41:33.428947995Z I0524 18:41:33.428910       1 request.go:665] Waited for 1.196590512s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-11-ip-10-0-149-121.ec2.internal
2022-05-24T18:41:34.429160533Z I0524 18:41:34.429102       1 request.go:665] Waited for 1.19540176s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-169-205.ec2.internal
2022-05-24T18:41:34.632846071Z I0524 18:41:34.632808       1 installer_controller.go:512] "ip-10-0-149-121.ec2.internal" is in transition to 11, but has not made progress because static pod is pending
2022-05-24T18:41:37.240827139Z I0524 18:41:37.240784       1 request.go:665] Waited for 1.009276153s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-169-205.ec2.internal
2022-05-24T18:41:37.439089654Z I0524 18:41:37.438980       1 installer_controller.go:512] "ip-10-0-149-121.ec2.internal" is in transition to 11, but has not made progress because static pod is pending
2022-05-24T18:41:38.429275812Z I0524 18:41:38.429242       1 request.go:665] Waited for 1.238729493s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler
2022-05-24T18:41:40.831878807Z I0524 18:41:40.831838       1 installer_controller.go:512] "ip-10-0-149-121.ec2.internal" is in transition to 11, but has not made progress because static pod is pending
2022-05-24T18:42:22.034090798Z I0524 18:42:22.034038       1 installer_controller.go:500] "ip-10-0-149-121.ec2.internal" moving to (v1.NodeStatus) {
2022-05-24T18:42:22.034090798Z  NodeName: (string) (len=28) "ip-10-0-149-121.ec2.internal",
2022-05-24T18:42:22.034090798Z  CurrentRevision: (int32) 11,
2022-05-24T18:42:22.034090798Z  TargetRevision: (int32) 0,
2022-05-24T18:42:22.034090798Z  LastFailedRevision: (int32) 0,
2022-05-24T18:42:22.034090798Z  LastFailedTime: (*v1.Time)(<nil>),
2022-05-24T18:42:22.034090798Z  LastFailedReason: (string) "",
2022-05-24T18:42:22.034090798Z  LastFailedCount: (int) 0,
2022-05-24T18:42:22.034090798Z  LastFallbackCount: (int) 0,
2022-05-24T18:42:22.034090798Z  LastFailedRevisionErrors: ([]string) <nil>
2022-05-24T18:42:22.034090798Z }
2022-05-24T18:42:22.034090798Z  because static pod is ready
2022-05-24T18:42:22.043552051Z I0524 18:42:22.043502       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "ip-10-0-149-121.ec2.internal" from revision 10 to 11 because static pod is ready
2022-05-24T18:42:22.043881338Z I0524 18:42:22.043840       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:35:50Z","message":"NodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:27:28Z","message":"NodeInstallerProgressing: 1 nodes are at revision 10; 2 nodes are at revision 11","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:11:34Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 10; 2 nodes are at revision 11","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2022-05-24T18:42:22.055458377Z I0524 18:42:22.054950       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Progressing message changed from "NodeInstallerProgressing: 2 nodes are at revision 10; 1 nodes are at revision 11" to "NodeInstallerProgressing: 1 nodes are at revision 10; 2 nodes are at revision 11",Available message changed from "StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 10; 1 nodes are at revision 11" to "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 10; 2 nodes are at revision 11"
2022-05-24T18:42:23.229976367Z I0524 18:42:23.229937       1 request.go:665] Waited for 1.185902649s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-149-121.ec2.internal
2022-05-24T18:42:24.230585446Z I0524 18:42:24.230531       1 request.go:665] Waited for 1.197269081s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-138-197.ec2.internal
2022-05-24T18:42:25.429670642Z I0524 18:42:25.429626       1 request.go:665] Waited for 1.195020292s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-169-205.ec2.internal
2022-05-24T18:42:26.430156068Z I0524 18:42:26.430111       1 request.go:665] Waited for 1.196953746s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/secrets/serving-cert
2022-05-24T18:42:28.232678608Z I0524 18:42:28.232642       1 installer_controller.go:524] node ip-10-0-138-197.ec2.internal with revision 10 is the oldest and needs new revision 11
2022-05-24T18:42:28.232737546Z I0524 18:42:28.232690       1 installer_controller.go:532] "ip-10-0-138-197.ec2.internal" moving to (v1.NodeStatus) {
2022-05-24T18:42:28.232737546Z  NodeName: (string) (len=28) "ip-10-0-138-197.ec2.internal",
2022-05-24T18:42:28.232737546Z  CurrentRevision: (int32) 10,
2022-05-24T18:42:28.232737546Z  TargetRevision: (int32) 11,
2022-05-24T18:42:28.232737546Z  LastFailedRevision: (int32) 10,
2022-05-24T18:42:28.232737546Z  LastFailedTime: (*v1.Time)(0xc001a2a810)(2022-05-24 18:35:53 +0000 UTC),
2022-05-24T18:42:28.232737546Z  LastFailedReason: (string) (len=15) "InstallerFailed",
2022-05-24T18:42:28.232737546Z  LastFailedCount: (int) 1,
2022-05-24T18:42:28.232737546Z  LastFallbackCount: (int) 0,
2022-05-24T18:42:28.232737546Z  LastFailedRevisionErrors: ([]string) (len=1 cap=1) {
2022-05-24T18:42:28.232737546Z   (string) (len=73) "installer: The container could not be located when the pod was terminated"
2022-05-24T18:42:28.232737546Z  }
2022-05-24T18:42:28.232737546Z }
2022-05-24T18:42:28.249238251Z I0524 18:42:28.246573       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "ip-10-0-138-197.ec2.internal" from revision 10 to 11 because node ip-10-0-138-197.ec2.internal with revision 10 is the oldest
2022-05-24T18:42:29.430018052Z I0524 18:42:29.429969       1 request.go:665] Waited for 1.18249916s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-11-ip-10-0-138-197.ec2.internal
2022-05-24T18:42:30.440043139Z I0524 18:42:30.439996       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-11-ip-10-0-138-197.ec2.internal -n openshift-kube-scheduler because it was missing
2022-05-24T18:42:31.436062616Z I0524 18:42:31.436021       1 installer_controller.go:512] "ip-10-0-138-197.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Pending phase
2022-05-24T18:42:31.630384184Z I0524 18:42:31.630345       1 request.go:665] Waited for 1.189175217s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2022-05-24T18:42:32.829915437Z I0524 18:42:32.829878       1 request.go:665] Waited for 1.390549738s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-11-ip-10-0-138-197.ec2.internal
2022-05-24T18:42:33.830422808Z I0524 18:42:33.830385       1 request.go:665] Waited for 1.396578667s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler
2022-05-24T18:42:34.232817696Z I0524 18:42:34.232774       1 installer_controller.go:512] "ip-10-0-138-197.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2022-05-24T18:42:35.029607308Z I0524 18:42:35.029566       1 request.go:665] Waited for 1.393482654s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/kube-scheduler-pod
2022-05-24T18:42:36.030636881Z I0524 18:42:36.030596       1 request.go:665] Waited for 1.198608393s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-149-121.ec2.internal
2022-05-24T18:42:36.832742987Z I0524 18:42:36.832705       1 installer_controller.go:512] "ip-10-0-138-197.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2022-05-24T18:42:38.229956913Z I0524 18:42:38.229921       1 request.go:665] Waited for 1.041862751s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2022-05-24T18:43:06.970623610Z I0524 18:43:06.970588       1 request.go:665] Waited for 1.074819344s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-11-ip-10-0-138-197.ec2.internal
2022-05-24T18:43:08.970271274Z I0524 18:43:08.970235       1 request.go:665] Waited for 1.063679857s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2022-05-24T18:43:09.172768292Z I0524 18:43:09.172733       1 installer_controller.go:512] "ip-10-0-138-197.ec2.internal" is in transition to 11, but has not made progress because waiting for static pod of revision 11, found 10
2022-05-24T18:43:11.573218682Z I0524 18:43:11.573162       1 installer_controller.go:512] "ip-10-0-138-197.ec2.internal" is in transition to 11, but has not made progress because waiting for static pod of revision 11, found 10
2022-05-24T18:43:16.570971066Z I0524 18:43:16.570933       1 request.go:665] Waited for 1.135847136s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-149-121.ec2.internal
2022-05-24T18:43:18.373842734Z I0524 18:43:18.373808       1 installer_controller.go:512] "ip-10-0-138-197.ec2.internal" is in transition to 11, but has not made progress because waiting for static pod of revision 11, found 10
2022-05-24T18:43:21.173577944Z I0524 18:43:21.173538       1 installer_controller.go:512] "ip-10-0-138-197.ec2.internal" is in transition to 11, but has not made progress because static pod is pending
2022-05-24T18:43:21.770036117Z I0524 18:43:21.770002       1 request.go:665] Waited for 1.015321866s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/config
2022-05-24T18:43:22.770642421Z I0524 18:43:22.770607       1 request.go:665] Waited for 1.186051141s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler
2022-05-24T18:43:24.373174653Z I0524 18:43:24.373139       1 installer_controller.go:512] "ip-10-0-138-197.ec2.internal" is in transition to 11, but has not made progress because static pod is pending
2022-05-24T18:44:16.850567808Z I0524 18:44:16.850514       1 installer_controller.go:500] "ip-10-0-138-197.ec2.internal" moving to (v1.NodeStatus) {
2022-05-24T18:44:16.850567808Z  NodeName: (string) (len=28) "ip-10-0-138-197.ec2.internal",
2022-05-24T18:44:16.850567808Z  CurrentRevision: (int32) 11,
2022-05-24T18:44:16.850567808Z  TargetRevision: (int32) 0,
2022-05-24T18:44:16.850567808Z  LastFailedRevision: (int32) 10,
2022-05-24T18:44:16.850567808Z  LastFailedTime: (*v1.Time)(0xc000764408)(2022-05-24 18:35:53 +0000 UTC),
2022-05-24T18:44:16.850567808Z  LastFailedReason: (string) (len=15) "InstallerFailed",
2022-05-24T18:44:16.850567808Z  LastFailedCount: (int) 1,
2022-05-24T18:44:16.850567808Z  LastFallbackCount: (int) 0,
2022-05-24T18:44:16.850567808Z  LastFailedRevisionErrors: ([]string) (len=1 cap=1) {
2022-05-24T18:44:16.850567808Z   (string) (len=73) "installer: The container could not be located when the pod was terminated"
2022-05-24T18:44:16.850567808Z  }
2022-05-24T18:44:16.850567808Z }
2022-05-24T18:44:16.850567808Z  because static pod is ready
2022-05-24T18:44:16.860709936Z I0524 18:44:16.860672       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:35:50Z","message":"NodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:44:16Z","message":"NodeInstallerProgressing: 3 nodes are at revision 11","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:11:34Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 11","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2022-05-24T18:44:16.861341846Z I0524 18:44:16.861303       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "ip-10-0-138-197.ec2.internal" from revision 10 to 11 because static pod is ready
2022-05-24T18:44:16.868782254Z I0524 18:44:16.868731       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"1a831732-9132-48af-8708-993b3a98291a", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Progressing changed from True to False ("NodeInstallerProgressing: 3 nodes are at revision 11"),Available message changed from "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 10; 2 nodes are at revision 11" to "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 11"
2022-05-24T18:44:18.042680473Z I0524 18:44:18.042637       1 request.go:665] Waited for 1.181321286s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler
2022-05-24T18:44:19.042916807Z I0524 18:44:19.042878       1 request.go:665] Waited for 1.395726554s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/revision-pruner-11-ip-10-0-138-197.ec2.internal
2022-05-24T18:44:20.043248216Z I0524 18:44:20.043210       1 request.go:665] Waited for 1.196781333s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client
2022-05-24T18:46:38.239879380Z I0524 18:46:38.239838       1 request.go:665] Waited for 1.143210828s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2022-05-24T18:46:39.240039875Z I0524 18:46:39.240001       1 request.go:665] Waited for 1.395726611s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-138-197.ec2.internal
2022-05-24T18:46:40.240781747Z I0524 18:46:40.240741       1 request.go:665] Waited for 1.39754964s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/secrets/localhost-recovery-client-token
2022-05-24T18:56:38.285046394Z I0524 18:56:38.285009       1 request.go:665] Waited for 1.185017314s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler
2022-05-24T18:56:39.285745842Z I0524 18:56:39.285704       1 request.go:665] Waited for 1.396728559s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/secrets/localhost-recovery-client-token
2022-05-24T18:56:40.485332120Z I0524 18:56:40.485298       1 request.go:665] Waited for 1.196545305s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/scheduler-kubeconfig
2022-05-24T19:06:38.286227750Z I0524 19:06:38.286173       1 request.go:665] Waited for 1.176769684s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler
2022-05-24T19:06:39.487436242Z I0524 19:06:39.487398       1 request.go:665] Waited for 1.393350293s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2022-05-24T19:16:38.287201610Z I0524 19:16:38.287148       1 request.go:665] Waited for 1.18907344s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2022-05-24T19:16:39.486595330Z I0524 19:16:39.486558       1 request.go:665] Waited for 1.39595086s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/secrets/localhost-recovery-client-token
2022-05-24T19:16:40.486818526Z I0524 19:16:40.486772       1 request.go:665] Waited for 1.19760915s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-138-197.ec2.internal
2022-05-24T19:26:38.287245815Z I0524 19:26:38.287204       1 request.go:665] Waited for 1.187547764s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler
2022-05-24T19:26:39.288365259Z I0524 19:26:39.288326       1 request.go:665] Waited for 1.398693768s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-149-121.ec2.internal
2022-05-24T19:26:40.487033354Z I0524 19:26:40.486998       1 request.go:665] Waited for 1.195972148s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-149-121.ec2.internal
2022-05-24T19:36:38.281714311Z I0524 19:36:38.281671       1 request.go:665] Waited for 1.186939538s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2022-05-24T19:36:39.482422244Z I0524 19:36:39.482385       1 request.go:665] Waited for 1.392725867s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2022-05-24T19:36:40.681442920Z I0524 19:36:40.681404       1 request.go:665] Waited for 1.395818238s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-149-121.ec2.internal
2022-05-24T19:36:42.088322064Z I0524 19:36:42.085964       1 request.go:665] Waited for 1.002177288s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/scheduler-kubeconfig
2022-05-24T19:46:38.287967226Z I0524 19:46:38.287929       1 request.go:665] Waited for 1.18433087s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler
2022-05-24T19:46:39.288068287Z I0524 19:46:39.288034       1 request.go:665] Waited for 1.397007144s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/secrets/localhost-recovery-client-token
2022-05-24T19:46:40.488478953Z I0524 19:46:40.488443       1 request.go:665] Waited for 1.195633284s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/scheduler-kubeconfig
2022-05-24T19:56:38.122414641Z I0524 19:56:38.122377       1 request.go:665] Waited for 1.026334667s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2022-05-24T19:56:39.322519159Z I0524 19:56:39.322478       1 request.go:665] Waited for 1.396779356s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-149-121.ec2.internal
2022-05-24T19:56:40.322695327Z I0524 19:56:40.322653       1 request.go:665] Waited for 1.394194112s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-149-121.ec2.internal
2022-05-24T20:06:38.291360719Z I0524 20:06:38.291325       1 request.go:665] Waited for 1.191119261s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-169-205.ec2.internal
2022-05-24T20:06:39.291900626Z I0524 20:06:39.291859       1 request.go:665] Waited for 1.397155756s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-149-121.ec2.internal
2022-05-24T20:06:41.296226072Z I0524 20:06:41.296172       1 request.go:665] Waited for 1.0005787s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-169-205.ec2.internal
2022-05-24T20:16:38.292558860Z I0524 20:16:38.292520       1 request.go:665] Waited for 1.1914428s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-169-205.ec2.internal
2022-05-24T20:16:39.292772978Z I0524 20:16:39.292733       1 request.go:665] Waited for 1.39130598s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2022-05-24T20:26:38.226399139Z I0524 20:26:38.226355       1 request.go:665] Waited for 1.134207518s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/config
2022-05-24T20:26:39.423353282Z I0524 20:26:39.423317       1 request.go:665] Waited for 1.392748361s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2022-05-24T20:36:38.131504161Z I0524 20:36:38.131467       1 request.go:665] Waited for 1.031580693s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2022-05-24T20:36:39.132268210Z I0524 20:36:39.132227       1 request.go:665] Waited for 1.397839129s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-169-205.ec2.internal
2022-05-24T20:36:40.134229521Z I0524 20:36:40.132743       1 request.go:665] Waited for 1.39637943s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-149-121.ec2.internal
2022-05-24T20:46:38.292898895Z I0524 20:46:38.292854       1 request.go:665] Waited for 1.189091114s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-169-205.ec2.internal
2022-05-24T20:46:39.492710600Z I0524 20:46:39.492654       1 request.go:665] Waited for 1.394077398s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2022-05-24T20:46:40.493373427Z I0524 20:46:40.493317       1 request.go:665] Waited for 1.196089829s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-138-197.ec2.internal
2022-05-24T20:56:38.293128989Z I0524 20:56:38.293092       1 request.go:665] Waited for 1.170324686s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler
2022-05-24T20:56:39.296359402Z I0524 20:56:39.296321       1 request.go:665] Waited for 1.400530531s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-149-121.ec2.internal
2022-05-24T20:56:40.493647963Z I0524 20:56:40.493612       1 request.go:665] Waited for 1.189836859s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-138-197.ec2.internal
2022-05-24T21:06:38.293825684Z I0524 21:06:38.293780       1 request.go:665] Waited for 1.191420593s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-169-205.ec2.internal
2022-05-24T21:06:39.493493581Z I0524 21:06:39.493438       1 request.go:665] Waited for 1.397072208s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/secrets/localhost-recovery-client-token
2022-05-24T21:06:41.498167470Z I0524 21:06:41.497285       1 request.go:665] Waited for 1.000094869s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/secrets/serving-cert
2022-05-24T21:16:38.294976338Z I0524 21:16:38.294941       1 request.go:665] Waited for 1.191212292s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-169-205.ec2.internal
2022-05-24T21:16:39.494843893Z I0524 21:16:39.494795       1 request.go:665] Waited for 1.396408783s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/secrets/localhost-recovery-client-token
2022-05-24T21:26:38.295932382Z I0524 21:26:38.295892       1 request.go:665] Waited for 1.190579851s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2022-05-24T21:26:39.495256777Z I0524 21:26:39.495216       1 request.go:665] Waited for 1.395925063s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-149-121.ec2.internal
2022-05-24T21:26:40.495651353Z I0524 21:26:40.495612       1 request.go:665] Waited for 1.194905177s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-138-197.ec2.internal
2022-05-24T21:26:42.102226063Z I0524 21:26:42.102149       1 request.go:665] Waited for 1.004007155s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-149-121.ec2.internal
2022-05-24T21:36:38.198117173Z I0524 21:36:38.198080       1 request.go:665] Waited for 1.092001811s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2022-05-24T21:36:39.397757703Z I0524 21:36:39.397716       1 request.go:665] Waited for 1.394067014s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2022-05-24T21:36:40.397906689Z I0524 21:36:40.397870       1 request.go:665] Waited for 1.394919227s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-138-197.ec2.internal
2022-05-24T21:46:38.296750730Z I0524 21:46:38.296715       1 request.go:665] Waited for 1.183405204s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler
2022-05-24T21:46:39.297008872Z I0524 21:46:39.296973       1 request.go:665] Waited for 1.39709894s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-149-121.ec2.internal
2022-05-24T21:46:40.901259247Z I0524 21:46:40.900885       1 request.go:665] Waited for 1.001971869s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-138-197.ec2.internal
2022-05-24T21:56:38.297841303Z I0524 21:56:38.297804       1 request.go:665] Waited for 1.190066872s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-138-197.ec2.internal
2022-05-24T21:56:39.497715952Z I0524 21:56:39.497665       1 request.go:665] Waited for 1.397525265s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-149-121.ec2.internal
2022-05-24T22:06:38.299332174Z I0524 22:06:38.299296       1 request.go:665] Waited for 1.175120872s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2022-05-24T22:06:39.299517782Z I0524 22:06:39.299477       1 request.go:665] Waited for 1.397677099s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-138-197.ec2.internal
2022-05-24T22:06:40.499233761Z I0524 22:06:40.499168       1 request.go:665] Waited for 1.197004198s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-138-197.ec2.internal
2022-05-24T22:16:38.299039134Z I0524 22:16:38.298997       1 request.go:665] Waited for 1.183335418s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler
2022-05-24T22:16:39.498413768Z I0524 22:16:39.498377       1 request.go:665] Waited for 1.397225974s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-138-197.ec2.internal
2022-05-24T22:16:40.698877877Z I0524 22:16:40.698840       1 request.go:665] Waited for 1.197494012s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-169-205.ec2.internal
2022-05-24T22:26:38.242375686Z I0524 22:26:38.242331       1 request.go:665] Waited for 1.134377768s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2022-05-24T22:26:39.442733301Z I0524 22:26:39.442697       1 request.go:665] Waited for 1.395288368s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2022-05-24T22:26:40.642260523Z I0524 22:26:40.642223       1 request.go:665] Waited for 1.396554494s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-149-121.ec2.internal
2022-05-24T22:36:38.299853661Z I0524 22:36:38.299810       1 request.go:665] Waited for 1.189532052s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2022-05-24T22:36:39.499641899Z I0524 22:36:39.499597       1 request.go:665] Waited for 1.394663574s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2022-05-24T22:36:40.500134957Z I0524 22:36:40.500090       1 request.go:665] Waited for 1.396562996s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/secrets/localhost-recovery-client-token
2022-05-24T22:46:38.300815247Z I0524 22:46:38.300772       1 request.go:665] Waited for 1.179347051s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler
2022-05-24T22:46:39.300926251Z I0524 22:46:39.300884       1 request.go:665] Waited for 1.391798308s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/secrets/localhost-recovery-client-token
2022-05-24T22:46:40.500852980Z I0524 22:46:40.500816       1 request.go:665] Waited for 1.195013152s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/scheduler-kubeconfig
2022-05-24T22:46:41.505310168Z I0524 22:46:41.505261       1 request.go:665] Waited for 1.00052119s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/secrets/serving-cert
2022-05-24T22:56:38.300747610Z I0524 22:56:38.300706       1 request.go:665] Waited for 1.170183543s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2022-05-24T22:56:39.301547378Z I0524 22:56:39.301511       1 request.go:665] Waited for 1.397239947s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/secrets/localhost-recovery-client-token
2022-05-24T22:56:40.500897767Z I0524 22:56:40.500863       1 request.go:665] Waited for 1.195831562s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/scheduler-kubeconfig
2022-05-24T22:56:41.506371109Z I0524 22:56:41.506330       1 request.go:665] Waited for 1.000994292s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/secrets/serving-cert
2022-05-24T23:06:38.301769871Z I0524 23:06:38.301729       1 request.go:665] Waited for 1.183471751s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler
2022-05-24T23:06:39.302826406Z I0524 23:06:39.302783       1 request.go:665] Waited for 1.397498268s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-149-121.ec2.internal
2022-05-24T23:06:40.502472125Z I0524 23:06:40.502434       1 request.go:665] Waited for 1.195674248s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-138-197.ec2.internal
2022-05-24T23:16:38.303428352Z I0524 23:16:38.303387       1 request.go:665] Waited for 1.182601071s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler
2022-05-24T23:16:39.502725934Z I0524 23:16:39.502687       1 request.go:665] Waited for 1.396873342s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-149-121.ec2.internal
2022-05-24T23:16:40.502863381Z I0524 23:16:40.502821       1 request.go:665] Waited for 1.194473693s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/scheduler-kubeconfig
2022-05-24T23:16:41.509201053Z I0524 23:16:41.509145       1 request.go:665] Waited for 1.002585642s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/secrets/serving-cert
2022-05-24T23:26:38.303766422Z I0524 23:26:38.303731       1 request.go:665] Waited for 1.178889786s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler
2022-05-24T23:26:39.304087021Z I0524 23:26:39.304044       1 request.go:665] Waited for 1.396301325s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/secrets/localhost-recovery-client-token
2022-05-24T23:26:40.304704097Z I0524 23:26:40.304351       1 request.go:665] Waited for 1.197577664s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-138-197.ec2.internal
2022-05-24T23:26:41.514088019Z I0524 23:26:41.510073       1 request.go:665] Waited for 1.002466916s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/secrets/serving-cert
2022-05-24T23:36:38.304902003Z I0524 23:36:38.304865       1 request.go:665] Waited for 1.189326377s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-169-205.ec2.internal
2022-05-24T23:36:39.504005901Z I0524 23:36:39.503964       1 request.go:665] Waited for 1.394206547s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2022-05-24T23:36:40.504369225Z I0524 23:36:40.504312       1 request.go:665] Waited for 1.197569115s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-138-197.ec2.internal
