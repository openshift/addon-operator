2022-05-24T18:32:09.883137472Z I0524 18:32:09.882989       1 profiler.go:21] Starting profiling endpoint at http://127.0.0.1:6060/debug/pprof/
2022-05-24T18:32:09.883890364Z I0524 18:32:09.883853       1 observer_polling.go:52] Starting from specified content for file "/var/run/secrets/serving-cert/tls.crt"
2022-05-24T18:32:09.883919860Z I0524 18:32:09.883895       1 observer_polling.go:52] Starting from specified content for file "/var/run/secrets/serving-cert/tls.key"
2022-05-24T18:32:09.883967107Z I0524 18:32:09.883938       1 observer_polling.go:159] Starting file observer
2022-05-24T18:32:09.884113968Z I0524 18:32:09.884084       1 observer_polling.go:135] File observer successfully synced
2022-05-24T18:32:09.885038001Z I0524 18:32:09.885005       1 cmd.go:209] Using service-serving-cert provided certificates
2022-05-24T18:32:09.885089986Z I0524 18:32:09.885071       1 observer_polling.go:74] Adding reactor for file "/var/run/secrets/serving-cert/tls.crt"
2022-05-24T18:32:09.885141962Z I0524 18:32:09.885125       1 observer_polling.go:74] Adding reactor for file "/var/run/secrets/serving-cert/tls.key"
2022-05-24T18:32:09.885208186Z I0524 18:32:09.885166       1 observer_polling.go:52] Starting from specified content for file "/var/run/configmaps/config/config.yaml"
2022-05-24T18:32:09.885560640Z I0524 18:32:09.885535       1 observer_polling.go:159] Starting file observer
2022-05-24T18:32:09.885711625Z I0524 18:32:09.885698       1 observer_polling.go:135] File observer successfully synced
2022-05-24T18:32:10.949593558Z I0524 18:32:10.949558       1 builder.go:262] openshift-cluster-etcd-operator version 4.10.0-202204290637.p0.g9790f8d.assembly.stream-9790f8d-9790f8d6d7ff2a4c758183c9750ece9700ab6d93
2022-05-24T18:32:10.950224980Z I0524 18:32:10.950197       1 dynamic_serving_content.go:112] "Loaded a new cert/key pair" name="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key"
2022-05-24T18:32:12.360169491Z I0524 18:32:12.360133       1 requestheader_controller.go:244] Loaded a new request header values for RequestHeaderAuthRequestController
2022-05-24T18:32:12.372340034Z I0524 18:32:12.372309       1 config.go:700] Not requested to run hook priority-and-fairness-config-consumer
2022-05-24T18:32:12.395811803Z W0524 18:32:12.386823       1 secure_serving.go:69] Use of insecure cipher 'TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256' detected.
2022-05-24T18:32:12.395811803Z W0524 18:32:12.386839       1 secure_serving.go:69] Use of insecure cipher 'TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' detected.
2022-05-24T18:32:12.395811803Z I0524 18:32:12.387016       1 genericapiserver.go:406] MuxAndDiscoveryComplete has all endpoints registered and discovery information is complete
2022-05-24T18:32:12.403414131Z I0524 18:32:12.403377       1 leaderelection.go:248] attempting to acquire leader lease openshift-etcd-operator/openshift-cluster-etcd-operator-lock...
2022-05-24T18:32:12.433366284Z I0524 18:32:12.432930       1 tlsconfig.go:200] "Loaded serving cert" certName="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key" certDetail="\"metrics.openshift-etcd-operator.svc\" [serving] validServingFor=[metrics.openshift-etcd-operator.svc,metrics.openshift-etcd-operator.svc.cluster.local] issuer=\"openshift-service-serving-signer@1653415662\" (2022-05-24 18:07:55 +0000 UTC to 2024-05-23 18:07:56 +0000 UTC (now=2022-05-24 18:32:12.432891365 +0000 UTC))"
2022-05-24T18:32:12.433366284Z I0524 18:32:12.433112       1 named_certificates.go:53] "Loaded SNI cert" index=0 certName="self-signed loopback" certDetail="\"apiserver-loopback-client@1653417132\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1653417131\" (2022-05-24 17:32:10 +0000 UTC to 2023-05-24 17:32:10 +0000 UTC (now=2022-05-24 18:32:12.433088802 +0000 UTC))"
2022-05-24T18:32:12.433366284Z I0524 18:32:12.433132       1 secure_serving.go:266] Serving securely on [::]:8443
2022-05-24T18:32:12.433366284Z I0524 18:32:12.433223       1 genericapiserver.go:462] [graceful-termination] waiting for shutdown to be initiated
2022-05-24T18:32:12.433366284Z I0524 18:32:12.433309       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
2022-05-24T18:32:12.433366284Z I0524 18:32:12.433330       1 shared_informer.go:240] Waiting for caches to sync for RequestHeaderAuthRequestController
2022-05-24T18:32:12.433417098Z I0524 18:32:12.433365       1 dynamic_serving_content.go:131] "Starting controller" name="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key"
2022-05-24T18:32:12.433629691Z I0524 18:32:12.433592       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
2022-05-24T18:32:12.433930667Z I0524 18:32:12.433912       1 reflector.go:219] Starting reflector *v1.ConfigMap (12h0m0s) from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.433930667Z I0524 18:32:12.433926       1 reflector.go:255] Listing and watching *v1.ConfigMap from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.434287303Z I0524 18:32:12.434255       1 configmap_cafile_content.go:201] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
2022-05-24T18:32:12.434287303Z I0524 18:32:12.434276       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
2022-05-24T18:32:12.434364476Z I0524 18:32:12.434342       1 reflector.go:219] Starting reflector *v1.ConfigMap (12h0m0s) from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.434402225Z I0524 18:32:12.434392       1 reflector.go:255] Listing and watching *v1.ConfigMap from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.434608693Z I0524 18:32:12.434173       1 configmap_cafile_content.go:201] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
2022-05-24T18:32:12.434637669Z I0524 18:32:12.434628       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
2022-05-24T18:32:12.434777699Z I0524 18:32:12.434752       1 reflector.go:219] Starting reflector *v1.ConfigMap (12h0m0s) from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.434832836Z I0524 18:32:12.434816       1 reflector.go:255] Listing and watching *v1.ConfigMap from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.499845067Z I0524 18:32:12.499803       1 leaderelection.go:258] successfully acquired lease openshift-etcd-operator/openshift-cluster-etcd-operator-lock
2022-05-24T18:32:12.500126041Z I0524 18:32:12.500083       1 event.go:285] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"openshift-etcd-operator", Name:"openshift-cluster-etcd-operator-lock", UID:"ac109000-df61-486a-a730-e8dc12fc9d0c", APIVersion:"v1", ResourceVersion:"55604", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' etcd-operator-6565d86cc-jcwp7_e144f90f-9824-4eb2-8f61-3ecaa2eaab2d became leader
2022-05-24T18:32:12.500126041Z I0524 18:32:12.500111       1 event.go:285] Event(v1.ObjectReference{Kind:"Lease", Namespace:"openshift-etcd-operator", Name:"openshift-cluster-etcd-operator-lock", UID:"5d70454e-3f68-4735-b1bd-f240f0293546", APIVersion:"coordination.k8s.io/v1", ResourceVersion:"55608", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' etcd-operator-6565d86cc-jcwp7_e144f90f-9824-4eb2-8f61-3ecaa2eaab2d became leader
2022-05-24T18:32:12.535174127Z I0524 18:32:12.533998       1 shared_informer.go:270] caches populated
2022-05-24T18:32:12.535174127Z I0524 18:32:12.534014       1 shared_informer.go:247] Caches are synced for RequestHeaderAuthRequestController 
2022-05-24T18:32:12.535174127Z I0524 18:32:12.535058       1 shared_informer.go:270] caches populated
2022-05-24T18:32:12.535174127Z I0524 18:32:12.535069       1 shared_informer.go:270] caches populated
2022-05-24T18:32:12.535174127Z I0524 18:32:12.535081       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file 
2022-05-24T18:32:12.535174127Z I0524 18:32:12.535070       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file 
2022-05-24T18:32:12.535346484Z I0524 18:32:12.535314       1 tlsconfig.go:178] "Loaded client CA" index=0 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"aggregator-signer\" [] issuer=\"<self>\" (2022-05-24 17:58:10 +0000 UTC to 2022-05-25 17:58:10 +0000 UTC (now=2022-05-24 18:32:12.535284914 +0000 UTC))"
2022-05-24T18:32:12.535540273Z I0524 18:32:12.535522       1 tlsconfig.go:200] "Loaded serving cert" certName="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key" certDetail="\"metrics.openshift-etcd-operator.svc\" [serving] validServingFor=[metrics.openshift-etcd-operator.svc,metrics.openshift-etcd-operator.svc.cluster.local] issuer=\"openshift-service-serving-signer@1653415662\" (2022-05-24 18:07:55 +0000 UTC to 2024-05-23 18:07:56 +0000 UTC (now=2022-05-24 18:32:12.53549585 +0000 UTC))"
2022-05-24T18:32:12.535720871Z I0524 18:32:12.535702       1 named_certificates.go:53] "Loaded SNI cert" index=0 certName="self-signed loopback" certDetail="\"apiserver-loopback-client@1653417132\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1653417131\" (2022-05-24 17:32:10 +0000 UTC to 2023-05-24 17:32:10 +0000 UTC (now=2022-05-24 18:32:12.535676923 +0000 UTC))"
2022-05-24T18:32:12.535941285Z I0524 18:32:12.535924       1 tlsconfig.go:178] "Loaded client CA" index=0 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"admin-kubeconfig-signer\" [] issuer=\"<self>\" (2022-05-24 17:58:08 +0000 UTC to 2032-05-21 17:58:08 +0000 UTC (now=2022-05-24 18:32:12.535903545 +0000 UTC))"
2022-05-24T18:32:12.535980499Z I0524 18:32:12.535966       1 tlsconfig.go:178] "Loaded client CA" index=1 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kubelet-signer\" [] issuer=\"<self>\" (2022-05-24 17:58:11 +0000 UTC to 2022-05-25 17:58:11 +0000 UTC (now=2022-05-24 18:32:12.535939764 +0000 UTC))"
2022-05-24T18:32:12.536013573Z I0524 18:32:12.535998       1 tlsconfig.go:178] "Loaded client CA" index=2 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-control-plane-signer\" [] issuer=\"<self>\" (2022-05-24 17:58:12 +0000 UTC to 2023-05-24 17:58:12 +0000 UTC (now=2022-05-24 18:32:12.535980822 +0000 UTC))"
2022-05-24T18:32:12.536045901Z I0524 18:32:12.536031       1 tlsconfig.go:178] "Loaded client CA" index=3 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-apiserver-to-kubelet-signer\" [] issuer=\"<self>\" (2022-05-24 17:58:12 +0000 UTC to 2023-05-24 17:58:12 +0000 UTC (now=2022-05-24 18:32:12.536012699 +0000 UTC))"
2022-05-24T18:32:12.536078910Z I0524 18:32:12.536064       1 tlsconfig.go:178] "Loaded client CA" index=4 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kubelet-bootstrap-kubeconfig-signer\" [] issuer=\"<self>\" (2022-05-24 17:58:09 +0000 UTC to 2032-05-21 17:58:09 +0000 UTC (now=2022-05-24 18:32:12.536045242 +0000 UTC))"
2022-05-24T18:32:12.536110293Z I0524 18:32:12.536095       1 tlsconfig.go:178] "Loaded client CA" index=5 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-csr-signer_@1653415662\" [] issuer=\"kubelet-signer\" (2022-05-24 18:07:41 +0000 UTC to 2022-05-25 17:58:11 +0000 UTC (now=2022-05-24 18:32:12.536077698 +0000 UTC))"
2022-05-24T18:32:12.536144687Z I0524 18:32:12.536130       1 tlsconfig.go:178] "Loaded client CA" index=6 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"openshift-kube-apiserver-operator_node-system-admin-signer@1653415661\" [] issuer=\"<self>\" (2022-05-24 18:07:40 +0000 UTC to 2023-05-24 18:07:41 +0000 UTC (now=2022-05-24 18:32:12.536109364 +0000 UTC))"
2022-05-24T18:32:12.536193619Z I0524 18:32:12.536161       1 tlsconfig.go:178] "Loaded client CA" index=7 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"aggregator-signer\" [] issuer=\"<self>\" (2022-05-24 17:58:10 +0000 UTC to 2022-05-25 17:58:10 +0000 UTC (now=2022-05-24 18:32:12.536144212 +0000 UTC))"
2022-05-24T18:32:12.536348253Z I0524 18:32:12.536332       1 tlsconfig.go:200] "Loaded serving cert" certName="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key" certDetail="\"metrics.openshift-etcd-operator.svc\" [serving] validServingFor=[metrics.openshift-etcd-operator.svc,metrics.openshift-etcd-operator.svc.cluster.local] issuer=\"openshift-service-serving-signer@1653415662\" (2022-05-24 18:07:55 +0000 UTC to 2024-05-23 18:07:56 +0000 UTC (now=2022-05-24 18:32:12.536309961 +0000 UTC))"
2022-05-24T18:32:12.536492371Z I0524 18:32:12.536477       1 named_certificates.go:53] "Loaded SNI cert" index=0 certName="self-signed loopback" certDetail="\"apiserver-loopback-client@1653417132\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1653417131\" (2022-05-24 17:32:10 +0000 UTC to 2023-05-24 17:32:10 +0000 UTC (now=2022-05-24 18:32:12.536458525 +0000 UTC))"
2022-05-24T18:32:12.561250791Z I0524 18:32:12.561200       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "RevisionController" resync interval is set to 0s which might lead to client request throttling
2022-05-24T18:32:12.561643054Z I0524 18:32:12.561605       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "PruneController" resync interval is set to 0s which might lead to client request throttling
2022-05-24T18:32:12.561662913Z I0524 18:32:12.561647       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "NodeController" resync interval is set to 0s which might lead to client request throttling
2022-05-24T18:32:12.561945418Z I0524 18:32:12.561917       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "UnsupportedConfigOverridesController" resync interval is set to 0s which might lead to client request throttling
2022-05-24T18:32:12.563144205Z I0524 18:32:12.563061       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "LoggingSyncer" resync interval is set to 0s which might lead to client request throttling
2022-05-24T18:32:12.566412571Z I0524 18:32:12.566381       1 reflector.go:219] Starting reflector *v1.Endpoints (10m0s) from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.566475509Z I0524 18:32:12.566447       1 reflector.go:255] Listing and watching *v1.Endpoints from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.566475509Z I0524 18:32:12.566456       1 reflector.go:219] Starting reflector *v1.Pod (10m0s) from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.566475509Z I0524 18:32:12.566467       1 reflector.go:255] Listing and watching *v1.Pod from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.566499839Z I0524 18:32:12.566479       1 reflector.go:219] Starting reflector *v1.Namespace (10m0s) from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.566499839Z I0524 18:32:12.566485       1 reflector.go:255] Listing and watching *v1.Namespace from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.566499839Z I0524 18:32:12.566489       1 reflector.go:219] Starting reflector *v1.Etcd (10m0s) from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.566499839Z I0524 18:32:12.566495       1 reflector.go:255] Listing and watching *v1.Etcd from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.566580716Z I0524 18:32:12.566559       1 reflector.go:219] Starting reflector *v1.Namespace (10m0s) from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.566616934Z I0524 18:32:12.566606       1 reflector.go:255] Listing and watching *v1.Namespace from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.566740193Z I0524 18:32:12.566714       1 reflector.go:219] Starting reflector *v1.ClusterRoleBinding (10m0s) from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.566740193Z I0524 18:32:12.566730       1 reflector.go:255] Listing and watching *v1.ClusterRoleBinding from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.566825085Z I0524 18:32:12.566802       1 reflector.go:219] Starting reflector *v1.ServiceAccount (10m0s) from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.566825085Z I0524 18:32:12.566820       1 reflector.go:255] Listing and watching *v1.ServiceAccount from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.566870431Z I0524 18:32:12.566431       1 reflector.go:219] Starting reflector *v1.ClusterOperator (10m0s) from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.566940571Z I0524 18:32:12.566907       1 reflector.go:255] Listing and watching *v1.ClusterOperator from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.567012658Z I0524 18:32:12.566995       1 reflector.go:219] Starting reflector *v1.ConfigMap (10m0s) from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.567012658Z I0524 18:32:12.567008       1 reflector.go:255] Listing and watching *v1.ConfigMap from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.567042971Z I0524 18:32:12.567023       1 reflector.go:219] Starting reflector *v1.Secret (10m0s) from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.567042971Z I0524 18:32:12.567031       1 reflector.go:219] Starting reflector *v1.ConfigMap (10m0s) from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.567054222Z I0524 18:32:12.567041       1 reflector.go:255] Listing and watching *v1.ConfigMap from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.567189481Z I0524 18:32:12.567156       1 reflector.go:219] Starting reflector *v1.ConfigMap (10m0s) from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.567189481Z I0524 18:32:12.567171       1 reflector.go:255] Listing and watching *v1.ConfigMap from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.567363558Z I0524 18:32:12.567341       1 reflector.go:219] Starting reflector *v1.Secret (10m0s) from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.567421370Z I0524 18:32:12.566456       1 reflector.go:219] Starting reflector *v1.Secret (10m0s) from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.567421370Z I0524 18:32:12.567415       1 reflector.go:255] Listing and watching *v1.Secret from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.567516764Z I0524 18:32:12.567499       1 reflector.go:219] Starting reflector *v1.Network (10m0s) from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.567516764Z I0524 18:32:12.567510       1 reflector.go:255] Listing and watching *v1.Network from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.567561218Z I0524 18:32:12.566466       1 reflector.go:219] Starting reflector *v1.Node (10m0s) from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.567561218Z I0524 18:32:12.567556       1 reflector.go:255] Listing and watching *v1.Node from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.567662599Z I0524 18:32:12.567645       1 reflector.go:219] Starting reflector *v1.APIServer (10m0s) from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.567662599Z I0524 18:32:12.567657       1 reflector.go:255] Listing and watching *v1.APIServer from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.567690378Z I0524 18:32:12.567001       1 reflector.go:219] Starting reflector *v1.ConfigMap (10m0s) from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.567708706Z I0524 18:32:12.567689       1 reflector.go:255] Listing and watching *v1.ConfigMap from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.567938818Z I0524 18:32:12.567913       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "UnsupportedConfigOverridesController" resync interval is set to 0s which might lead to client request throttling
2022-05-24T18:32:12.568053244Z I0524 18:32:12.567035       1 reflector.go:255] Listing and watching *v1.Secret from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.568148418Z I0524 18:32:12.567343       1 reflector.go:219] Starting reflector *v1.ConfigMap (10m0s) from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.568203931Z I0524 18:32:12.568172       1 reflector.go:255] Listing and watching *v1.ConfigMap from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.568257510Z I0524 18:32:12.567367       1 reflector.go:219] Starting reflector *v1.Secret (10m0s) from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.568302670Z I0524 18:32:12.568277       1 reflector.go:255] Listing and watching *v1.Secret from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.568344773Z I0524 18:32:12.568322       1 base_controller.go:67] Waiting for caches to sync for MissingStaticPodController
2022-05-24T18:32:12.568374020Z I0524 18:32:12.568360       1 base_controller.go:67] Waiting for caches to sync for ConfigObserver
2022-05-24T18:32:12.568387117Z I0524 18:32:12.568379       1 base_controller.go:67] Waiting for caches to sync for EtcdCertSignerController
2022-05-24T18:32:12.568420043Z I0524 18:32:12.568407       1 base_controller.go:67] Waiting for caches to sync for ClusterMemberController
2022-05-24T18:32:12.568433902Z I0524 18:32:12.568424       1 base_controller.go:67] Waiting for caches to sync for StatusSyncer_etcd
2022-05-24T18:32:12.568443383Z I0524 18:32:12.568432       1 base_controller.go:67] Waiting for caches to sync for EtcdMembersController
2022-05-24T18:32:12.568455010Z I0524 18:32:12.568446       1 base_controller.go:67] Waiting for caches to sync for EtcdEndpointsController
2022-05-24T18:32:12.568476982Z I0524 18:32:12.568463       1 shared_informer.go:270] caches populated
2022-05-24T18:32:12.568476982Z I0524 18:32:12.568468       1 base_controller.go:67] Waiting for caches to sync for ResourceSyncController
2022-05-24T18:32:12.568488362Z I0524 18:32:12.568474       1 base_controller.go:73] Caches are synced for EtcdMembersController 
2022-05-24T18:32:12.568488362Z I0524 18:32:12.568481       1 base_controller.go:110] Starting #1 worker of EtcdMembersController controller ...
2022-05-24T18:32:12.568588743Z I0524 18:32:12.568423       1 reflector.go:219] Starting reflector *v1.ClusterVersion (10m0s) from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.568599843Z I0524 18:32:12.568586       1 reflector.go:255] Listing and watching *v1.ClusterVersion from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.568677871Z I0524 18:32:12.568658       1 reflector.go:219] Starting reflector *v1.Secret (10m0s) from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.568677871Z I0524 18:32:12.568673       1 reflector.go:255] Listing and watching *v1.Secret from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.568765093Z I0524 18:32:12.568751       1 base_controller.go:67] Waiting for caches to sync for BootstrapTeardownController
2022-05-24T18:32:12.568786189Z I0524 18:32:12.568774       1 base_controller.go:67] Waiting for caches to sync for UnsupportedConfigOverridesController
2022-05-24T18:32:12.568786189Z I0524 18:32:12.568779       1 base_controller.go:67] Waiting for caches to sync for NodeController
2022-05-24T18:32:12.568799749Z I0524 18:32:12.568257       1 reflector.go:219] Starting reflector *v1.Infrastructure (10m0s) from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.568799749Z I0524 18:32:12.568796       1 base_controller.go:67] Waiting for caches to sync for ScriptController
2022-05-24T18:32:12.568815140Z I0524 18:32:12.568797       1 reflector.go:255] Listing and watching *v1.Infrastructure from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.569339656Z I0524 18:32:12.569316       1 base_controller.go:67] Waiting for caches to sync for QuorumGuardController
2022-05-24T18:32:12.569506795Z I0524 18:32:12.569492       1 base_controller.go:67] Waiting for caches to sync for StaticResourceController
2022-05-24T18:32:12.569522542Z I0524 18:32:12.569515       1 base_controller.go:67] Waiting for caches to sync for UnsupportedConfigOverridesController
2022-05-24T18:32:12.569544580Z I0524 18:32:12.569533       1 base_controller.go:67] Waiting for caches to sync for LoggingSyncer
2022-05-24T18:32:12.569578251Z I0524 18:32:12.569566       1 reflector.go:203] Reflector from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167 configured with expectedType of *unstructured.Unstructured with empty GroupVersionKind.
2022-05-24T18:32:12.569591391Z I0524 18:32:12.569583       1 reflector.go:219] Starting reflector *unstructured.Unstructured (12h0m0s) from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.569601077Z I0524 18:32:12.569590       1 reflector.go:255] Listing and watching *unstructured.Unstructured from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.569736582Z E0524 18:32:12.569716       1 base_controller.go:270] "EtcdMembersController" controller failed to sync "key", err: node lister not synced
2022-05-24T18:32:12.569792131Z I0524 18:32:12.569777       1 base_controller.go:67] Waiting for caches to sync for FSyncController
2022-05-24T18:32:12.569804133Z I0524 18:32:12.569794       1 shared_informer.go:270] caches populated
2022-05-24T18:32:12.569804133Z I0524 18:32:12.569799       1 base_controller.go:73] Caches are synced for FSyncController 
2022-05-24T18:32:12.569814700Z I0524 18:32:12.569805       1 base_controller.go:110] Starting #1 worker of FSyncController controller ...
2022-05-24T18:32:12.569962936Z I0524 18:32:12.569950       1 base_controller.go:67] Waiting for caches to sync for StaticResourceController
2022-05-24T18:32:12.569986310Z I0524 18:32:12.569974       1 base_controller.go:67] Waiting for caches to sync for TargetConfigController
2022-05-24T18:32:12.570022036Z I0524 18:32:12.569992       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ReportEtcdMembersErrorUpdatingStatus' etcds.operator.openshift.io "cluster" not found
2022-05-24T18:32:12.570080396Z I0524 18:32:12.570064       1 base_controller.go:67] Waiting for caches to sync for InstallerController
2022-05-24T18:32:12.570101867Z I0524 18:32:12.570089       1 base_controller.go:67] Waiting for caches to sync for DefragController
2022-05-24T18:32:12.570114022Z I0524 18:32:12.570108       1 base_controller.go:67] Waiting for caches to sync for ClusterBackupController
2022-05-24T18:32:12.570139509Z I0524 18:32:12.570126       1 envvarcontroller.go:170] Starting EnvVarController
2022-05-24T18:32:12.570152193Z I0524 18:32:12.570146       1 base_controller.go:67] Waiting for caches to sync for RevisionController
2022-05-24T18:32:12.570218438Z I0524 18:32:12.570195       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "InstallerController" resync interval is set to 0s which might lead to client request throttling
2022-05-24T18:32:12.571174606Z I0524 18:32:12.571140       1 base_controller.go:67] Waiting for caches to sync for PruneController
2022-05-24T18:32:12.571174606Z I0524 18:32:12.571169       1 base_controller.go:67] Waiting for caches to sync for InstallerStateController
2022-05-24T18:32:12.571226619Z I0524 18:32:12.571201       1 base_controller.go:67] Waiting for caches to sync for StaticPodStateController
2022-05-24T18:32:12.572612213Z I0524 18:32:12.567402       1 reflector.go:255] Listing and watching *v1.Secret from k8s.io/client-go@v0.23.0/tools/cache/reflector.go:167
2022-05-24T18:32:12.585368976Z E0524 18:32:12.585322       1 base_controller.go:270] "EtcdMembersController" controller failed to sync "key", err: configmaps lister not synced
2022-05-24T18:32:12.585565188Z I0524 18:32:12.585529       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ReportEtcdMembersErrorUpdatingStatus' etcds.operator.openshift.io "cluster" not found
2022-05-24T18:32:12.668541643Z I0524 18:32:12.668497       1 shared_informer.go:270] caches populated
2022-05-24T18:32:12.668541643Z I0524 18:32:12.668523       1 base_controller.go:73] Caches are synced for ClusterMemberController 
2022-05-24T18:32:12.668617334Z I0524 18:32:12.668601       1 shared_informer.go:270] caches populated
2022-05-24T18:32:12.668617334Z I0524 18:32:12.668612       1 base_controller.go:73] Caches are synced for MissingStaticPodController 
2022-05-24T18:32:12.668635286Z I0524 18:32:12.668627       1 base_controller.go:110] Starting #1 worker of MissingStaticPodController controller ...
2022-05-24T18:32:12.669349796Z I0524 18:32:12.669300       1 shared_informer.go:270] caches populated
2022-05-24T18:32:12.669349796Z I0524 18:32:12.669316       1 base_controller.go:73] Caches are synced for UnsupportedConfigOverridesController 
2022-05-24T18:32:12.669349796Z I0524 18:32:12.669323       1 base_controller.go:110] Starting #1 worker of UnsupportedConfigOverridesController controller ...
2022-05-24T18:32:12.669433727Z I0524 18:32:12.669405       1 shared_informer.go:270] caches populated
2022-05-24T18:32:12.669477268Z I0524 18:32:12.669465       1 base_controller.go:73] Caches are synced for ScriptController 
2022-05-24T18:32:12.669508437Z I0524 18:32:12.669498       1 base_controller.go:110] Starting #1 worker of ScriptController controller ...
2022-05-24T18:32:12.669539942Z I0524 18:32:12.669522       1 shared_informer.go:270] caches populated
2022-05-24T18:32:12.669550614Z I0524 18:32:12.669538       1 base_controller.go:73] Caches are synced for StaticResourceController 
2022-05-24T18:32:12.669560418Z I0524 18:32:12.669549       1 base_controller.go:110] Starting #1 worker of StaticResourceController controller ...
2022-05-24T18:32:12.669573471Z I0524 18:32:12.669567       1 shared_informer.go:270] caches populated
2022-05-24T18:32:12.669583137Z I0524 18:32:12.669573       1 base_controller.go:73] Caches are synced for UnsupportedConfigOverridesController 
2022-05-24T18:32:12.669583137Z I0524 18:32:12.669579       1 base_controller.go:110] Starting #1 worker of UnsupportedConfigOverridesController controller ...
2022-05-24T18:32:12.669650204Z I0524 18:32:12.669413       1 shared_informer.go:270] caches populated
2022-05-24T18:32:12.669675640Z I0524 18:32:12.669666       1 base_controller.go:73] Caches are synced for EtcdEndpointsController 
2022-05-24T18:32:12.669699785Z I0524 18:32:12.669691       1 base_controller.go:110] Starting #1 worker of EtcdEndpointsController controller ...
2022-05-24T18:32:12.669732732Z I0524 18:32:12.669470       1 shared_informer.go:270] caches populated
2022-05-24T18:32:12.669762965Z I0524 18:32:12.669754       1 base_controller.go:73] Caches are synced for NodeController 
2022-05-24T18:32:12.669787664Z I0524 18:32:12.669778       1 base_controller.go:110] Starting #1 worker of NodeController controller ...
2022-05-24T18:32:12.669884105Z I0524 18:32:12.669448       1 shared_informer.go:270] caches populated
2022-05-24T18:32:12.669931938Z I0524 18:32:12.669916       1 base_controller.go:73] Caches are synced for StatusSyncer_etcd 
2022-05-24T18:32:12.669961174Z I0524 18:32:12.669951       1 base_controller.go:110] Starting #1 worker of StatusSyncer_etcd controller ...
2022-05-24T18:32:12.670415894Z I0524 18:32:12.670391       1 bootstrap.go:153] bootstrap considered incomplete because the kube-system/bootstrap configmap wasn't found
2022-05-24T18:32:12.670475012Z I0524 18:32:12.670461       1 shared_informer.go:270] caches populated
2022-05-24T18:32:12.670475012Z I0524 18:32:12.670471       1 base_controller.go:73] Caches are synced for InstallerController 
2022-05-24T18:32:12.670488441Z I0524 18:32:12.670477       1 base_controller.go:110] Starting #1 worker of InstallerController controller ...
2022-05-24T18:32:12.671778753Z I0524 18:32:12.670879       1 shared_informer.go:270] caches populated
2022-05-24T18:32:12.671778753Z I0524 18:32:12.670892       1 envvarcontroller.go:176] caches synced
2022-05-24T18:32:12.671778753Z I0524 18:32:12.670912       1 shared_informer.go:270] caches populated
2022-05-24T18:32:12.671778753Z I0524 18:32:12.670917       1 base_controller.go:73] Caches are synced for DefragController 
2022-05-24T18:32:12.671778753Z I0524 18:32:12.670924       1 base_controller.go:110] Starting #1 worker of DefragController controller ...
2022-05-24T18:32:12.671778753Z I0524 18:32:12.670944       1 shared_informer.go:270] caches populated
2022-05-24T18:32:12.671778753Z I0524 18:32:12.670951       1 base_controller.go:73] Caches are synced for ClusterBackupController 
2022-05-24T18:32:12.671778753Z I0524 18:32:12.670956       1 base_controller.go:110] Starting #1 worker of ClusterBackupController controller ...
2022-05-24T18:32:12.671778753Z I0524 18:32:12.671007       1 bootstrap.go:153] bootstrap considered incomplete because the kube-system/bootstrap configmap wasn't found
2022-05-24T18:32:12.671778753Z I0524 18:32:12.671575       1 bootstrap.go:140] node count 3 satisfies minimum of 3 required by the HAScalingStrategy bootstrap scaling strategy
2022-05-24T18:32:12.672032558Z I0524 18:32:12.672003       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: etcd-all-certs, secrets: etcd-all-certs-6
2022-05-24T18:32:12.672584123Z I0524 18:32:12.672057       1 base_controller.go:110] Starting #1 worker of ClusterMemberController controller ...
2022-05-24T18:32:12.672664261Z I0524 18:32:12.672164       1 shared_informer.go:270] caches populated
2022-05-24T18:32:12.672675160Z I0524 18:32:12.672663       1 base_controller.go:73] Caches are synced for StaticPodStateController 
2022-05-24T18:32:12.672684561Z I0524 18:32:12.672674       1 base_controller.go:110] Starting #1 worker of StaticPodStateController controller ...
2022-05-24T18:32:12.672717274Z I0524 18:32:12.672231       1 shared_informer.go:270] caches populated
2022-05-24T18:32:12.672717274Z I0524 18:32:12.672705       1 base_controller.go:73] Caches are synced for InstallerStateController 
2022-05-24T18:32:12.672717274Z I0524 18:32:12.672711       1 base_controller.go:110] Starting #1 worker of InstallerStateController controller ...
2022-05-24T18:32:12.672746884Z I0524 18:32:12.672297       1 shared_informer.go:270] caches populated
2022-05-24T18:32:12.672746884Z I0524 18:32:12.672740       1 base_controller.go:73] Caches are synced for StaticResourceController 
2022-05-24T18:32:12.672758581Z I0524 18:32:12.672745       1 base_controller.go:110] Starting #1 worker of StaticResourceController controller ...
2022-05-24T18:32:12.672758581Z I0524 18:32:12.672504       1 shared_informer.go:270] caches populated
2022-05-24T18:32:12.672768955Z I0524 18:32:12.672759       1 base_controller.go:73] Caches are synced for PruneController 
2022-05-24T18:32:12.672768955Z I0524 18:32:12.672762       1 base_controller.go:110] Starting #1 worker of PruneController controller ...
2022-05-24T18:32:12.673274837Z I0524 18:32:12.672529       1 shared_informer.go:270] caches populated
2022-05-24T18:32:12.673350493Z I0524 18:32:12.673337       1 base_controller.go:73] Caches are synced for LoggingSyncer 
2022-05-24T18:32:12.673395116Z I0524 18:32:12.673371       1 base_controller.go:110] Starting #1 worker of LoggingSyncer controller ...
2022-05-24T18:32:12.673755490Z I0524 18:32:12.673730       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorLogLevelChange' Operator log level changed from "Debug" to "Normal"
2022-05-24T18:32:12.683619104Z E0524 18:32:12.683592       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-6]
2022-05-24T18:32:12.687219735Z I0524 18:32:12.684058       1 base_controller.go:73] Caches are synced for QuorumGuardController 
2022-05-24T18:32:12.687219735Z I0524 18:32:12.684073       1 base_controller.go:110] Starting #1 worker of QuorumGuardController controller ...
2022-05-24T18:32:12.687219735Z E0524 18:32:12.684790       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-6]
2022-05-24T18:32:12.687219735Z I0524 18:32:12.686572       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:31:32Z","message":"NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-6]\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-169-205.ec2.internal is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-169-205.ec2.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:32:12.687219735Z I0524 18:32:12.686912       1 quorumguardcontroller.go:113] HA mode is: HighlyAvailable
2022-05-24T18:32:12.687219735Z I0524 18:32:12.686934       1 quorumguardcontroller.go:245] Getting number of expected masters from cluster-config-v1
2022-05-24T18:32:12.687219735Z I0524 18:32:12.686969       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: etcd-all-certs, secrets: etcd-all-certs-6
2022-05-24T18:32:12.690366288Z I0524 18:32:12.690336       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: etcd-all-certs, secrets: etcd-all-certs-6
2022-05-24T18:32:12.690510521Z E0524 18:32:12.690450       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-6]
2022-05-24T18:32:12.696389023Z I0524 18:32:12.696360       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-169-205.ec2.internal is unhealthy" to "NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-6]\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-169-205.ec2.internal is unhealthy"
2022-05-24T18:32:12.712516215Z E0524 18:32:12.711943       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-6]
2022-05-24T18:32:12.714310648Z I0524 18:32:12.714258       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: etcd-all-certs, secrets: etcd-all-certs-6
2022-05-24T18:32:12.723008940Z I0524 18:32:12.722982       1 quorumguardcontroller.go:214] etcd-quorum-guard was modified
2022-05-24T18:32:12.723227125Z I0524 18:32:12.723171       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ModifiedQuorumGuardDeployment' etcd-quorum-guard was modified
2022-05-24T18:32:12.735544150Z E0524 18:32:12.735518       1 base_controller.go:272] ScriptController reconciliation failed: "configmap/etcd-pod": missing env var values
2022-05-24T18:32:12.736268893Z E0524 18:32:12.736241       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-6]
2022-05-24T18:32:12.736291795Z I0524 18:32:12.736271       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: etcd-all-certs, secrets: etcd-all-certs-6
2022-05-24T18:32:12.736509286Z I0524 18:32:12.736491       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:31:32Z","message":"ScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nNodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-6]\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-169-205.ec2.internal is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-169-205.ec2.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:32:12.745231147Z I0524 18:32:12.745198       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-6]\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-169-205.ec2.internal is unhealthy" to "ScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nNodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-6]\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-169-205.ec2.internal is unhealthy"
2022-05-24T18:32:12.753640918Z I0524 18:32:12.753608       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: etcd-all-certs, secrets: etcd-all-certs-6
2022-05-24T18:32:12.753855749Z E0524 18:32:12.753818       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-6]
2022-05-24T18:32:12.754746228Z E0524 18:32:12.754719       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-6]
2022-05-24T18:32:12.755237327Z I0524 18:32:12.755216       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:31:32Z","message":"ScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nNodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-6]\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-169-205.ec2.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:32:12.755405621Z I0524 18:32:12.755368       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: etcd-all-certs, secrets: etcd-all-certs-6
2022-05-24T18:32:12.762282508Z I0524 18:32:12.762258       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "ScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nNodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-6]\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-169-205.ec2.internal is unhealthy" to "ScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nNodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-6]\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T18:32:12.802462508Z E0524 18:32:12.802426       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-6]
2022-05-24T18:32:12.802537393Z I0524 18:32:12.802464       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: etcd-all-certs, secrets: etcd-all-certs-6
2022-05-24T18:32:12.802756522Z I0524 18:32:12.802735       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:31:32Z","message":"ScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nNodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-6]\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:32:12.840380990Z I0524 18:32:12.840336       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Available message changed from "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-169-205.ec2.internal is unhealthy" to "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available"
2022-05-24T18:32:12.915467804Z I0524 18:32:12.915426       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: etcd-all-certs, secrets: etcd-all-certs-6
2022-05-24T18:32:12.915854958Z E0524 18:32:12.915807       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-6]
2022-05-24T18:32:13.068681266Z I0524 18:32:13.068646       1 base_controller.go:73] Caches are synced for ConfigObserver 
2022-05-24T18:32:13.068746666Z I0524 18:32:13.068721       1 base_controller.go:110] Starting #1 worker of ConfigObserver controller ...
2022-05-24T18:32:13.269364306Z I0524 18:32:13.269318       1 base_controller.go:73] Caches are synced for BootstrapTeardownController 
2022-05-24T18:32:13.269398670Z I0524 18:32:13.269358       1 base_controller.go:110] Starting #1 worker of BootstrapTeardownController controller ...
2022-05-24T18:32:13.669300434Z I0524 18:32:13.669264       1 base_controller.go:73] Caches are synced for EtcdCertSignerController 
2022-05-24T18:32:13.669300434Z I0524 18:32:13.669294       1 base_controller.go:110] Starting #1 worker of EtcdCertSignerController controller ...
2022-05-24T18:32:13.670432111Z I0524 18:32:13.670367       1 base_controller.go:73] Caches are synced for TargetConfigController 
2022-05-24T18:32:13.670432111Z I0524 18:32:13.670383       1 base_controller.go:73] Caches are synced for RevisionController 
2022-05-24T18:32:13.670432111Z I0524 18:32:13.670395       1 base_controller.go:110] Starting #1 worker of RevisionController controller ...
2022-05-24T18:32:13.670498005Z I0524 18:32:13.670386       1 base_controller.go:110] Starting #1 worker of TargetConfigController controller ...
2022-05-24T18:32:13.767010993Z I0524 18:32:13.766977       1 request.go:665] Waited for 1.194255672s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd-operator/secrets?limit=500&resourceVersion=0
2022-05-24T18:32:13.868628197Z I0524 18:32:13.868585       1 base_controller.go:73] Caches are synced for ResourceSyncController 
2022-05-24T18:32:13.868628197Z I0524 18:32:13.868614       1 base_controller.go:110] Starting #1 worker of ResourceSyncController controller ...
2022-05-24T18:32:14.767412020Z I0524 18:32:14.767373       1 request.go:665] Waited for 2.093821875s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd
2022-05-24T18:32:14.992231541Z I0524 18:32:14.992190       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:31:32Z","message":"NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-6]\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:32:15.009227934Z I0524 18:32:15.009161       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "ScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nNodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-6]\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-6]\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T18:32:15.967526271Z I0524 18:32:15.967486       1 request.go:665] Waited for 1.396014166s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/revision-pruner-6-ip-10-0-138-197.ec2.internal
2022-05-24T18:32:17.167623740Z I0524 18:32:17.167591       1 request.go:665] Waited for 1.396354636s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-138-197.ec2.internal
2022-05-24T18:32:17.374656689Z I0524 18:32:17.374605       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-6-ip-10-0-138-197.ec2.internal -n openshift-etcd because it was missing
2022-05-24T18:32:18.366629961Z I0524 18:32:18.366592       1 request.go:665] Waited for 1.194668754s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-169-205.ec2.internal
2022-05-24T18:32:19.367516915Z I0524 18:32:19.367476       1 request.go:665] Waited for 1.196588739s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-138-197.ec2.internal
2022-05-24T18:32:20.367643860Z I0524 18:32:20.367603       1 request.go:665] Waited for 1.196653212s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2022-05-24T18:32:21.369148598Z I0524 18:32:21.369110       1 request.go:665] Waited for 1.19835074s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2022-05-24T18:32:21.983585669Z I0524 18:32:21.983549       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:31:32Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:32:21.999664532Z I0524 18:32:21.999604       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-6]\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T18:32:22.386900655Z I0524 18:32:22.375832       1 request.go:665] Waited for 1.204521645s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/revision-pruner-6-ip-10-0-169-205.ec2.internal
2022-05-24T18:32:23.566722800Z I0524 18:32:23.566681       1 request.go:665] Waited for 1.58357296s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd
2022-05-24T18:32:24.566805034Z I0524 18:32:24.566748       1 request.go:665] Waited for 1.595473541s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-138-197.ec2.internal
2022-05-24T18:33:26.753154247Z I0524 18:33:26.753101       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-138-197.ec2.internal
2022-05-24T18:33:26.764454742Z E0524 18:33:26.764422       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2022-05-24T18:33:26.766565223Z I0524 18:33:26.766520       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:31:32Z","message":"ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:33:26.787808411Z I0524 18:33:26.787762       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T18:33:27.587169685Z I0524 18:33:27.587124       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:31:32Z","message":"ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:33:27.604239806Z I0524 18:33:27.604196       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy"
2022-05-24T18:33:27.606134651Z I0524 18:33:27.606101       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:31:32Z","message":"ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:33:27.614699570Z I0524 18:33:27.614662       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Available message changed from "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available" to "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy"
2022-05-24T18:33:28.766154028Z I0524 18:33:28.766118       1 request.go:665] Waited for 1.178417534s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2022-05-24T18:33:29.965707868Z I0524 18:33:29.965676       1 request.go:665] Waited for 1.394406549s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2022-05-24T18:33:30.966074522Z I0524 18:33:30.966034       1 request.go:665] Waited for 1.393480213s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-169-205.ec2.internal
2022-05-24T18:33:34.618907293Z I0524 18:33:34.618861       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:31:32Z","message":"ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nNodeControllerDegraded: The master nodes not ready: node \"ip-10-0-138-197.ec2.internal\" not ready since 2022-05-24 18:33:34 +0000 UTC because NodeStatusUnknown (Kubelet stopped posting node status.)\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:33:34.633957387Z I0524 18:33:34.633899       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy" to "ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nNodeControllerDegraded: The master nodes not ready: node \"ip-10-0-138-197.ec2.internal\" not ready since 2022-05-24 18:33:34 +0000 UTC because NodeStatusUnknown (Kubelet stopped posting node status.)\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy"
2022-05-24T18:33:35.765918379Z I0524 18:33:35.765879       1 request.go:665] Waited for 1.131188365s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2022-05-24T18:33:36.766298627Z I0524 18:33:36.766264       1 request.go:665] Waited for 1.596394145s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2022-05-24T18:33:41.786408020Z I0524 18:33:41.786366       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-138-197.ec2.internal
2022-05-24T18:33:41.786594458Z E0524 18:33:41.786576       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2022-05-24T18:33:41.818420142Z E0524 18:33:41.818389       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy
2022-05-24T18:33:41.825748158Z I0524 18:33:41.825714       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:31:32Z","message":"ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nNodeControllerDegraded: The master nodes not ready: node \"ip-10-0-138-197.ec2.internal\" not ready since 2022-05-24 18:33:34 +0000 UTC because NodeStatusUnknown (Kubelet stopped posting node status.)\nDefragControllerDegraded: cluster is unhealthy: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:33:41.837665346Z I0524 18:33:41.837625       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nNodeControllerDegraded: The master nodes not ready: node \"ip-10-0-138-197.ec2.internal\" not ready since 2022-05-24 18:33:34 +0000 UTC because NodeStatusUnknown (Kubelet stopped posting node status.)\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy" to "ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nNodeControllerDegraded: The master nodes not ready: node \"ip-10-0-138-197.ec2.internal\" not ready since 2022-05-24 18:33:34 +0000 UTC because NodeStatusUnknown (Kubelet stopped posting node status.)\nDefragControllerDegraded: cluster is unhealthy: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy"
2022-05-24T18:33:56.799319788Z I0524 18:33:56.799276       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-138-197.ec2.internal
2022-05-24T18:33:56.799404295Z E0524 18:33:56.799386       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2022-05-24T18:33:56.845832334Z E0524 18:33:56.845790       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy
2022-05-24T18:34:11.845021930Z I0524 18:34:11.844974       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-138-197.ec2.internal
2022-05-24T18:34:11.845313826Z E0524 18:34:11.845297       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2022-05-24T18:34:11.865886843Z E0524 18:34:11.865857       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy
2022-05-24T18:34:26.849484647Z I0524 18:34:26.849438       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-138-197.ec2.internal
2022-05-24T18:34:26.849601525Z E0524 18:34:26.849583       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2022-05-24T18:34:26.877874061Z E0524 18:34:26.877844       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy
2022-05-24T18:34:41.860701409Z I0524 18:34:41.860654       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-138-197.ec2.internal
2022-05-24T18:34:41.860853807Z E0524 18:34:41.860837       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2022-05-24T18:34:41.889745021Z E0524 18:34:41.889713       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy
2022-05-24T18:34:56.865063324Z I0524 18:34:56.865018       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-138-197.ec2.internal
2022-05-24T18:34:56.865225156Z E0524 18:34:56.865208       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2022-05-24T18:34:56.893166170Z E0524 18:34:56.893136       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy
2022-05-24T18:35:11.877715675Z I0524 18:35:11.877671       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-138-197.ec2.internal
2022-05-24T18:35:11.877831687Z E0524 18:35:11.877814       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2022-05-24T18:35:11.896592201Z E0524 18:35:11.896561       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy
2022-05-24T18:35:26.521070039Z I0524 18:35:26.521030       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:35:26Z","message":"ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nDefragControllerDegraded: cluster is unhealthy: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy\nNodeControllerDegraded: The master nodes not ready: node \"ip-10-0-138-197.ec2.internal\" not ready since 2022-05-24 18:33:34 +0000 UTC because NodeStatusUnknown (Kubelet stopped posting node status.)","reason":"ClusterMemberController_Error::DefragController_Error::EtcdMembers_UnhealthyMembers::NodeController_MasterNodesReady","status":"True","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:35:26.534955494Z I0524 18:35:26.534908       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded changed from False to True ("ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nDefragControllerDegraded: cluster is unhealthy: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy\nNodeControllerDegraded: The master nodes not ready: node \"ip-10-0-138-197.ec2.internal\" not ready since 2022-05-24 18:33:34 +0000 UTC because NodeStatusUnknown (Kubelet stopped posting node status.)")
2022-05-24T18:35:26.888653878Z I0524 18:35:26.888617       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-138-197.ec2.internal
2022-05-24T18:35:26.888840020Z E0524 18:35:26.888818       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2022-05-24T18:35:26.909443290Z E0524 18:35:26.909405       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy
2022-05-24T18:35:41.900878706Z I0524 18:35:41.900833       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-138-197.ec2.internal
2022-05-24T18:35:41.901036426Z E0524 18:35:41.901012       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2022-05-24T18:35:41.919413721Z E0524 18:35:41.919380       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy
2022-05-24T18:35:50.601864877Z I0524 18:35:50.601828       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:35:26Z","message":"ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nDefragControllerDegraded: cluster is unhealthy: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy","reason":"ClusterMemberController_Error::DefragController_Error::EtcdMembers_UnhealthyMembers","status":"True","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:35:50.620543142Z I0524 18:35:50.620484       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nDefragControllerDegraded: cluster is unhealthy: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy\nNodeControllerDegraded: The master nodes not ready: node \"ip-10-0-138-197.ec2.internal\" not ready since 2022-05-24 18:33:34 +0000 UTC because NodeStatusUnknown (Kubelet stopped posting node status.)" to "ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nDefragControllerDegraded: cluster is unhealthy: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy"
2022-05-24T18:35:50.620716228Z I0524 18:35:50.620698       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:35:26Z","message":"ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nDefragControllerDegraded: cluster is unhealthy: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy","reason":"ClusterMemberController_Error::DefragController_Error::EtcdMembers_UnhealthyMembers","status":"True","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:35:50.631877617Z I0524 18:35:50.631832       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nDefragControllerDegraded: cluster is unhealthy: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy\nNodeControllerDegraded: The master nodes not ready: node \"ip-10-0-138-197.ec2.internal\" not ready since 2022-05-24 18:33:34 +0000 UTC because NodeStatusUnknown (Kubelet stopped posting node status.)" to "ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nDefragControllerDegraded: cluster is unhealthy: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy"
2022-05-24T18:35:56.912304812Z I0524 18:35:56.912231       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-138-197.ec2.internal
2022-05-24T18:35:56.912502049Z E0524 18:35:56.912478       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2022-05-24T18:35:56.930536224Z E0524 18:35:56.930507       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy
2022-05-24T18:36:11.923349234Z I0524 18:36:11.923305       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-138-197.ec2.internal
2022-05-24T18:36:11.923525994Z E0524 18:36:11.923509       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2022-05-24T18:36:11.944106239Z E0524 18:36:11.944072       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy
2022-05-24T18:36:13.104303125Z I0524 18:36:13.104253       1 clustermembercontroller.go:155] skipping unready pod "etcd-ip-10-0-138-197.ec2.internal" because it is already an etcd member: &etcdserverpb.Member{ID:0xa414a9d88f840335, Name:"ip-10-0-138-197.ec2.internal", PeerURLs:[]string{"https://10.0.138.197:2380"}, ClientURLs:[]string{"https://10.0.138.197:2379"}, IsLearner:false, XXX_NoUnkeyedLiteral:struct {}{}, XXX_unrecognized:[]uint8(nil), XXX_sizecache:0}
2022-05-24T18:36:13.116770194Z I0524 18:36:13.116723       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:36:13Z","message":"NodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: cluster is unhealthy: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:36:13.123877652Z I0524 18:36:13.123834       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded changed from True to False ("NodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: cluster is unhealthy: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy")
2022-05-24T18:36:13.195704840Z I0524 18:36:13.195665       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 12.19 %, dbSize: 266686464
2022-05-24T18:36:13.195704840Z I0524 18:36:13.195685       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 11.44 %, dbSize: 285437952
2022-05-24T18:36:13.195704840Z I0524 18:36:13.195692       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 12.12 %, dbSize: 266883072
2022-05-24T18:36:13.208664789Z I0524 18:36:13.208630       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:36:13Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:36:13.223124228Z I0524 18:36:13.223060       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: cluster is unhealthy: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy"
2022-05-24T18:36:13.395366702Z I0524 18:36:13.395321       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 12.16 %, dbSize: 266686464
2022-05-24T18:36:13.395366702Z I0524 18:36:13.395348       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 13.98 %, dbSize: 285437952
2022-05-24T18:36:13.395366702Z I0524 18:36:13.395355       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 12.09 %, dbSize: 266883072
2022-05-24T18:36:13.535223630Z I0524 18:36:13.535189       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 12.13 %, dbSize: 266686464
2022-05-24T18:36:13.535278672Z I0524 18:36:13.535264       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 15.53 %, dbSize: 285437952
2022-05-24T18:36:13.535348365Z I0524 18:36:13.535296       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 12.05 %, dbSize: 266883072
2022-05-24T18:36:13.641669304Z I0524 18:36:13.641615       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:36:13Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:36:13.693715302Z I0524 18:36:13.693668       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T18:36:13.702450993Z I0524 18:36:13.702412       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:36:13Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:36:13.725847572Z I0524 18:36:13.725809       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Available message changed from "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-138-197.ec2.internal is unhealthy" to "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available"
2022-05-24T18:36:13.886885813Z I0524 18:36:13.886849       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 12.11 %, dbSize: 266686464
2022-05-24T18:36:13.886885813Z I0524 18:36:13.886866       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 17.95 %, dbSize: 285437952
2022-05-24T18:36:13.886885813Z I0524 18:36:13.886872       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 12.04 %, dbSize: 266883072
2022-05-24T18:36:13.971993711Z I0524 18:36:13.971960       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 12.10 %, dbSize: 266686464
2022-05-24T18:36:13.971993711Z I0524 18:36:13.971976       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 17.95 %, dbSize: 285437952
2022-05-24T18:36:13.971993711Z I0524 18:36:13.971980       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 12.04 %, dbSize: 266883072
2022-05-24T18:36:14.272932134Z I0524 18:36:14.272895       1 request.go:665] Waited for 1.152919342s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2022-05-24T18:36:14.490226566Z E0524 18:36:14.490174       1 base_controller.go:272] ClusterMemberController reconciliation failed: could not get etcd pod: rpc error: code = Canceled desc = grpc: the client connection is closing
2022-05-24T18:36:14.491584191Z I0524 18:36:14.491547       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:36:13Z","message":"ClusterMemberControllerDegraded: could not get etcd pod: rpc error: code = Canceled desc = grpc: the client connection is closing\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:36:14.502019575Z I0524 18:36:14.501982       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "ClusterMemberControllerDegraded: could not get etcd pod: rpc error: code = Canceled desc = grpc: the client connection is closing\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T18:36:14.543764598Z I0524 18:36:14.543728       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:36:13Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:36:14.554575333Z I0524 18:36:14.554535       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "ClusterMemberControllerDegraded: could not get etcd pod: rpc error: code = Canceled desc = grpc: the client connection is closing\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T18:36:14.622209659Z I0524 18:36:14.622162       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 12.06 %, dbSize: 266686464
2022-05-24T18:36:14.622240845Z I0524 18:36:14.622216       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 17.91 %, dbSize: 285437952
2022-05-24T18:36:14.622240845Z I0524 18:36:14.622222       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 12.00 %, dbSize: 266883072
2022-05-24T18:36:14.707200174Z I0524 18:36:14.707152       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 12.06 %, dbSize: 266686464
2022-05-24T18:36:14.707200174Z I0524 18:36:14.707168       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 17.91 %, dbSize: 285437952
2022-05-24T18:36:14.707223582Z I0524 18:36:14.707201       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 12.00 %, dbSize: 266883072
2022-05-24T18:36:15.273174790Z I0524 18:36:15.273134       1 request.go:665] Waited for 1.571298908s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2022-05-24T18:36:16.472709437Z I0524 18:36:16.472673       1 request.go:665] Waited for 1.594831625s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2022-05-24T18:36:17.137132559Z I0524 18:36:17.137081       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 11.93 %, dbSize: 266686464
2022-05-24T18:36:17.137132559Z I0524 18:36:17.137100       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 17.79 %, dbSize: 285437952
2022-05-24T18:36:17.137132559Z I0524 18:36:17.137107       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 11.87 %, dbSize: 266883072
2022-05-24T18:36:17.473351837Z I0524 18:36:17.473309       1 request.go:665] Waited for 1.597292699s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2022-05-24T18:36:18.673365917Z I0524 18:36:18.673323       1 request.go:665] Waited for 1.197023552s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2022-05-24T18:36:34.668812124Z E0524 18:36:34.668092       1 base_controller.go:272] ClusterMemberController reconciliation failed: could not get list of unhealthy members: could not get member list rpc error: code = Canceled desc = grpc: the client connection is closing
2022-05-24T18:36:34.670869507Z I0524 18:36:34.670834       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:36:13Z","message":"ClusterMemberControllerDegraded: could not get list of unhealthy members: could not get member list rpc error: code = Canceled desc = grpc: the client connection is closing\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:36:34.745562375Z I0524 18:36:34.745524       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:36:13Z","message":"ClusterMemberControllerDegraded: could not get list of unhealthy members: could not get member list rpc error: code = Canceled desc = grpc: the client connection is closing\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:36:34.747773016Z I0524 18:36:34.747733       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "ClusterMemberControllerDegraded: could not get list of unhealthy members: could not get member list rpc error: code = Canceled desc = grpc: the client connection is closing\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T18:36:34.777367572Z I0524 18:36:34.767847       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "ClusterMemberControllerDegraded: could not get list of unhealthy members: could not get member list rpc error: code = Canceled desc = grpc: the client connection is closing\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T18:36:34.950601766Z I0524 18:36:34.950562       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:36:13Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:36:34.985655561Z I0524 18:36:34.985604       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "ClusterMemberControllerDegraded: could not get list of unhealthy members: could not get member list rpc error: code = Canceled desc = grpc: the client connection is closing\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T18:36:35.512143668Z E0524 18:36:35.501946       1 base_controller.go:272] DefragController reconciliation failed: failed to dial endpoint https://10.0.138.197:2379 with maintenance client: context canceled
2022-05-24T18:36:35.526662094Z I0524 18:36:35.526606       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:36:13Z","message":"NodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: failed to dial endpoint https://10.0.138.197:2379 with maintenance client: context canceled\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:36:35.602622172Z I0524 18:36:35.601911       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: failed to dial endpoint https://10.0.138.197:2379 with maintenance client: context canceled\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T18:36:35.753839888Z I0524 18:36:35.753807       1 request.go:665] Waited for 1.084114265s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd
2022-05-24T18:36:36.007815438Z I0524 18:36:36.007777       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 10.35 %, dbSize: 266686464
2022-05-24T18:36:36.007815438Z I0524 18:36:36.007800       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 16.30 %, dbSize: 285437952
2022-05-24T18:36:36.007815438Z I0524 18:36:36.007806       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 10.28 %, dbSize: 266883072
2022-05-24T18:36:36.049275489Z I0524 18:36:36.049235       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:36:13Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:36:36.115760891Z I0524 18:36:36.115680       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: failed to dial endpoint https://10.0.138.197:2379 with maintenance client: context canceled\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T18:36:36.945954315Z I0524 18:36:36.945899       1 request.go:665] Waited for 1.524719963s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2022-05-24T18:36:37.353259609Z I0524 18:36:37.351575       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 10.28 %, dbSize: 266686464
2022-05-24T18:36:37.353259609Z I0524 18:36:37.351598       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 16.22 %, dbSize: 285437952
2022-05-24T18:36:37.353259609Z I0524 18:36:37.351604       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 10.20 %, dbSize: 266883072
2022-05-24T18:36:37.793519782Z I0524 18:36:37.793453       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-6-ip-10-0-149-121.ec2.internal -n openshift-etcd because it was missing
2022-05-24T18:36:38.145798939Z I0524 18:36:38.145757       1 request.go:665] Waited for 1.474019578s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-138-197.ec2.internal
2022-05-24T18:36:38.172718552Z I0524 18:36:38.172679       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 10.14 %, dbSize: 266686464
2022-05-24T18:36:38.172718552Z I0524 18:36:38.172702       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 16.12 %, dbSize: 285437952
2022-05-24T18:36:38.172718552Z I0524 18:36:38.172708       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 10.08 %, dbSize: 266883072
2022-05-24T18:36:38.312168873Z E0524 18:36:38.312134       1 base_controller.go:272] BootstrapTeardownController reconciliation failed: rpc error: code = Canceled desc = grpc: the client connection is closing
2022-05-24T18:36:38.316799436Z I0524 18:36:38.316767       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:36:13Z","message":"NodeControllerDegraded: All master nodes are ready\nBootstrapTeardownDegraded: rpc error: code = Canceled desc = grpc: the client connection is closing\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:36:38.390742944Z I0524 18:36:38.390698       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nBootstrapTeardownDegraded: rpc error: code = Canceled desc = grpc: the client connection is closing\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T18:36:38.701568590Z I0524 18:36:38.701517       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:36:13Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:36:38.751866722Z I0524 18:36:38.751820       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nBootstrapTeardownDegraded: rpc error: code = Canceled desc = grpc: the client connection is closing\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T18:36:39.088796865Z I0524 18:36:39.088755       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 10.03 %, dbSize: 266686464
2022-05-24T18:36:39.088889751Z I0524 18:36:39.088872       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 15.99 %, dbSize: 285437952
2022-05-24T18:36:39.088921389Z I0524 18:36:39.088909       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 9.97 %, dbSize: 266883072
2022-05-24T18:36:39.146790500Z I0524 18:36:39.146554       1 request.go:665] Waited for 1.458068119s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2022-05-24T18:36:39.716871492Z I0524 18:36:39.716831       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 9.98 %, dbSize: 266686464
2022-05-24T18:36:39.716871492Z I0524 18:36:39.716849       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 15.94 %, dbSize: 285437952
2022-05-24T18:36:39.716871492Z I0524 18:36:39.716854       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 9.90 %, dbSize: 266883072
2022-05-24T18:36:40.345926915Z I0524 18:36:40.345889       1 request.go:665] Waited for 1.563790268s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2022-05-24T18:36:41.545659038Z I0524 18:36:41.545619       1 request.go:665] Waited for 1.592744733s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-169-205.ec2.internal
2022-05-24T18:36:42.546278614Z I0524 18:36:42.546240       1 request.go:665] Waited for 1.093273784s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-138-197.ec2.internal
2022-05-24T18:36:53.227283033Z I0524 18:36:53.227248       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:36:13Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdEndpointsDegraded: could not get member health: rpc error: code = Canceled desc = grpc: the client connection is closing\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:36:53.241039071Z E0524 18:36:53.241004       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: could not get member health: rpc error: code = Canceled desc = grpc: the client connection is closing
2022-05-24T18:36:53.241813921Z I0524 18:36:53.241780       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nEtcdEndpointsDegraded: could not get member health: rpc error: code = Canceled desc = grpc: the client connection is closing\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T18:36:53.309832593Z I0524 18:36:53.309784       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:36:13Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:36:53.317293833Z I0524 18:36:53.317260       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 9.19 %, dbSize: 266686464
2022-05-24T18:36:53.317293833Z I0524 18:36:53.317280       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 15.22 %, dbSize: 285437952
2022-05-24T18:36:53.317293833Z I0524 18:36:53.317287       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 9.12 %, dbSize: 266883072
2022-05-24T18:36:53.318969334Z I0524 18:36:53.318929       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdEndpointsDegraded: could not get member health: rpc error: code = Canceled desc = grpc: the client connection is closing\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T18:36:53.438336826Z I0524 18:36:53.438281       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 9.19 %, dbSize: 266686464
2022-05-24T18:36:53.438336826Z I0524 18:36:53.438303       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 15.21 %, dbSize: 285437952
2022-05-24T18:36:53.438336826Z I0524 18:36:53.438310       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 9.11 %, dbSize: 266883072
2022-05-24T18:36:54.345911282Z I0524 18:36:54.345876       1 request.go:665] Waited for 1.037005602s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2022-05-24T18:36:55.545933123Z I0524 18:36:55.545899       1 request.go:665] Waited for 1.396024697s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-149-121.ec2.internal
2022-05-24T18:38:12.206788697Z I0524 18:38:12.206723       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-149-121.ec2.internal
2022-05-24T18:38:12.218565828Z E0524 18:38:12.218531       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2022-05-24T18:38:12.219653979Z I0524 18:38:12.219592       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:36:13Z","message":"ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:38:12.233991147Z I0524 18:38:12.233950       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T18:38:12.236294308Z I0524 18:38:12.236254       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:36:13Z","message":"ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:38:12.242267125Z I0524 18:38:12.242216       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T18:38:13.818739147Z I0524 18:38:13.818703       1 request.go:665] Waited for 1.145561985s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2022-05-24T18:38:14.819518309Z I0524 18:38:14.819466       1 request.go:665] Waited for 1.1451339s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2022-05-24T18:38:27.242278922Z I0524 18:38:27.242228       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-149-121.ec2.internal
2022-05-24T18:38:27.242443032Z E0524 18:38:27.242421       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2022-05-24T18:38:27.264633323Z E0524 18:38:27.264595       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy
2022-05-24T18:38:27.266105311Z I0524 18:38:27.266044       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:36:13Z","message":"ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nNodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: cluster is unhealthy: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:38:27.280329685Z I0524 18:38:27.280280       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nNodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: cluster is unhealthy: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T18:38:27.593505760Z I0524 18:38:27.593463       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:36:13Z","message":"ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nNodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: cluster is unhealthy: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:38:27.603856092Z I0524 18:38:27.603814       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nNodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: cluster is unhealthy: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy\nEtcdMembersDegraded: No unhealthy members found" to "ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nNodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: cluster is unhealthy: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy"
2022-05-24T18:38:27.610221373Z I0524 18:38:27.610156       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:36:13Z","message":"ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nNodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: cluster is unhealthy: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:38:27.618779145Z I0524 18:38:27.618740       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Available message changed from "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available" to "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy"
2022-05-24T18:38:28.665611246Z I0524 18:38:28.665575       1 request.go:665] Waited for 1.072554025s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2022-05-24T18:38:29.865601856Z I0524 18:38:29.865553       1 request.go:665] Waited for 1.394176467s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2022-05-24T18:38:30.868672130Z I0524 18:38:30.868621       1 request.go:665] Waited for 1.397990321s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-138-197.ec2.internal
2022-05-24T18:38:36.450104512Z I0524 18:38:36.450065       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:36:13Z","message":"ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nNodeControllerDegraded: The master nodes not ready: node \"ip-10-0-149-121.ec2.internal\" not ready since 2022-05-24 18:38:36 +0000 UTC because NodeStatusUnknown (Kubelet stopped posting node status.)\nDefragControllerDegraded: cluster is unhealthy: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:38:36.471507764Z I0524 18:38:36.471459       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nNodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: cluster is unhealthy: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy" to "ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nNodeControllerDegraded: The master nodes not ready: node \"ip-10-0-149-121.ec2.internal\" not ready since 2022-05-24 18:38:36 +0000 UTC because NodeStatusUnknown (Kubelet stopped posting node status.)\nDefragControllerDegraded: cluster is unhealthy: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy"
2022-05-24T18:38:37.865127738Z I0524 18:38:37.865092       1 request.go:665] Waited for 1.129631028s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2022-05-24T18:38:38.865451553Z I0524 18:38:38.865416       1 request.go:665] Waited for 1.395818307s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2022-05-24T18:38:42.256605615Z I0524 18:38:42.256558       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-149-121.ec2.internal
2022-05-24T18:38:42.256750379Z E0524 18:38:42.256734       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2022-05-24T18:38:42.294663300Z E0524 18:38:42.294636       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy
2022-05-24T18:38:57.272974175Z I0524 18:38:57.272925       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-149-121.ec2.internal
2022-05-24T18:38:57.273269076Z E0524 18:38:57.273226       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2022-05-24T18:38:57.311598270Z E0524 18:38:57.311565       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy
2022-05-24T18:39:12.289372253Z I0524 18:39:12.289323       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-149-121.ec2.internal
2022-05-24T18:39:12.289612230Z E0524 18:39:12.289594       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2022-05-24T18:39:12.327210950Z E0524 18:39:12.327158       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy
2022-05-24T18:39:27.301571434Z I0524 18:39:27.301519       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-149-121.ec2.internal
2022-05-24T18:39:27.301752823Z E0524 18:39:27.301733       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2022-05-24T18:39:27.339623337Z E0524 18:39:27.339588       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy
2022-05-24T18:39:42.307751336Z E0524 18:39:42.307701       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2022-05-24T18:39:42.307842857Z I0524 18:39:42.307817       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-149-121.ec2.internal
2022-05-24T18:39:42.354917071Z E0524 18:39:42.354880       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy
2022-05-24T18:39:57.313883612Z I0524 18:39:57.313834       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-149-121.ec2.internal
2022-05-24T18:39:57.314050791Z E0524 18:39:57.314031       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2022-05-24T18:39:57.374705442Z E0524 18:39:57.374672       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy
2022-05-24T18:40:12.328947881Z I0524 18:40:12.328904       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-149-121.ec2.internal
2022-05-24T18:40:12.329125064Z E0524 18:40:12.329106       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2022-05-24T18:40:12.388836501Z E0524 18:40:12.388804       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy
2022-05-24T18:40:12.674019457Z I0524 18:40:12.673967       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:40:12Z","message":"ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nDefragControllerDegraded: cluster is unhealthy: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy\nNodeControllerDegraded: The master nodes not ready: node \"ip-10-0-149-121.ec2.internal\" not ready since 2022-05-24 18:38:36 +0000 UTC because NodeStatusUnknown (Kubelet stopped posting node status.)","reason":"ClusterMemberController_Error::DefragController_Error::EtcdMembers_UnhealthyMembers::NodeController_MasterNodesReady","status":"True","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:40:12.682400227Z I0524 18:40:12.682333       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded changed from False to True ("ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nDefragControllerDegraded: cluster is unhealthy: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy\nNodeControllerDegraded: The master nodes not ready: node \"ip-10-0-149-121.ec2.internal\" not ready since 2022-05-24 18:38:36 +0000 UTC because NodeStatusUnknown (Kubelet stopped posting node status.)")
2022-05-24T18:40:25.817108108Z I0524 18:40:25.817068       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:40:12Z","message":"ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nDefragControllerDegraded: cluster is unhealthy: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy","reason":"ClusterMemberController_Error::DefragController_Error::EtcdMembers_UnhealthyMembers","status":"True","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:40:25.837799867Z I0524 18:40:25.837750       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nDefragControllerDegraded: cluster is unhealthy: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy\nNodeControllerDegraded: The master nodes not ready: node \"ip-10-0-149-121.ec2.internal\" not ready since 2022-05-24 18:38:36 +0000 UTC because NodeStatusUnknown (Kubelet stopped posting node status.)" to "ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nDefragControllerDegraded: cluster is unhealthy: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy"
2022-05-24T18:40:26.978486091Z I0524 18:40:26.978452       1 request.go:665] Waited for 1.128444102s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2022-05-24T18:40:27.343516868Z I0524 18:40:27.343472       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-149-121.ec2.internal
2022-05-24T18:40:27.343886315Z E0524 18:40:27.343854       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2022-05-24T18:40:27.405021973Z E0524 18:40:27.404986       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy
2022-05-24T18:40:42.361612438Z I0524 18:40:42.361568       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-149-121.ec2.internal
2022-05-24T18:40:42.361900503Z E0524 18:40:42.361879       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2022-05-24T18:40:42.422935289Z E0524 18:40:42.422897       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy
2022-05-24T18:40:49.475820265Z I0524 18:40:49.475775       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:40:49Z","message":"NodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: cluster is unhealthy: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:40:49.486104170Z I0524 18:40:49.486061       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded changed from True to False ("NodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: cluster is unhealthy: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy")
2022-05-24T18:40:49.582040908Z I0524 18:40:49.581955       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 0.01 %, dbSize: 301109248
2022-05-24T18:40:49.596776763Z I0524 18:40:49.596739       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:40:49Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:40:49.614881619Z I0524 18:40:49.614838       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: cluster is unhealthy: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy"
2022-05-24T18:40:49.727215931Z I0524 18:40:49.727148       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 0.01 %, dbSize: 301195264
2022-05-24T18:40:50.489007587Z I0524 18:40:50.488972       1 request.go:665] Waited for 1.013047602s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/revision-pruner-6-ip-10-0-149-121.ec2.internal
2022-05-24T18:40:51.489631225Z I0524 18:40:51.489585       1 request.go:665] Waited for 1.397473931s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2022-05-24T18:40:52.489743465Z I0524 18:40:52.489706       1 request.go:665] Waited for 1.398145151s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-138-197.ec2.internal
2022-05-24T18:40:54.295319668Z I0524 18:40:54.295279       1 request.go:665] Waited for 1.003315738s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2022-05-24T18:41:12.620417851Z I0524 18:41:12.620382       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:40:49Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:41:12.644306029Z I0524 18:41:12.644262       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T18:41:12.662516152Z I0524 18:41:12.662480       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:40:49Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:41:12.676655999Z I0524 18:41:12.676611       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Available message changed from "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-149-121.ec2.internal is unhealthy" to "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available"
2022-05-24T18:41:12.820839244Z E0524 18:41:12.820790       1 base_controller.go:272] DefragController reconciliation failed: failed to dial endpoint https://10.0.138.197:2379 with maintenance client: context canceled
2022-05-24T18:41:12.823445534Z I0524 18:41:12.823396       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:40:49Z","message":"NodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: failed to dial endpoint https://10.0.138.197:2379 with maintenance client: context canceled\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:41:12.838762919Z I0524 18:41:12.838701       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: failed to dial endpoint https://10.0.138.197:2379 with maintenance client: context canceled\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T18:41:12.896839556Z I0524 18:41:12.896808       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 0.02 %, dbSize: 309166080
2022-05-24T18:41:12.896839556Z I0524 18:41:12.896825       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 0.02 %, dbSize: 309141504
2022-05-24T18:41:12.910304089Z I0524 18:41:12.909079       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:40:49Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T18:41:12.932495942Z I0524 18:41:12.932455       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: failed to dial endpoint https://10.0.138.197:2379 with maintenance client: context canceled\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T18:41:13.023147943Z I0524 18:41:13.023114       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 0.01 %, dbSize: 309211136
2022-05-24T18:41:13.023147943Z I0524 18:41:13.023131       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 0.01 %, dbSize: 308932608
2022-05-24T18:41:13.023147943Z I0524 18:41:13.023136       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 0.01 %, dbSize: 309231616
2022-05-24T18:41:13.111843790Z I0524 18:41:13.111797       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 0.03 %, dbSize: 309211136
2022-05-24T18:41:13.111843790Z I0524 18:41:13.111823       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 0.03 %, dbSize: 309231616
2022-05-24T18:41:13.819519083Z I0524 18:41:13.819481       1 request.go:665] Waited for 1.157920078s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2022-05-24T18:41:15.019231414Z I0524 18:41:15.019151       1 request.go:665] Waited for 1.5957627s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-149-121.ec2.internal
2022-05-24T18:41:16.219761281Z I0524 18:41:16.219727       1 request.go:665] Waited for 1.396628953s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-138-197.ec2.internal
2022-05-24T18:42:13.705452816Z I0524 18:42:13.705417       1 request.go:665] Waited for 1.115166822s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2022-05-24T18:42:14.705685455Z I0524 18:42:14.705651       1 request.go:665] Waited for 1.396163539s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2022-05-24T18:43:12.761388005Z I0524 18:43:12.761355       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 42.97 %, dbSize: 310960128
2022-05-24T18:43:12.761388005Z I0524 18:43:12.761371       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 42.80 %, dbSize: 310767616
2022-05-24T18:43:12.761388005Z I0524 18:43:12.761375       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 42.85 %, dbSize: 311033856
2022-05-24T18:45:10.834626065Z I0524 18:45:10.834590       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 39.76 %, dbSize: 310960128
2022-05-24T18:45:10.834626065Z I0524 18:45:10.834605       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 39.58 %, dbSize: 310767616
2022-05-24T18:45:10.834626065Z I0524 18:45:10.834610       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 39.65 %, dbSize: 311033856
2022-05-24T18:52:13.771901011Z I0524 18:52:13.771869       1 request.go:665] Waited for 1.163356923s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2022-05-24T18:52:14.771987931Z I0524 18:52:14.771947       1 request.go:665] Waited for 1.395691136s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2022-05-24T18:54:12.755127908Z I0524 18:54:12.755090       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 61.70 %, dbSize: 310960128
2022-05-24T18:54:12.755257728Z I0524 18:54:12.755231       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'DefragControllerDefragmentAttempt' Attempting defrag on member: ip-10-0-149-121.ec2.internal, memberID: 4e1a3e013d5c6145, dbSize: 310960128, dbInUse: 119087104, leader ID: 4678424576509431263
2022-05-24T18:54:13.278831442Z I0524 18:54:13.278745       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'DefragControllerDefragmentSuccess' etcd member has been defragmented: ip-10-0-149-121.ec2.internal, memberID: 5627878859398209861
2022-05-24T18:54:51.300827561Z I0524 18:54:51.300785       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 61.66 %, dbSize: 310767616
2022-05-24T18:54:51.300945838Z I0524 18:54:51.300919       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'DefragControllerDefragmentAttempt' Attempting defrag on member: ip-10-0-138-197.ec2.internal, memberID: a414a9d88f840335, dbSize: 310767616, dbInUse: 119156736, leader ID: 4678424576509431263
2022-05-24T18:54:51.854069719Z I0524 18:54:51.854021       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'DefragControllerDefragmentSuccess' etcd member has been defragmented: ip-10-0-138-197.ec2.internal, memberID: 11823261669340218165
2022-05-24T18:55:29.879053749Z I0524 18:55:29.879011       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 61.67 %, dbSize: 311033856
2022-05-24T18:55:29.879208262Z I0524 18:55:29.879153       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'DefragControllerDefragmentAttempt' Attempting defrag on member: ip-10-0-169-205.ec2.internal, memberID: 40ed1a5d6f4d4ddf, dbSize: 311033856, dbInUse: 119218176, leader ID: 4678424576509431263
2022-05-24T18:55:30.437224897Z I0524 18:55:30.436861       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'DefragControllerDefragmentSuccess' etcd member has been defragmented: ip-10-0-169-205.ec2.internal, memberID: 4678424576509431263
2022-05-24T19:02:13.772763266Z I0524 19:02:13.772723       1 request.go:665] Waited for 1.167897269s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-138-197.ec2.internal
2022-05-24T19:02:14.973489455Z I0524 19:02:14.973451       1 request.go:665] Waited for 1.396175201s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2022-05-24T19:05:12.760676128Z I0524 19:05:12.760641       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 1.62 %, dbSize: 120963072
2022-05-24T19:05:12.760676128Z I0524 19:05:12.760657       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 1.54 %, dbSize: 120909824
2022-05-24T19:05:12.760676128Z I0524 19:05:12.760661       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 1.56 %, dbSize: 120897536
2022-05-24T19:12:13.773101778Z I0524 19:12:13.773064       1 request.go:665] Waited for 1.155944213s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2022-05-24T19:12:14.773360687Z I0524 19:12:14.773324       1 request.go:665] Waited for 1.395853132s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2022-05-24T19:13:13.979706823Z E0524 19:13:13.979295       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: could not get member health: rpc error: code = Canceled desc = grpc: the client connection is closing
2022-05-24T19:13:13.988644135Z I0524 19:13:13.988606       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:40:49Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdEndpointsDegraded: could not get member health: rpc error: code = Canceled desc = grpc: the client connection is closing\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T19:13:14.003580051Z I0524 19:13:14.003532       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nEtcdEndpointsDegraded: could not get member health: rpc error: code = Canceled desc = grpc: the client connection is closing\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T19:13:14.068129983Z I0524 19:13:14.068088       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:40:49Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T19:13:14.086244564Z I0524 19:13:14.086149       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdEndpointsDegraded: could not get member health: rpc error: code = Canceled desc = grpc: the client connection is closing\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T19:13:14.093162227Z E0524 19:13:14.089007       1 base_controller.go:272] DefragController reconciliation failed: failed to dial endpoint https://10.0.169.205:2379 with maintenance client: context canceled
2022-05-24T19:13:14.095857260Z I0524 19:13:14.095824       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:40:49Z","message":"NodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: failed to dial endpoint https://10.0.169.205:2379 with maintenance client: context canceled\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T19:13:14.114321714Z I0524 19:13:14.114273       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: failed to dial endpoint https://10.0.169.205:2379 with maintenance client: context canceled\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T19:13:14.220792583Z I0524 19:13:14.220752       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 8.17 %, dbSize: 122908672
2022-05-24T19:13:14.220792583Z I0524 19:13:14.220772       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 8.23 %, dbSize: 122929152
2022-05-24T19:13:14.220792583Z I0524 19:13:14.220779       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 8.22 %, dbSize: 122908672
2022-05-24T19:13:14.233104702Z I0524 19:13:14.233061       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:40:49Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T19:13:14.241104321Z I0524 19:13:14.241059       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: failed to dial endpoint https://10.0.169.205:2379 with maintenance client: context canceled\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T19:13:14.319224623Z I0524 19:13:14.319163       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 8.17 %, dbSize: 122908672
2022-05-24T19:13:14.319224623Z I0524 19:13:14.319192       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 8.22 %, dbSize: 122929152
2022-05-24T19:13:14.319224623Z I0524 19:13:14.319199       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 8.22 %, dbSize: 122908672
2022-05-24T19:13:14.380505759Z I0524 19:13:14.380473       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 8.16 %, dbSize: 122908672
2022-05-24T19:13:14.380505759Z I0524 19:13:14.380488       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 8.22 %, dbSize: 122929152
2022-05-24T19:13:14.380505759Z I0524 19:13:14.380492       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 8.20 %, dbSize: 122908672
2022-05-24T19:13:15.096890696Z I0524 19:13:15.096847       1 request.go:665] Waited for 1.030801104s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2022-05-24T19:13:16.297223689Z I0524 19:13:16.297170       1 request.go:665] Waited for 1.395852572s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-169-205.ec2.internal
2022-05-24T19:13:17.497358007Z I0524 19:13:17.497322       1 request.go:665] Waited for 1.196631802s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-149-121.ec2.internal
2022-05-24T19:16:12.767505165Z I0524 19:16:12.767463       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 3.12 %, dbSize: 122908672
2022-05-24T19:16:12.767505165Z I0524 19:16:12.767481       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 3.17 %, dbSize: 122929152
2022-05-24T19:16:12.767505165Z I0524 19:16:12.767486       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 3.17 %, dbSize: 122908672
2022-05-24T19:22:13.773729171Z I0524 19:22:13.773676       1 request.go:665] Waited for 1.166519418s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2022-05-24T19:22:14.973567354Z I0524 19:22:14.973535       1 request.go:665] Waited for 1.393815875s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-138-197.ec2.internal
2022-05-24T19:27:12.746109143Z E0524 19:27:12.746065       1 base_controller.go:272] DefragController reconciliation failed: failed to dial endpoint https://10.0.169.205:2379 with maintenance client: context canceled
2022-05-24T19:27:12.747257520Z I0524 19:27:12.747222       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:40:49Z","message":"NodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: failed to dial endpoint https://10.0.169.205:2379 with maintenance client: context canceled\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T19:27:12.755908571Z I0524 19:27:12.755787       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: failed to dial endpoint https://10.0.169.205:2379 with maintenance client: context canceled\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T19:27:12.859710914Z I0524 19:27:12.859658       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 10.97 %, dbSize: 122908672
2022-05-24T19:27:12.859710914Z I0524 19:27:12.859676       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 11.18 %, dbSize: 123191296
2022-05-24T19:27:12.859710914Z I0524 19:27:12.859681       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 10.96 %, dbSize: 122908672
2022-05-24T19:27:12.871588371Z I0524 19:27:12.871549       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:40:49Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T19:27:12.886016758Z I0524 19:27:12.885792       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: failed to dial endpoint https://10.0.169.205:2379 with maintenance client: context canceled\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T19:27:12.957607115Z I0524 19:27:12.957568       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 10.97 %, dbSize: 122908672
2022-05-24T19:27:12.957607115Z I0524 19:27:12.957591       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 11.16 %, dbSize: 123191296
2022-05-24T19:27:12.957607115Z I0524 19:27:12.957597       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 10.95 %, dbSize: 122908672
2022-05-24T19:27:13.051776539Z I0524 19:27:13.051742       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 10.95 %, dbSize: 122908672
2022-05-24T19:27:13.051776539Z I0524 19:27:13.051758       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 11.16 %, dbSize: 123191296
2022-05-24T19:27:13.051776539Z I0524 19:27:13.051762       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 10.94 %, dbSize: 122908672
2022-05-24T19:27:13.910770902Z I0524 19:27:13.910734       1 request.go:665] Waited for 1.163687442s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd
2022-05-24T19:27:15.110417351Z I0524 19:27:15.110383       1 request.go:665] Waited for 1.392752257s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2022-05-24T19:32:13.775047271Z I0524 19:32:13.775009       1 request.go:665] Waited for 1.171729834s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-149-121.ec2.internal
2022-05-24T19:32:14.974962493Z I0524 19:32:14.974921       1 request.go:665] Waited for 1.396209415s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2022-05-24T19:38:12.780448174Z E0524 19:38:12.780406       1 base_controller.go:272] DefragController reconciliation failed: failed to dial endpoint https://10.0.138.197:2379 with maintenance client: context canceled
2022-05-24T19:38:12.780765256Z I0524 19:38:12.780732       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:40:49Z","message":"NodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: failed to dial endpoint https://10.0.138.197:2379 with maintenance client: context canceled\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T19:38:12.799417049Z I0524 19:38:12.799380       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: failed to dial endpoint https://10.0.138.197:2379 with maintenance client: context canceled\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T19:38:12.891976935Z I0524 19:38:12.891941       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 8.58 %, dbSize: 123383808
2022-05-24T19:38:12.891976935Z I0524 19:38:12.891958       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 8.46 %, dbSize: 123191296
2022-05-24T19:38:12.891976935Z I0524 19:38:12.891963       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 8.46 %, dbSize: 123179008
2022-05-24T19:38:12.903519938Z I0524 19:38:12.903484       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:40:49Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T19:38:12.916878996Z I0524 19:38:12.916834       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: failed to dial endpoint https://10.0.138.197:2379 with maintenance client: context canceled\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T19:38:13.006895497Z I0524 19:38:13.006851       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 8.57 %, dbSize: 123383808
2022-05-24T19:38:13.006895497Z I0524 19:38:13.006874       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 8.45 %, dbSize: 123191296
2022-05-24T19:38:13.006895497Z I0524 19:38:13.006882       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 8.45 %, dbSize: 123179008
2022-05-24T19:38:13.918089689Z I0524 19:38:13.918055       1 request.go:665] Waited for 1.132383779s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/revision-pruner-6-ip-10-0-149-121.ec2.internal
2022-05-24T19:38:14.919020820Z I0524 19:38:14.918976       1 request.go:665] Waited for 1.396814469s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2022-05-24T19:42:13.775486214Z I0524 19:42:13.775453       1 request.go:665] Waited for 1.169432888s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2022-05-24T19:42:14.775681008Z I0524 19:42:14.775633       1 request.go:665] Waited for 1.39481297s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2022-05-24T19:42:15.775978791Z I0524 19:42:15.775933       1 request.go:665] Waited for 1.196731471s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-169-205.ec2.internal
2022-05-24T19:49:12.755546075Z I0524 19:49:12.755499       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 6.91 %, dbSize: 123793408
2022-05-24T19:49:12.755546075Z I0524 19:49:12.755521       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 6.65 %, dbSize: 123396096
2022-05-24T19:49:12.755546075Z I0524 19:49:12.755528       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 6.62 %, dbSize: 123383808
2022-05-24T19:52:13.776499898Z I0524 19:52:13.776454       1 request.go:665] Waited for 1.048564503s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2022-05-24T19:52:14.975553020Z I0524 19:52:14.975515       1 request.go:665] Waited for 1.388952265s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2022-05-24T20:00:12.770903109Z I0524 20:00:12.770864       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 5.51 %, dbSize: 124575744
2022-05-24T20:00:12.770903109Z I0524 20:00:12.770880       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 5.59 %, dbSize: 124645376
2022-05-24T20:00:12.770903109Z I0524 20:00:12.770885       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 5.40 %, dbSize: 124444672
2022-05-24T20:02:13.778275916Z I0524 20:02:13.778242       1 request.go:665] Waited for 1.161273239s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2022-05-24T20:02:14.977281961Z I0524 20:02:14.977246       1 request.go:665] Waited for 1.397146227s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2022-05-24T20:11:12.761804161Z E0524 20:11:12.761704       1 base_controller.go:272] DefragController reconciliation failed: failed to dial endpoint https://10.0.138.197:2379 with maintenance client: context canceled
2022-05-24T20:11:12.765164863Z I0524 20:11:12.765128       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:40:49Z","message":"NodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: failed to dial endpoint https://10.0.138.197:2379 with maintenance client: context canceled\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T20:11:12.774829488Z I0524 20:11:12.774780       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: failed to dial endpoint https://10.0.138.197:2379 with maintenance client: context canceled\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T20:11:12.857500133Z I0524 20:11:12.857453       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 1.12 %, dbSize: 124964864
2022-05-24T20:11:12.857500133Z I0524 20:11:12.857470       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 1.36 %, dbSize: 125362176
2022-05-24T20:11:12.857500133Z I0524 20:11:12.857474       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 1.60 %, dbSize: 125607936
2022-05-24T20:11:12.868285252Z I0524 20:11:12.868230       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:40:49Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T20:11:12.880461721Z I0524 20:11:12.880414       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: failed to dial endpoint https://10.0.138.197:2379 with maintenance client: context canceled\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T20:11:12.963691614Z I0524 20:11:12.963658       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 1.10 %, dbSize: 124964864
2022-05-24T20:11:12.963691614Z I0524 20:11:12.963674       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 1.34 %, dbSize: 125362176
2022-05-24T20:11:12.963691614Z I0524 20:11:12.963678       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 1.59 %, dbSize: 125607936
2022-05-24T20:11:13.037217686Z I0524 20:11:13.037152       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 1.10 %, dbSize: 124964864
2022-05-24T20:11:13.037217686Z I0524 20:11:13.037170       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 1.34 %, dbSize: 125362176
2022-05-24T20:11:13.037217686Z I0524 20:11:13.037190       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 1.57 %, dbSize: 125607936
2022-05-24T20:11:13.940257202Z I0524 20:11:13.940220       1 request.go:665] Waited for 1.173316589s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-138-197.ec2.internal
2022-05-24T20:11:14.940759284Z I0524 20:11:14.940719       1 request.go:665] Waited for 1.395025389s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2022-05-24T20:11:16.140835824Z I0524 20:11:16.140795       1 request.go:665] Waited for 1.196085192s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2022-05-24T20:12:13.777567053Z I0524 20:12:13.777534       1 request.go:665] Waited for 1.160199618s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2022-05-24T20:12:14.977331734Z I0524 20:12:14.977300       1 request.go:665] Waited for 1.394650633s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2022-05-24T20:12:15.977822586Z I0524 20:12:15.977789       1 request.go:665] Waited for 1.197188146s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-169-205.ec2.internal
2022-05-24T20:22:12.768800429Z I0524 20:22:12.768759       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:40:49Z","message":"NodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: failed to dial endpoint https://10.0.138.197:2379 with maintenance client: context canceled\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T20:22:12.770236955Z E0524 20:22:12.768843       1 base_controller.go:272] DefragController reconciliation failed: failed to dial endpoint https://10.0.138.197:2379 with maintenance client: context canceled
2022-05-24T20:22:12.778395324Z I0524 20:22:12.778330       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: failed to dial endpoint https://10.0.138.197:2379 with maintenance client: context canceled\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T20:22:12.857063832Z I0524 20:22:12.857024       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 10.99 %, dbSize: 126525440
2022-05-24T20:22:12.857063832Z I0524 20:22:12.857042       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 11.66 %, dbSize: 127516672
2022-05-24T20:22:12.857063832Z I0524 20:22:12.857047       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 11.45 %, dbSize: 127160320
2022-05-24T20:22:12.868656033Z I0524 20:22:12.868606       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:40:49Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T20:22:12.881397529Z I0524 20:22:12.881342       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: failed to dial endpoint https://10.0.138.197:2379 with maintenance client: context canceled\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T20:22:12.954534838Z E0524 20:22:12.954497       1 base_controller.go:272] DefragController reconciliation failed: failed to dial endpoint https://10.0.149.121:2379 with maintenance client: context canceled
2022-05-24T20:22:12.957320754Z I0524 20:22:12.957287       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:40:49Z","message":"NodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: failed to dial endpoint https://10.0.149.121:2379 with maintenance client: context canceled\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T20:22:12.972144128Z I0524 20:22:12.972103       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: failed to dial endpoint https://10.0.149.121:2379 with maintenance client: context canceled\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T20:22:13.083871263Z I0524 20:22:13.083826       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 10.96 %, dbSize: 126525440
2022-05-24T20:22:13.083871263Z I0524 20:22:13.083849       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 11.64 %, dbSize: 127516672
2022-05-24T20:22:13.083871263Z I0524 20:22:13.083857       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 11.41 %, dbSize: 127160320
2022-05-24T20:22:13.096098828Z I0524 20:22:13.096062       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:40:49Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T20:22:13.110933103Z I0524 20:22:13.110889       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: failed to dial endpoint https://10.0.149.121:2379 with maintenance client: context canceled\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T20:22:13.221985078Z I0524 20:22:13.221944       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:40:49Z","message":"NodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: failed to dial endpoint https://10.0.149.121:2379 with maintenance client: context canceled\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T20:22:13.222521133Z E0524 20:22:13.222487       1 base_controller.go:272] DefragController reconciliation failed: failed to dial endpoint https://10.0.149.121:2379 with maintenance client: context canceled
2022-05-24T20:22:13.235862584Z I0524 20:22:13.235822       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: failed to dial endpoint https://10.0.149.121:2379 with maintenance client: context canceled\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T20:22:13.333679478Z I0524 20:22:13.333639       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 10.93 %, dbSize: 126525440
2022-05-24T20:22:13.333679478Z I0524 20:22:13.333657       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 11.62 %, dbSize: 127516672
2022-05-24T20:22:13.333679478Z I0524 20:22:13.333662       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 11.39 %, dbSize: 127160320
2022-05-24T20:22:13.347008187Z I0524 20:22:13.346965       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:40:49Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T20:22:13.357377875Z I0524 20:22:13.355986       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nDefragControllerDegraded: failed to dial endpoint https://10.0.149.121:2379 with maintenance client: context canceled\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T20:22:13.436569600Z I0524 20:22:13.435285       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 10.89 %, dbSize: 126525440
2022-05-24T20:22:13.436569600Z I0524 20:22:13.435306       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 11.60 %, dbSize: 127516672
2022-05-24T20:22:13.436569600Z I0524 20:22:13.435313       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 11.37 %, dbSize: 127160320
2022-05-24T20:22:13.508359062Z I0524 20:22:13.508326       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 10.89 %, dbSize: 126525440
2022-05-24T20:22:13.508359062Z I0524 20:22:13.508344       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 11.59 %, dbSize: 127516672
2022-05-24T20:22:13.508359062Z I0524 20:22:13.508350       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 11.36 %, dbSize: 127160320
2022-05-24T20:22:13.777998725Z I0524 20:22:13.777964       1 request.go:665] Waited for 1.160247799s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2022-05-24T20:22:14.778591431Z I0524 20:22:14.778556       1 request.go:665] Waited for 1.594015375s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2022-05-24T20:22:15.977681128Z I0524 20:22:15.977647       1 request.go:665] Waited for 1.594765568s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2022-05-24T20:32:13.779034388Z I0524 20:32:13.779001       1 request.go:665] Waited for 1.168058574s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2022-05-24T20:32:14.979006783Z I0524 20:32:14.978972       1 request.go:665] Waited for 1.391680929s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-138-197.ec2.internal
2022-05-24T20:33:12.759926718Z I0524 20:33:12.759872       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 8.27 %, dbSize: 126967808
2022-05-24T20:33:12.759926718Z I0524 20:33:12.759891       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 8.82 %, dbSize: 127737856
2022-05-24T20:33:12.759926718Z I0524 20:33:12.759898       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 8.81 %, dbSize: 127803392
2022-05-24T20:37:25.813784869Z I0524 20:37:25.813738       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:40:49Z","message":"ClusterMemberControllerDegraded: could not get list of unhealthy members: could not get member list rpc error: code = Canceled desc = grpc: the client connection is closing\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T20:37:25.814966193Z E0524 20:37:25.814926       1 base_controller.go:272] ClusterMemberController reconciliation failed: could not get list of unhealthy members: could not get member list rpc error: code = Canceled desc = grpc: the client connection is closing
2022-05-24T20:37:25.835039717Z I0524 20:37:25.834988       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "ClusterMemberControllerDegraded: could not get list of unhealthy members: could not get member list rpc error: code = Canceled desc = grpc: the client connection is closing\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T20:37:25.874663584Z I0524 20:37:25.874616       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:40:49Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T20:37:25.883523221Z I0524 20:37:25.882856       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "ClusterMemberControllerDegraded: could not get list of unhealthy members: could not get member list rpc error: code = Canceled desc = grpc: the client connection is closing\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T20:37:25.929358664Z I0524 20:37:25.929323       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 10.21 %, dbSize: 127717376
2022-05-24T20:37:25.929358664Z I0524 20:37:25.929340       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 10.44 %, dbSize: 128045056
2022-05-24T20:37:25.929358664Z I0524 20:37:25.929346       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 10.54 %, dbSize: 128225280
2022-05-24T20:37:26.007345845Z I0524 20:37:26.007297       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 10.19 %, dbSize: 127717376
2022-05-24T20:37:26.007345845Z I0524 20:37:26.007315       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 10.44 %, dbSize: 128045056
2022-05-24T20:37:26.007345845Z I0524 20:37:26.007319       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 10.53 %, dbSize: 128225280
2022-05-24T20:37:27.013507080Z I0524 20:37:27.013474       1 request.go:665] Waited for 1.138881156s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2022-05-24T20:37:28.014262982Z I0524 20:37:28.014226       1 request.go:665] Waited for 1.390418759s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-149-121.ec2.internal
2022-05-24T20:42:13.779248202Z I0524 20:42:13.779214       1 request.go:665] Waited for 1.159248204s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2022-05-24T20:42:14.779494430Z I0524 20:42:14.779459       1 request.go:665] Waited for 1.395061829s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2022-05-24T20:44:12.751811035Z I0524 20:44:12.751771       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 6.50 %, dbSize: 127717376
2022-05-24T20:44:12.751811035Z I0524 20:44:12.751788       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 6.74 %, dbSize: 128045056
2022-05-24T20:44:12.751811035Z I0524 20:44:12.751792       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 6.81 %, dbSize: 128225280
2022-05-24T20:52:13.779233391Z I0524 20:52:13.779198       1 request.go:665] Waited for 1.157674012s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2022-05-24T20:52:14.779713521Z I0524 20:52:14.779673       1 request.go:665] Waited for 1.396508305s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2022-05-24T20:55:12.765932256Z I0524 20:55:12.765892       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 6.31 %, dbSize: 128774144
2022-05-24T20:55:12.765932256Z I0524 20:55:12.765911       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 6.85 %, dbSize: 129523712
2022-05-24T20:55:12.765932256Z I0524 20:55:12.765917       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 6.68 %, dbSize: 129282048
2022-05-24T21:02:13.629083543Z I0524 21:02:13.629046       1 request.go:665] Waited for 1.015657274s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-149-121.ec2.internal
2022-05-24T21:02:14.629348627Z I0524 21:02:14.629316       1 request.go:665] Waited for 1.395451791s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-138-197.ec2.internal
2022-05-24T21:06:12.773927636Z I0524 21:06:12.773872       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 2.22 %, dbSize: 129581056
2022-05-24T21:06:12.773927636Z I0524 21:06:12.773889       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 2.57 %, dbSize: 130011136
2022-05-24T21:06:12.773927636Z I0524 21:06:12.773894       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 2.32 %, dbSize: 129736704
2022-05-24T21:12:13.780729633Z I0524 21:12:13.780693       1 request.go:665] Waited for 1.155977272s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2022-05-24T21:12:14.781275319Z I0524 21:12:14.781238       1 request.go:665] Waited for 1.396854926s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-138-197.ec2.internal
2022-05-24T21:17:12.758725220Z I0524 21:17:12.758688       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 10.85 %, dbSize: 131125248
2022-05-24T21:17:12.758725220Z I0524 21:17:12.758704       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 11.70 %, dbSize: 132280320
2022-05-24T21:17:12.758725220Z I0524 21:17:12.758709       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 11.32 %, dbSize: 131805184
2022-05-24T21:22:13.781876963Z I0524 21:22:13.781846       1 request.go:665] Waited for 1.166580728s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2022-05-24T21:22:14.980982335Z I0524 21:22:14.980946       1 request.go:665] Waited for 1.39230531s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-138-197.ec2.internal
2022-05-24T21:28:12.757346614Z I0524 21:28:12.757301       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 9.01 %, dbSize: 131452928
2022-05-24T21:28:12.757346614Z I0524 21:28:12.757318       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 9.75 %, dbSize: 132501504
2022-05-24T21:28:12.757346614Z I0524 21:28:12.757324       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 9.73 %, dbSize: 132562944
2022-05-24T21:32:13.782213353Z I0524 21:32:13.782168       1 request.go:665] Waited for 1.154610426s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2022-05-24T21:32:14.981653793Z I0524 21:32:14.981620       1 request.go:665] Waited for 1.395447356s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-138-197.ec2.internal
2022-05-24T21:39:12.762402751Z I0524 21:39:12.762363       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 6.50 %, dbSize: 132599808
2022-05-24T21:39:12.762402751Z I0524 21:39:12.762382       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 7.25 %, dbSize: 133758976
2022-05-24T21:39:12.762402751Z I0524 21:39:12.762386       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 7.27 %, dbSize: 133730304
2022-05-24T21:42:13.782267929Z I0524 21:42:13.782236       1 request.go:665] Waited for 1.166381337s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-149-121.ec2.internal
2022-05-24T21:42:14.782272718Z I0524 21:42:14.782237       1 request.go:665] Waited for 1.395442916s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2022-05-24T21:50:12.759797686Z I0524 21:50:12.759759       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 19.26 %, dbSize: 135319552
2022-05-24T21:50:12.759797686Z I0524 21:50:12.759780       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 19.81 %, dbSize: 136204288
2022-05-24T21:50:12.759797686Z I0524 21:50:12.759785       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 19.96 %, dbSize: 136486912
2022-05-24T21:52:13.782732282Z I0524 21:52:13.782701       1 request.go:665] Waited for 1.153154623s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2022-05-24T21:52:14.782838146Z I0524 21:52:14.782804       1 request.go:665] Waited for 1.39656901s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-138-197.ec2.internal
2022-05-24T21:52:15.783364948Z I0524 21:52:15.783329       1 request.go:665] Waited for 1.196893121s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-169-205.ec2.internal
2022-05-24T22:01:12.754886554Z I0524 22:01:12.754848       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 18.60 %, dbSize: 135319552
2022-05-24T22:01:12.754886554Z I0524 22:01:12.754864       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 19.16 %, dbSize: 136204288
2022-05-24T22:01:12.754886554Z I0524 22:01:12.754869       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 19.31 %, dbSize: 136486912
2022-05-24T22:02:13.784272315Z I0524 22:02:13.784238       1 request.go:665] Waited for 1.152769016s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2022-05-24T22:02:14.984156119Z I0524 22:02:14.984120       1 request.go:665] Waited for 1.394928087s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-138-197.ec2.internal
2022-05-24T22:12:12.760864248Z I0524 22:12:12.760816       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 24.80 %, dbSize: 135319552
2022-05-24T22:12:12.760864248Z I0524 22:12:12.760833       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 25.33 %, dbSize: 136204288
2022-05-24T22:12:12.760864248Z I0524 22:12:12.760837       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 25.48 %, dbSize: 136486912
2022-05-24T22:12:13.783984140Z I0524 22:12:13.783951       1 request.go:665] Waited for 1.152714279s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2022-05-24T22:12:14.784568699Z I0524 22:12:14.784534       1 request.go:665] Waited for 1.396340878s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-138-197.ec2.internal
2022-05-24T22:14:58.218550477Z E0524 22:14:58.218511       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: could not get member health: rpc error: code = Canceled desc = grpc: the client connection is closing
2022-05-24T22:14:58.220966724Z I0524 22:14:58.220932       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:40:49Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdEndpointsDegraded: could not get member health: rpc error: code = Canceled desc = grpc: the client connection is closing\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T22:14:58.236743848Z I0524 22:14:58.236696       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nEtcdEndpointsDegraded: could not get member health: rpc error: code = Canceled desc = grpc: the client connection is closing\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T22:14:58.288708352Z I0524 22:14:58.288666       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:40:49Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:24:42Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:09:41Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:41Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2022-05-24T18:07:41Z","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"}]}}
2022-05-24T22:14:58.300350371Z I0524 22:14:58.300299       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"f9580b51-c8f5-4a7c-8a00-03375aa02683", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdEndpointsDegraded: could not get member health: rpc error: code = Canceled desc = grpc: the client connection is closing\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2022-05-24T22:14:58.333784832Z I0524 22:14:58.333715       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 19.51 %, dbSize: 135319552
2022-05-24T22:14:58.333784832Z I0524 22:14:58.333736       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 20.06 %, dbSize: 136204288
2022-05-24T22:14:58.333784832Z I0524 22:14:58.333742       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 20.24 %, dbSize: 136486912
2022-05-24T22:14:58.431536841Z I0524 22:14:58.431496       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 19.48 %, dbSize: 135319552
2022-05-24T22:14:58.431536841Z I0524 22:14:58.431514       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 20.06 %, dbSize: 136204288
2022-05-24T22:14:58.431536841Z I0524 22:14:58.431520       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 20.21 %, dbSize: 136486912
2022-05-24T22:14:59.419117817Z I0524 22:14:59.419084       1 request.go:665] Waited for 1.1305373s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2022-05-24T22:15:00.619752420Z I0524 22:15:00.619715       1 request.go:665] Waited for 1.396344325s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd
2022-05-24T22:22:13.784848910Z I0524 22:22:13.784815       1 request.go:665] Waited for 1.16246129s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-149-121.ec2.internal
2022-05-24T22:22:14.784957998Z I0524 22:22:14.784912       1 request.go:665] Waited for 1.396553008s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-138-197.ec2.internal
2022-05-24T22:23:12.758925922Z I0524 22:23:12.758891       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 23.36 %, dbSize: 135319552
2022-05-24T22:23:12.758925922Z I0524 22:23:12.758908       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 23.81 %, dbSize: 136204288
2022-05-24T22:23:12.758925922Z I0524 22:23:12.758912       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 24.03 %, dbSize: 136486912
2022-05-24T22:32:13.786289404Z I0524 22:32:13.786257       1 request.go:665] Waited for 1.119184145s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2022-05-24T22:32:14.986104092Z I0524 22:32:14.986065       1 request.go:665] Waited for 1.394866901s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-138-197.ec2.internal
2022-05-24T22:34:12.767523331Z I0524 22:34:12.767489       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 21.59 %, dbSize: 135319552
2022-05-24T22:34:12.767523331Z I0524 22:34:12.767505       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 22.13 %, dbSize: 136204288
2022-05-24T22:34:12.767523331Z I0524 22:34:12.767510       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 22.32 %, dbSize: 136486912
2022-05-24T22:42:13.786266945Z I0524 22:42:13.786234       1 request.go:665] Waited for 1.153210294s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2022-05-24T22:42:14.786504809Z I0524 22:42:14.786458       1 request.go:665] Waited for 1.395815146s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2022-05-24T22:42:15.787011926Z I0524 22:42:15.786976       1 request.go:665] Waited for 1.195569758s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-169-205.ec2.internal
2022-05-24T22:45:12.768265518Z I0524 22:45:12.768227       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 18.97 %, dbSize: 135319552
2022-05-24T22:45:12.768265518Z I0524 22:45:12.768257       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 19.53 %, dbSize: 136204288
2022-05-24T22:45:12.768300133Z I0524 22:45:12.768263       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 19.93 %, dbSize: 136867840
2022-05-24T22:52:13.787162156Z I0524 22:52:13.787128       1 request.go:665] Waited for 1.163565803s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-149-121.ec2.internal
2022-05-24T22:52:14.987119192Z I0524 22:52:14.987086       1 request.go:665] Waited for 1.395191054s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-138-197.ec2.internal
2022-05-24T22:56:12.762479659Z I0524 22:56:12.762436       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 17.73 %, dbSize: 135319552
2022-05-24T22:56:12.762479659Z I0524 22:56:12.762455       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 18.27 %, dbSize: 136204288
2022-05-24T22:56:12.762479659Z I0524 22:56:12.762459       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 18.70 %, dbSize: 136867840
2022-05-24T23:02:13.788204619Z I0524 23:02:13.788150       1 request.go:665] Waited for 1.162097221s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-149-121.ec2.internal
2022-05-24T23:02:14.988346087Z I0524 23:02:14.988315       1 request.go:665] Waited for 1.396512361s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2022-05-24T23:07:12.746371829Z I0524 23:07:12.746336       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 23.12 %, dbSize: 135319552
2022-05-24T23:07:12.746371829Z I0524 23:07:12.746354       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 23.67 %, dbSize: 136204288
2022-05-24T23:07:12.746371829Z I0524 23:07:12.746361       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 24.06 %, dbSize: 136867840
2022-05-24T23:12:13.788767711Z I0524 23:12:13.788732       1 request.go:665] Waited for 1.160658706s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd
2022-05-24T23:12:14.989278789Z I0524 23:12:14.989243       1 request.go:665] Waited for 1.390467076s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2022-05-24T23:18:12.760383414Z I0524 23:18:12.760340       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 22.37 %, dbSize: 135688192
2022-05-24T23:18:12.760383414Z I0524 23:18:12.760362       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 22.87 %, dbSize: 136572928
2022-05-24T23:18:12.760383414Z I0524 23:18:12.760367       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 23.27 %, dbSize: 137236480
2022-05-24T23:22:13.789514984Z I0524 23:22:13.789480       1 request.go:665] Waited for 1.160559267s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2022-05-24T23:22:14.989771221Z I0524 23:22:14.989735       1 request.go:665] Waited for 1.39558321s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-138-197.ec2.internal
2022-05-24T23:22:16.189574825Z I0524 23:22:16.189534       1 request.go:665] Waited for 1.196616398s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-169-205.ec2.internal
2022-05-24T23:29:12.765232577Z I0524 23:29:12.765195       1 defragcontroller.go:238] etcd member "ip-10-0-149-121.ec2.internal" backend store fragmented: 20.42 %, dbSize: 136060928
2022-05-24T23:29:12.765232577Z I0524 23:29:12.765212       1 defragcontroller.go:238] etcd member "ip-10-0-138-197.ec2.internal" backend store fragmented: 20.90 %, dbSize: 136945664
2022-05-24T23:29:12.765232577Z I0524 23:29:12.765217       1 defragcontroller.go:238] etcd member "ip-10-0-169-205.ec2.internal" backend store fragmented: 21.11 %, dbSize: 137236480
2022-05-24T23:32:13.790297842Z I0524 23:32:13.790268       1 request.go:665] Waited for 1.161274045s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-149-121.ec2.internal
2022-05-24T23:32:14.989703020Z I0524 23:32:14.989664       1 request.go:665] Waited for 1.394975655s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
