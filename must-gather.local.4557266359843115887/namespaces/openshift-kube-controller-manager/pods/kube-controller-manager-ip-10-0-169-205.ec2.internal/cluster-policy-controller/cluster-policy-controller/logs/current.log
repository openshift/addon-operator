2022-05-24T18:43:59.890439198Z + timeout 3m /bin/bash -exuo pipefail -c 'while [ -n "$(ss -Htanop \( sport = 10357 \))" ]; do sleep 1; done'
2022-05-24T18:43:59.893495091Z ++ ss -Htanop '(' sport = 10357 ')'
2022-05-24T18:43:59.898274056Z + '[' -n '' ']'
2022-05-24T18:43:59.898839897Z + exec cluster-policy-controller start --config=/etc/kubernetes/static-pod-resources/configmaps/cluster-policy-controller-config/config.yaml --kubeconfig=/etc/kubernetes/static-pod-resources/configmaps/controller-manager-kubeconfig/kubeconfig --namespace=openshift-kube-controller-manager -v=2
2022-05-24T18:43:59.948353626Z I0524 18:43:59.948230       1 observer_polling.go:159] Starting file observer
2022-05-24T18:43:59.949281911Z I0524 18:43:59.949243       1 builder.go:262] cluster-policy-controller version 4.10.0-202204211158.p0.g8e5b365.assembly.stream-8e5b365-8e5b36511861e63664cf4abd4f4f63642ae3c644
2022-05-24T18:43:59.949769960Z I0524 18:43:59.949732       1 dynamic_serving_content.go:112] "Loaded a new cert/key pair" name="serving-cert::/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.crt::/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.key"
2022-05-24T18:44:00.451252400Z I0524 18:44:00.451205       1 requestheader_controller.go:244] Loaded a new request header values for RequestHeaderAuthRequestController
2022-05-24T18:44:00.454243516Z I0524 18:44:00.454212       1 genericapiserver.go:406] MuxAndDiscoveryComplete has all endpoints registered and discovery information is complete
2022-05-24T18:44:00.457770978Z W0524 18:44:00.457748       1 builder.go:321] unable to get cluster infrastructure status, using HA cluster values for leader election: infrastructures.config.openshift.io "cluster" is forbidden: User "system:kube-controller-manager" cannot get resource "infrastructures" in API group "config.openshift.io" at the cluster scope
2022-05-24T18:44:00.457886977Z I0524 18:44:00.457844       1 event.go:285] Event(v1.ObjectReference{Kind:"Pod", Namespace:"openshift-kube-controller-manager", Name:"kube-controller-manager-ip-10-0-169-205.ec2.internal", UID:"", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ClusterInfrastructureStatus' unable to get cluster infrastructure status, using HA cluster values for leader election: infrastructures.config.openshift.io "cluster" is forbidden: User "system:kube-controller-manager" cannot get resource "infrastructures" in API group "config.openshift.io" at the cluster scope
2022-05-24T18:44:00.458581262Z I0524 18:44:00.458556       1 leaderelection.go:248] attempting to acquire leader lease openshift-kube-controller-manager/cluster-policy-controller-lock...
2022-05-24T18:44:00.459500282Z I0524 18:44:00.459466       1 configmap_cafile_content.go:201] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
2022-05-24T18:44:00.459581435Z I0524 18:44:00.459466       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
2022-05-24T18:44:00.459628373Z I0524 18:44:00.459616       1 shared_informer.go:240] Waiting for caches to sync for RequestHeaderAuthRequestController
2022-05-24T18:44:00.459663233Z I0524 18:44:00.459483       1 configmap_cafile_content.go:201] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
2022-05-24T18:44:00.459693685Z I0524 18:44:00.459683       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
2022-05-24T18:44:00.459851060Z I0524 18:44:00.459560       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
2022-05-24T18:44:00.459917676Z I0524 18:44:00.459565       1 dynamic_serving_content.go:131] "Starting controller" name="serving-cert::/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.crt::/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.key"
2022-05-24T18:44:00.460009258Z I0524 18:44:00.459665       1 tlsconfig.go:200] "Loaded serving cert" certName="serving-cert::/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.crt::/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.key" certDetail="\"kube-controller-manager.openshift-kube-controller-manager.svc\" [serving] validServingFor=[kube-controller-manager.openshift-kube-controller-manager.svc,kube-controller-manager.openshift-kube-controller-manager.svc.cluster.local] issuer=\"openshift-service-serving-signer@1653415662\" (2022-05-24 18:07:55 +0000 UTC to 2024-05-23 18:07:56 +0000 UTC (now=2022-05-24 18:44:00.459630706 +0000 UTC))"
2022-05-24T18:44:00.460317596Z I0524 18:44:00.460294       1 named_certificates.go:53] "Loaded SNI cert" index=0 certName="self-signed loopback" certDetail="\"apiserver-loopback-client@1653417840\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1653417840\" (2022-05-24 17:43:59 +0000 UTC to 2023-05-24 17:43:59 +0000 UTC (now=2022-05-24 18:44:00.460253619 +0000 UTC))"
2022-05-24T18:44:00.460355256Z I0524 18:44:00.460337       1 secure_serving.go:266] Serving securely on [::]:10357
2022-05-24T18:44:00.460390473Z I0524 18:44:00.460370       1 genericapiserver.go:462] [graceful-termination] waiting for shutdown to be initiated
2022-05-24T18:44:00.460442005Z I0524 18:44:00.460396       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
2022-05-24T18:44:00.472562027Z I0524 18:44:00.472538       1 leaderelection.go:258] successfully acquired lease openshift-kube-controller-manager/cluster-policy-controller-lock
2022-05-24T18:44:00.472680114Z I0524 18:44:00.472606       1 event.go:285] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"openshift-kube-controller-manager", Name:"cluster-policy-controller-lock", UID:"e0ce8b7b-3542-441e-879e-10af93bc3aca", APIVersion:"v1", ResourceVersion:"78237", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' ip-10-0-169-205_a9312948-1b87-47ff-8353-88423a4e9c01 became leader
2022-05-24T18:44:00.474978901Z I0524 18:44:00.474955       1 policy_controller.go:78] Starting "openshift.io/namespace-security-allocation"
2022-05-24T18:44:00.485492525Z I0524 18:44:00.485065       1 policy_controller.go:88] Started "openshift.io/namespace-security-allocation"
2022-05-24T18:44:00.485492525Z I0524 18:44:00.485082       1 policy_controller.go:78] Starting "openshift.io/resourcequota"
2022-05-24T18:44:00.485492525Z I0524 18:44:00.485216       1 base_controller.go:67] Waiting for caches to sync for namespace-security-allocation-controller
2022-05-24T18:44:00.485492525Z I0524 18:44:00.485403       1 event.go:285] Event(v1.ObjectReference{Kind:"Pod", Namespace:"openshift-kube-controller-manager", Name:"kube-controller-manager-ip-10-0-169-205.ec2.internal", UID:"", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "namespace-security-allocation-controller" resync interval is set to 0s which might lead to client request throttling
2022-05-24T18:44:00.538836199Z I0524 18:44:00.538793       1 policy_controller.go:88] Started "openshift.io/resourcequota"
2022-05-24T18:44:00.538836199Z I0524 18:44:00.538812       1 policy_controller.go:78] Starting "openshift.io/cluster-quota-reconciliation"
2022-05-24T18:44:00.538836199Z I0524 18:44:00.538812       1 resource_quota_controller.go:273] Starting resource quota controller
2022-05-24T18:44:00.538873493Z I0524 18:44:00.538833       1 shared_informer.go:240] Waiting for caches to sync for resource quota
2022-05-24T18:44:00.538873493Z I0524 18:44:00.538856       1 resource_quota_monitor.go:308] QuotaMonitor running
2022-05-24T18:44:00.560270810Z I0524 18:44:00.560232       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file 
2022-05-24T18:44:00.560350895Z I0524 18:44:00.560255       1 shared_informer.go:247] Caches are synced for RequestHeaderAuthRequestController 
2022-05-24T18:44:00.560809060Z I0524 18:44:00.560777       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file 
2022-05-24T18:44:00.561355381Z I0524 18:44:00.561330       1 tlsconfig.go:178] "Loaded client CA" index=0 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"admin-kubeconfig-signer\" [] issuer=\"<self>\" (2022-05-24 17:58:08 +0000 UTC to 2032-05-21 17:58:08 +0000 UTC (now=2022-05-24 18:44:00.561290715 +0000 UTC))"
2022-05-24T18:44:00.561428885Z I0524 18:44:00.561408       1 tlsconfig.go:178] "Loaded client CA" index=1 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kubelet-signer\" [] issuer=\"<self>\" (2022-05-24 17:58:11 +0000 UTC to 2022-05-25 17:58:11 +0000 UTC (now=2022-05-24 18:44:00.561355634 +0000 UTC))"
2022-05-24T18:44:00.561463884Z I0524 18:44:00.561450       1 tlsconfig.go:178] "Loaded client CA" index=2 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-control-plane-signer\" [] issuer=\"<self>\" (2022-05-24 17:58:12 +0000 UTC to 2023-05-24 17:58:12 +0000 UTC (now=2022-05-24 18:44:00.561430825 +0000 UTC))"
2022-05-24T18:44:00.561505735Z I0524 18:44:00.561490       1 tlsconfig.go:178] "Loaded client CA" index=3 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-apiserver-to-kubelet-signer\" [] issuer=\"<self>\" (2022-05-24 17:58:12 +0000 UTC to 2023-05-24 17:58:12 +0000 UTC (now=2022-05-24 18:44:00.561465938 +0000 UTC))"
2022-05-24T18:44:00.561603734Z I0524 18:44:00.561589       1 tlsconfig.go:178] "Loaded client CA" index=4 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kubelet-bootstrap-kubeconfig-signer\" [] issuer=\"<self>\" (2022-05-24 17:58:09 +0000 UTC to 2032-05-21 17:58:09 +0000 UTC (now=2022-05-24 18:44:00.561507054 +0000 UTC))"
2022-05-24T18:44:00.561644235Z I0524 18:44:00.561629       1 tlsconfig.go:178] "Loaded client CA" index=5 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-csr-signer_@1653415662\" [] issuer=\"kubelet-signer\" (2022-05-24 18:07:41 +0000 UTC to 2022-05-25 17:58:11 +0000 UTC (now=2022-05-24 18:44:00.561605848 +0000 UTC))"
2022-05-24T18:44:00.561682589Z I0524 18:44:00.561669       1 tlsconfig.go:178] "Loaded client CA" index=6 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"openshift-kube-apiserver-operator_node-system-admin-signer@1653415661\" [] issuer=\"<self>\" (2022-05-24 18:07:40 +0000 UTC to 2023-05-24 18:07:41 +0000 UTC (now=2022-05-24 18:44:00.561643739 +0000 UTC))"
2022-05-24T18:44:00.561894001Z I0524 18:44:00.561748       1 tlsconfig.go:178] "Loaded client CA" index=7 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"aggregator-signer\" [] issuer=\"<self>\" (2022-05-24 17:58:10 +0000 UTC to 2022-05-25 17:58:10 +0000 UTC (now=2022-05-24 18:44:00.561696385 +0000 UTC))"
2022-05-24T18:44:00.562221006Z I0524 18:44:00.562167       1 tlsconfig.go:200] "Loaded serving cert" certName="serving-cert::/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.crt::/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.key" certDetail="\"kube-controller-manager.openshift-kube-controller-manager.svc\" [serving] validServingFor=[kube-controller-manager.openshift-kube-controller-manager.svc,kube-controller-manager.openshift-kube-controller-manager.svc.cluster.local] issuer=\"openshift-service-serving-signer@1653415662\" (2022-05-24 18:07:55 +0000 UTC to 2024-05-23 18:07:56 +0000 UTC (now=2022-05-24 18:44:00.562084647 +0000 UTC))"
2022-05-24T18:44:00.562451462Z I0524 18:44:00.562434       1 named_certificates.go:53] "Loaded SNI cert" index=0 certName="self-signed loopback" certDetail="\"apiserver-loopback-client@1653417840\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1653417840\" (2022-05-24 17:43:59 +0000 UTC to 2023-05-24 17:43:59 +0000 UTC (now=2022-05-24 18:44:00.562405462 +0000 UTC))"
2022-05-24T18:44:00.564609055Z I0524 18:44:00.564582       1 resource_quota_monitor.go:233] QuotaMonitor created object count evaluator for podtemplates
2022-05-24T18:44:00.564756563Z I0524 18:44:00.564743       1 resource_quota_monitor.go:233] QuotaMonitor created object count evaluator for builds.build.openshift.io
2022-05-24T18:44:00.564789172Z I0524 18:44:00.564778       1 resource_quota_monitor.go:233] QuotaMonitor created object count evaluator for roles.rbac.authorization.k8s.io
2022-05-24T18:44:00.564843888Z I0524 18:44:00.564831       1 resource_quota_monitor.go:233] QuotaMonitor created object count evaluator for networkpolicies.networking.k8s.io
2022-05-24T18:44:00.564893796Z I0524 18:44:00.564883       1 resource_quota_monitor.go:233] QuotaMonitor created object count evaluator for daemonsets.apps
2022-05-24T18:44:00.564904186Z I0524 18:44:00.564899       1 resource_quota_monitor.go:233] QuotaMonitor created object count evaluator for events.events.k8s.io
2022-05-24T18:44:00.564927767Z I0524 18:44:00.564916       1 resource_quota_monitor.go:233] QuotaMonitor created object count evaluator for routes.route.openshift.io
2022-05-24T18:44:00.564982129Z I0524 18:44:00.564971       1 resource_quota_monitor.go:233] QuotaMonitor created object count evaluator for leases.coordination.k8s.io
2022-05-24T18:44:00.565052911Z I0524 18:44:00.565042       1 resource_quota_monitor.go:233] QuotaMonitor created object count evaluator for statefulsets.apps
2022-05-24T18:44:00.565105374Z I0524 18:44:00.565094       1 resource_quota_monitor.go:233] QuotaMonitor created object count evaluator for controllerrevisions.apps
2022-05-24T18:44:00.565114938Z I0524 18:44:00.565110       1 resource_quota_monitor.go:233] QuotaMonitor created object count evaluator for deployments.apps
2022-05-24T18:44:00.565162979Z I0524 18:44:00.565152       1 resource_quota_monitor.go:233] QuotaMonitor created object count evaluator for limitranges
2022-05-24T18:44:00.565172539Z I0524 18:44:00.565168       1 resource_quota_monitor.go:233] QuotaMonitor created object count evaluator for cronjobs.batch
2022-05-24T18:44:00.565230522Z I0524 18:44:00.565218       1 resource_quota_monitor.go:233] QuotaMonitor created object count evaluator for serviceaccounts
2022-05-24T18:44:00.565289089Z I0524 18:44:00.565278       1 resource_quota_monitor.go:233] QuotaMonitor created object count evaluator for ingresses.networking.k8s.io
2022-05-24T18:44:00.565305726Z I0524 18:44:00.565295       1 resource_quota_monitor.go:233] QuotaMonitor created object count evaluator for endpointslices.discovery.k8s.io
2022-05-24T18:44:00.565383843Z I0524 18:44:00.565373       1 resource_quota_monitor.go:233] QuotaMonitor created object count evaluator for endpoints
2022-05-24T18:44:00.565402319Z I0524 18:44:00.565391       1 resource_quota_monitor.go:233] QuotaMonitor created object count evaluator for csistoragecapacities.storage.k8s.io
2022-05-24T18:44:00.565454219Z I0524 18:44:00.565443       1 resource_quota_monitor.go:233] QuotaMonitor created object count evaluator for replicasets.apps
2022-05-24T18:44:00.565498648Z I0524 18:44:00.565486       1 resource_quota_monitor.go:233] QuotaMonitor created object count evaluator for deploymentconfigs.apps.openshift.io
2022-05-24T18:44:00.565553804Z I0524 18:44:00.565543       1 resource_quota_monitor.go:233] QuotaMonitor created object count evaluator for jobs.batch
2022-05-24T18:44:00.565572205Z I0524 18:44:00.565561       1 resource_quota_monitor.go:233] QuotaMonitor created object count evaluator for rolebindings.rbac.authorization.k8s.io
2022-05-24T18:44:00.565635247Z I0524 18:44:00.565624       1 resource_quota_monitor.go:233] QuotaMonitor created object count evaluator for horizontalpodautoscalers.autoscaling
2022-05-24T18:44:00.565703334Z I0524 18:44:00.565692       1 resource_quota_monitor.go:233] QuotaMonitor created object count evaluator for buildconfigs.build.openshift.io
2022-05-24T18:44:00.565780062Z I0524 18:44:00.565769       1 resource_quota_monitor.go:233] QuotaMonitor created object count evaluator for poddisruptionbudgets.policy
2022-05-24T18:44:00.566194126Z E0524 18:44:00.566156       1 reconciliation_controller.go:124] initial monitor sync has error: [couldn't start monitor for resource "ocs.openshift.io/v1alpha1, Resource=storageconsumers": unable to monitor quota for resource "ocs.openshift.io/v1alpha1, Resource=storageconsumers", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=podmonitors": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=podmonitors", couldn't start monitor for resource "network.openshift.io/v1, Resource=egressnetworkpolicies": unable to monitor quota for resource "network.openshift.io/v1, Resource=egressnetworkpolicies", couldn't start monitor for resource "managed.openshift.io/v1alpha2, Resource=veleroinstalls": unable to monitor quota for resource "managed.openshift.io/v1alpha2, Resource=veleroinstalls", couldn't start monitor for resource "splunkforwarder.managed.openshift.io/v1alpha1, Resource=splunkforwarders": unable to monitor quota for resource "splunkforwarder.managed.openshift.io/v1alpha1, Resource=splunkforwarders", couldn't start monitor for resource "ingress.operator.openshift.io/v1, Resource=dnsrecords": unable to monitor quota for resource "ingress.operator.openshift.io/v1, Resource=dnsrecords", couldn't start monitor for resource "metal3.io/v1alpha1, Resource=hostfirmwaresettings": unable to monitor quota for resource "metal3.io/v1alpha1, Resource=hostfirmwaresettings", couldn't start monitor for resource "ceph.rook.io/v1, Resource=cephobjectrealms": unable to monitor quota for resource "ceph.rook.io/v1, Resource=cephobjectrealms", couldn't start monitor for resource "metal3.io/v1alpha1, Resource=preprovisioningimages": unable to monitor quota for resource "metal3.io/v1alpha1, Resource=preprovisioningimages", couldn't start monitor for resource "tuned.openshift.io/v1, Resource=profiles": unable to monitor quota for resource "tuned.openshift.io/v1, Resource=profiles", couldn't start monitor for resource "ceph.rook.io/v1, Resource=cephnfses": unable to monitor quota for resource "ceph.rook.io/v1, Resource=cephnfses", couldn't start monitor for resource "snapshot.storage.k8s.io/v1, Resource=volumesnapshots": unable to monitor quota for resource "snapshot.storage.k8s.io/v1, Resource=volumesnapshots", couldn't start monitor for resource "ocmagent.managed.openshift.io/v1alpha1, Resource=ocmagents": unable to monitor quota for resource "ocmagent.managed.openshift.io/v1alpha1, Resource=ocmagents", couldn't start monitor for resource "helm.openshift.io/v1beta1, Resource=projecthelmchartrepositories": unable to monitor quota for resource "helm.openshift.io/v1beta1, Resource=projecthelmchartrepositories", couldn't start monitor for resource "ceph.rook.io/v1, Resource=cephrbdmirrors": unable to monitor quota for resource "ceph.rook.io/v1, Resource=cephrbdmirrors", couldn't start monitor for resource "velero.io/v1, Resource=restores": unable to monitor quota for resource "velero.io/v1, Resource=restores", couldn't start monitor for resource "velero.io/v1, Resource=serverstatusrequests": unable to monitor quota for resource "velero.io/v1, Resource=serverstatusrequests", couldn't start monitor for resource "ceph.rook.io/v1, Resource=cephblockpools": unable to monitor quota for resource "ceph.rook.io/v1, Resource=cephblockpools", couldn't start monitor for resource "odf.openshift.io/v1alpha1, Resource=storagesystems": unable to monitor quota for resource "odf.openshift.io/v1alpha1, Resource=storagesystems", couldn't start monitor for resource "authorization.openshift.io/v1, Resource=rolebindingrestrictions": unable to monitor quota for resource "authorization.openshift.io/v1, Resource=rolebindingrestrictions", couldn't start monitor for resource "noobaa.io/v1alpha1, Resource=backingstores": unable to monitor quota for resource "noobaa.io/v1alpha1, Resource=backingstores", couldn't start monitor for resource "template.openshift.io/v1, Resource=templates": unable to monitor quota for resource "template.openshift.io/v1, Resource=templates", couldn't start monitor for resource "operators.coreos.com/v1alpha1, Resource=installplans": unable to monitor quota for resource "operators.coreos.com/v1alpha1, Resource=installplans", couldn't start monitor for resource "template.openshift.io/v1, Resource=templateinstances": unable to monitor quota for resource "template.openshift.io/v1, Resource=templateinstances", couldn't start monitor for resource "noobaa.io/v1alpha1, Resource=bucketclasses": unable to monitor quota for resource "noobaa.io/v1alpha1, Resource=bucketclasses", couldn't start monitor for resource "ceph.rook.io/v1, Resource=cephclusters": unable to monitor quota for resource "ceph.rook.io/v1, Resource=cephclusters", couldn't start monitor for resource "ceph.rook.io/v1, Resource=cephobjectstores": unable to monitor quota for resource "ceph.rook.io/v1, Resource=cephobjectstores", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=prometheusrules": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=prometheusrules", couldn't start monitor for resource "ocs.openshift.io/v1, Resource=storageclusters": unable to monitor quota for resource "ocs.openshift.io/v1, Resource=storageclusters", couldn't start monitor for resource "velero.io/v1, Resource=deletebackuprequests": unable to monitor quota for resource "velero.io/v1, Resource=deletebackuprequests", couldn't start monitor for resource "noobaa.io/v1alpha1, Resource=noobaas": unable to monitor quota for resource "noobaa.io/v1alpha1, Resource=noobaas", couldn't start monitor for resource "objectbucket.io/v1alpha1, Resource=objectbucketclaims": unable to monitor quota for resource "objectbucket.io/v1alpha1, Resource=objectbucketclaims", couldn't start monitor for resource "metal3.io/v1alpha1, Resource=bmceventsubscriptions": unable to monitor quota for resource "metal3.io/v1alpha1, Resource=bmceventsubscriptions", couldn't start monitor for resource "metal3.io/v1alpha1, Resource=firmwareschemas": unable to monitor quota for resource "metal3.io/v1alpha1, Resource=firmwareschemas", couldn't start monitor for resource "velero.io/v1, Resource=resticrepositories": unable to monitor quota for resource "velero.io/v1, Resource=resticrepositories", couldn't start monitor for resource "whereabouts.cni.cncf.io/v1alpha1, Resource=ippools": unable to monitor quota for resource "whereabouts.cni.cncf.io/v1alpha1, Resource=ippools", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=prometheuses": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=prometheuses", couldn't start monitor for resource "operators.coreos.com/v1alpha1, Resource=subscriptions": unable to monitor quota for resource "operators.coreos.com/v1alpha1, Resource=subscriptions", couldn't start monitor for resource "tuned.openshift.io/v1, Resource=tuneds": unable to monitor quota for resource "tuned.openshift.io/v1, Resource=tuneds", couldn't start monitor for resource "velero.io/v1, Resource=downloadrequests": unable to monitor quota for resource "velero.io/v1, Resource=downloadrequests", couldn't start monitor for resource "operators.coreos.com/v1alpha1, Resource=catalogsources": unable to monitor quota for resource "operators.coreos.com/v1alpha1, Resource=catalogsources", couldn't start monitor for resource "ocmagent.managed.openshift.io/v1alpha1, Resource=managednotifications": unable to monitor quota for resource "ocmagent.managed.openshift.io/v1alpha1, Resource=managednotifications", couldn't start monitor for resource "ceph.rook.io/v1, Resource=cephobjectzones": unable to monitor quota for resource "ceph.rook.io/v1, Resource=cephobjectzones", couldn't start monitor for resource "velero.io/v1, Resource=volumesnapshotlocations": unable to monitor quota for resource "velero.io/v1, Resource=volumesnapshotlocations", couldn't start monitor for resource "ceph.rook.io/v1, Resource=cephobjectstoreusers": unable to monitor quota for resource "ceph.rook.io/v1, Resource=cephobjectstoreusers", couldn't start monitor for resource "operators.coreos.com/v1, Resource=operatorgroups": unable to monitor quota for resource "operators.coreos.com/v1, Resourc2022-05-24T18:44:00.566212931Z e=operatorgroups", couldn't start monitor for resource "network.operator.openshift.io/v1, Resource=operatorpkis": unable to monitor quota for resource "network.operator.openshift.io/v1, Resource=operatorpkis", couldn't start monitor for resource "controlplane.operator.openshift.io/v1alpha1, Resource=podnetworkconnectivitychecks": unable to monitor quota for resource "controlplane.operator.openshift.io/v1alpha1, Resource=podnetworkconnectivitychecks", couldn't start monitor for resource "addons.managed.openshift.io/v1alpha1, Resource=addoninstances": unable to monitor quota for resource "addons.managed.openshift.io/v1alpha1, Resource=addoninstances", couldn't start monitor for resource "monitoring.openshift.io/v1alpha1, Resource=clusterurlmonitors": unable to monitor quota for resource "monitoring.openshift.io/v1alpha1, Resource=clusterurlmonitors", couldn't start monitor for resource "velero.io/v1, Resource=schedules": unable to monitor quota for resource "velero.io/v1, Resource=schedules", couldn't start monitor for resource "noobaa.io/v1alpha1, Resource=noobaaaccounts": unable to monitor quota for resource "noobaa.io/v1alpha1, Resource=noobaaaccounts", couldn't start monitor for resource "k8s.cni.cncf.io/v1, Resource=network-attachment-definitions": unable to monitor quota for resource "k8s.cni.cncf.io/v1, Resource=network-attachment-definitions", couldn't start monitor for resource "network.operator.openshift.io/v1, Resource=egressrouters": unable to monitor quota for resource "network.operator.openshift.io/v1, Resource=egressrouters", couldn't start monitor for resource "ceph.rook.io/v1, Resource=cephclients": unable to monitor quota for resource "ceph.rook.io/v1, Resource=cephclients", couldn't start monitor for resource "ceph.rook.io/v1, Resource=cephbucketnotifications": unable to monitor quota for resource "ceph.rook.io/v1, Resource=cephbucketnotifications", couldn't start monitor for resource "monitoring.coreos.com/v1alpha1, Resource=alertmanagerconfigs": unable to monitor quota for resource "monitoring.coreos.com/v1alpha1, Resource=alertmanagerconfigs", couldn't start monitor for resource "operators.coreos.com/v2, Resource=operatorconditions": unable to monitor quota for resource "operators.coreos.com/v2, Resource=operatorconditions", couldn't start monitor for resource "ocs.openshift.io/v1, Resource=ocsinitializations": unable to monitor quota for resource "ocs.openshift.io/v1, Resource=ocsinitializations", couldn't start monitor for resource "velero.io/v1, Resource=podvolumebackups": unable to monitor quota for resource "velero.io/v1, Resource=podvolumebackups", couldn't start monitor for resource "noobaa.io/v1alpha1, Resource=namespacestores": unable to monitor quota for resource "noobaa.io/v1alpha1, Resource=namespacestores", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=probes": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=probes", couldn't start monitor for resource "ceph.rook.io/v1, Resource=cephfilesystems": unable to monitor quota for resource "ceph.rook.io/v1, Resource=cephfilesystems", couldn't start monitor for resource "replication.storage.openshift.io/v1alpha1, Resource=volumereplications": unable to monitor quota for resource "replication.storage.openshift.io/v1alpha1, Resource=volumereplications", couldn't start monitor for resource "ceph.rook.io/v1, Resource=cephfilesystemmirrors": unable to monitor quota for resource "ceph.rook.io/v1, Resource=cephfilesystemmirrors", couldn't start monitor for resource "csiaddons.openshift.io/v1alpha1, Resource=csiaddonsnodes": unable to monitor quota for resource "csiaddons.openshift.io/v1alpha1, Resource=csiaddonsnodes", couldn't start monitor for resource "csiaddons.openshift.io/v1alpha1, Resource=reclaimspacejobs": unable to monitor quota for resource "csiaddons.openshift.io/v1alpha1, Resource=reclaimspacejobs", couldn't start monitor for resource "whereabouts.cni.cncf.io/v1alpha1, Resource=overlappingrangeipreservations": unable to monitor quota for resource "whereabouts.cni.cncf.io/v1alpha1, Resource=overlappingrangeipreservations", couldn't start monitor for resource "machine.openshift.io/v1beta1, Resource=machinesets": unable to monitor quota for resource "machine.openshift.io/v1beta1, Resource=machinesets", couldn't start monitor for resource "velero.io/v1, Resource=backupstoragelocations": unable to monitor quota for resource "velero.io/v1, Resource=backupstoragelocations", couldn't start monitor for resource "upgrade.managed.openshift.io/v1alpha1, Resource=upgradeconfigs": unable to monitor quota for resource "upgrade.managed.openshift.io/v1alpha1, Resource=upgradeconfigs", couldn't start monitor for resource "ceph.rook.io/v1, Resource=cephbuckettopics": unable to monitor quota for resource "ceph.rook.io/v1, Resource=cephbuckettopics", couldn't start monitor for resource "velero.io/v1, Resource=podvolumerestores": unable to monitor quota for resource "velero.io/v1, Resource=podvolumerestores", couldn't start monitor for resource "velero.io/v1, Resource=backups": unable to monitor quota for resource "velero.io/v1, Resource=backups", couldn't start monitor for resource "managed.openshift.io/v1alpha1, Resource=mustgathers": unable to monitor quota for resource "managed.openshift.io/v1alpha1, Resource=mustgathers", couldn't start monitor for resource "ceph.rook.io/v1, Resource=cephobjectzonegroups": unable to monitor quota for resource "ceph.rook.io/v1, Resource=cephobjectzonegroups", couldn't start monitor for resource "cloudcredential.openshift.io/v1, Resource=credentialsrequests": unable to monitor quota for resource "cloudcredential.openshift.io/v1, Resource=credentialsrequests", couldn't start monitor for resource "operators.coreos.com/v1alpha1, Resource=clusterserviceversions": unable to monitor quota for resource "operators.coreos.com/v1alpha1, Resource=clusterserviceversions", couldn't start monitor for resource "ceph.rook.io/v1, Resource=cephfilesystemsubvolumegroups": unable to monitor quota for resource "ceph.rook.io/v1, Resource=cephfilesystemsubvolumegroups", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=thanosrulers": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=thanosrulers", couldn't start monitor for resource "ocs.openshift.io/v1alpha1, Resource=managedocs": unable to monitor quota for resource "ocs.openshift.io/v1alpha1, Resource=managedocs", couldn't start monitor for resource "autoscaling.openshift.io/v1beta1, Resource=machineautoscalers": unable to monitor quota for resource "autoscaling.openshift.io/v1beta1, Resource=machineautoscalers", couldn't start monitor for resource "operator.openshift.io/v1, Resource=ingresscontrollers": unable to monitor quota for resource "operator.openshift.io/v1, Resource=ingresscontrollers", couldn't start monitor for resource "csiaddons.openshift.io/v1alpha1, Resource=reclaimspacecronjobs": unable to monitor quota for resource "csiaddons.openshift.io/v1alpha1, Resource=reclaimspacecronjobs", couldn't start monitor for resource "machine.openshift.io/v1beta1, Resource=machinehealthchecks": unable to monitor quota for resource "machine.openshift.io/v1beta1, Resource=machinehealthchecks", couldn't start monitor for resource "metal3.io/v1alpha1, Resource=baremetalhosts": unable to monitor quota for resource "metal3.io/v1alpha1, Resource=baremetalhosts", couldn't start monitor for resource "managed.openshift.io/v1alpha1, Resource=subjectpermissions": unable to monitor quota for resource "managed.openshift.io/v1alpha1, Resource=subjectpermissions", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=servicemonitors": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=servicemonitors", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=alertmanagers": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=alertmanagers", couldn't start monitor for resource "monitoring.openshift.io/v1alpha1, Resource=routemonitors": unable to monitor quota for resource "monitoring.openshift.io/v1alpha1, Resource=routemonitors", couldn't start monitor for re2022-05-24T18:44:00.566229117Z source "machine.openshift.io/v1beta1, Resource=machines": unable to monitor quota for resource "machine.openshift.io/v1beta1, Resource=machines"]
2022-05-24T18:44:00.566229117Z I0524 18:44:00.566207       1 policy_controller.go:88] Started "openshift.io/cluster-quota-reconciliation"
2022-05-24T18:44:00.566229117Z I0524 18:44:00.566216       1 policy_controller.go:78] Starting "openshift.io/cluster-csr-approver"
2022-05-24T18:44:00.566251334Z I0524 18:44:00.566233       1 clusterquotamapping.go:127] Starting ClusterQuotaMappingController controller
2022-05-24T18:44:00.566251334Z I0524 18:44:00.566244       1 reconciliation_controller.go:137] Starting the cluster quota reconciliation controller
2022-05-24T18:44:00.566279812Z I0524 18:44:00.566268       1 resource_quota_monitor.go:308] QuotaMonitor running
2022-05-24T18:44:00.570251573Z I0524 18:44:00.570223       1 policy_controller.go:88] Started "openshift.io/cluster-csr-approver"
2022-05-24T18:44:00.570251573Z I0524 18:44:00.570239       1 policy_controller.go:91] Started Origin Controllers
2022-05-24T18:44:00.570529445Z I0524 18:44:00.570483       1 base_controller.go:67] Waiting for caches to sync for WebhookAuthenticatorCertApprover_csr-approver-controller
2022-05-24T18:44:00.570581522Z I0524 18:44:00.570516       1 event.go:285] Event(v1.ObjectReference{Kind:"Pod", Namespace:"openshift-kube-controller-manager", Name:"kube-controller-manager-ip-10-0-169-205.ec2.internal", UID:"", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "WebhookAuthenticatorCertApprover_csr-approver-controller" resync interval is set to 0s which might lead to client request throttling
2022-05-24T18:44:00.671425679Z I0524 18:44:00.671380       1 base_controller.go:73] Caches are synced for WebhookAuthenticatorCertApprover_csr-approver-controller 
2022-05-24T18:44:00.671425679Z I0524 18:44:00.671408       1 base_controller.go:110] Starting #1 worker of WebhookAuthenticatorCertApprover_csr-approver-controller controller ...
2022-05-24T18:44:00.939744046Z I0524 18:44:00.939701       1 shared_informer.go:247] Caches are synced for resource quota 
2022-05-24T18:44:01.186156244Z I0524 18:44:01.186100       1 base_controller.go:73] Caches are synced for namespace-security-allocation-controller 
2022-05-24T18:44:01.186156244Z I0524 18:44:01.186128       1 base_controller.go:110] Starting #1 worker of namespace-security-allocation-controller controller ...
2022-05-24T18:44:01.186227758Z I0524 18:44:01.186208       1 namespace_scc_allocation_controller.go:111] Repairing SCC UID Allocations
2022-05-24T18:44:01.675297022Z I0524 18:44:01.675257       1 request.go:665] Waited for 1.104629335s due to client-side throttling, not priority and fairness, request: GET:https://api-int.odf-service.dif5.p1.openshiftapps.com:6443/api/v1/replicationcontrollers?limit=500&resourceVersion=0
2022-05-24T18:44:02.676102688Z I0524 18:44:02.676061       1 request.go:665] Waited for 2.104912608s due to client-side throttling, not priority and fairness, request: GET:https://api-int.odf-service.dif5.p1.openshiftapps.com:6443/api/v1/serviceaccounts?limit=500&resourceVersion=0
2022-05-24T18:44:03.324006326Z I0524 18:44:03.323961       1 namespace_scc_allocation_controller.go:116] Repair complete
2022-05-24T18:44:03.695372273Z I0524 18:44:03.695334       1 request.go:665] Waited for 3.153705388s due to client-side throttling, not priority and fairness, request: GET:https://api-int.odf-service.dif5.p1.openshiftapps.com:6443/apis/autoscaling/v2beta2
2022-05-24T18:44:04.695512775Z I0524 18:44:04.695472       1 request.go:665] Waited for 4.153825299s due to client-side throttling, not priority and fairness, request: GET:https://api-int.odf-service.dif5.p1.openshiftapps.com:6443/apis/batch/v1beta1
2022-05-24T18:44:05.750434918Z I0524 18:44:05.750401       1 request.go:665] Waited for 5.18142272s due to client-side throttling, not priority and fairness, request: GET:https://api-int.odf-service.dif5.p1.openshiftapps.com:6443/apis/objectbucket.io/v1alpha1
2022-05-24T18:44:06.751246999Z I0524 18:44:06.751197       1 request.go:665] Waited for 6.182178998s due to client-side throttling, not priority and fairness, request: GET:https://api-int.odf-service.dif5.p1.openshiftapps.com:6443/apis/ocmagent.managed.openshift.io/v1alpha1
2022-05-24T18:44:07.895191194Z I0524 18:44:07.895137       1 request.go:665] Waited for 7.353444928s due to client-side throttling, not priority and fairness, request: GET:https://api-int.odf-service.dif5.p1.openshiftapps.com:6443/apis/operators.coreos.com/v1
2022-05-24T18:44:08.895251646Z I0524 18:44:08.895217       1 request.go:665] Waited for 8.353486276s due to client-side throttling, not priority and fairness, request: GET:https://api-int.odf-service.dif5.p1.openshiftapps.com:6443/apis/project.openshift.io/v1
2022-05-24T18:44:09.895388377Z I0524 18:44:09.895351       1 request.go:665] Waited for 9.353634586s due to client-side throttling, not priority and fairness, request: GET:https://api-int.odf-service.dif5.p1.openshiftapps.com:6443/apis/samples.operator.openshift.io/v1
2022-05-24T18:44:10.950651034Z I0524 18:44:10.950608       1 request.go:665] Waited for 10.381573206s due to client-side throttling, not priority and fairness, request: GET:https://api-int.odf-service.dif5.p1.openshiftapps.com:6443/apis/k8s.cni.cncf.io/v1
2022-05-24T18:44:12.095273649Z I0524 18:44:12.095225       1 request.go:665] Waited for 11.553456525s due to client-side throttling, not priority and fairness, request: GET:https://api-int.odf-service.dif5.p1.openshiftapps.com:6443/apis/config.openshift.io/v1
2022-05-24T18:44:13.095433223Z I0524 18:44:13.095395       1 request.go:665] Waited for 12.553593035s due to client-side throttling, not priority and fairness, request: GET:https://api-int.odf-service.dif5.p1.openshiftapps.com:6443/apis/operator.openshift.io/v1alpha1
2022-05-24T18:44:14.150843383Z I0524 18:44:14.150810       1 request.go:665] Waited for 13.581747592s due to client-side throttling, not priority and fairness, request: GET:https://api-int.odf-service.dif5.p1.openshiftapps.com:6443/apis/tuned.openshift.io/v1
2022-05-24T18:44:15.151027761Z I0524 18:44:15.150984       1 request.go:665] Waited for 14.581904067s due to client-side throttling, not priority and fairness, request: GET:https://api-int.odf-service.dif5.p1.openshiftapps.com:6443/apis/velero.io/v1
2022-05-24T18:44:16.295504305Z I0524 18:44:16.295467       1 request.go:665] Waited for 15.753612938s due to client-side throttling, not priority and fairness, request: GET:https://api-int.odf-service.dif5.p1.openshiftapps.com:6443/apis/managed.openshift.io/v1alpha1
2022-05-24T18:44:17.350938289Z I0524 18:44:17.350901       1 request.go:665] Waited for 16.781819909s due to client-side throttling, not priority and fairness, request: GET:https://api-int.odf-service.dif5.p1.openshiftapps.com:6443/apis/rbac.authorization.k8s.io/v1
2022-05-24T18:44:18.096922865Z I0524 18:44:18.096882       1 resource_quota_controller.go:439] syncing resource quota controller with updated resources from discovery: added: [image.openshift.io/v1, Resource=imagestreams], removed: []
2022-05-24T18:44:18.096960722Z I0524 18:44:18.096918       1 shared_informer.go:240] Waiting for caches to sync for resource quota
2022-05-24T18:44:18.096960722Z I0524 18:44:18.096928       1 shared_informer.go:247] Caches are synced for resource quota 
2022-05-24T18:44:18.096960722Z I0524 18:44:18.096933       1 resource_quota_controller.go:458] synced quota controller
2022-05-24T18:44:18.152514170Z I0524 18:44:18.152322       1 reconciliation_controller.go:187] syncing resource quota controller with updated resources from discovery: map[/v1, Resource=configmaps:{} /v1, Resource=endpoints:{} /v1, Resource=events:{} /v1, Resource=limitranges:{} /v1, Resource=persistentvolumeclaims:{} /v1, Resource=pods:{} /v1, Resource=podtemplates:{} /v1, Resource=replicationcontrollers:{} /v1, Resource=resourcequotas:{} /v1, Resource=secrets:{} /v1, Resource=serviceaccounts:{} /v1, Resource=services:{} addons.managed.openshift.io/v1alpha1, Resource=addoninstances:{} apps/v1, Resource=controllerrevisions:{} apps/v1, Resource=daemonsets:{} apps/v1, Resource=deployments:{} apps/v1, Resource=replicasets:{} apps/v1, Resource=statefulsets:{} apps.openshift.io/v1, Resource=deploymentconfigs:{} authorization.openshift.io/v1, Resource=rolebindingrestrictions:{} autoscaling/v2, Resource=horizontalpodautoscalers:{} autoscaling.openshift.io/v1beta1, Resource=machineautoscalers:{} batch/v1, Resource=cronjobs:{} batch/v1, Resource=jobs:{} build.openshift.io/v1, Resource=buildconfigs:{} build.openshift.io/v1, Resource=builds:{} ceph.rook.io/v1, Resource=cephblockpools:{} ceph.rook.io/v1, Resource=cephbucketnotifications:{} ceph.rook.io/v1, Resource=cephbuckettopics:{} ceph.rook.io/v1, Resource=cephclients:{} ceph.rook.io/v1, Resource=cephclusters:{} ceph.rook.io/v1, Resource=cephfilesystemmirrors:{} ceph.rook.io/v1, Resource=cephfilesystems:{} ceph.rook.io/v1, Resource=cephfilesystemsubvolumegroups:{} ceph.rook.io/v1, Resource=cephnfses:{} ceph.rook.io/v1, Resource=cephobjectrealms:{} ceph.rook.io/v1, Resource=cephobjectstores:{} ceph.rook.io/v1, Resource=cephobjectstoreusers:{} ceph.rook.io/v1, Resource=cephobjectzonegroups:{} ceph.rook.io/v1, Resource=cephobjectzones:{} ceph.rook.io/v1, Resource=cephrbdmirrors:{} cloudcredential.openshift.io/v1, Resource=credentialsrequests:{} controlplane.operator.openshift.io/v1alpha1, Resource=podnetworkconnectivitychecks:{} coordination.k8s.io/v1, Resource=leases:{} csiaddons.openshift.io/v1alpha1, Resource=csiaddonsnodes:{} csiaddons.openshift.io/v1alpha1, Resource=reclaimspacecronjobs:{} csiaddons.openshift.io/v1alpha1, Resource=reclaimspacejobs:{} discovery.k8s.io/v1, Resource=endpointslices:{} events.k8s.io/v1, Resource=events:{} helm.openshift.io/v1beta1, Resource=projecthelmchartrepositories:{} image.openshift.io/v1, Resource=imagestreams:{} ingress.operator.openshift.io/v1, Resource=dnsrecords:{} k8s.cni.cncf.io/v1, Resource=network-attachment-definitions:{} machine.openshift.io/v1beta1, Resource=machinehealthchecks:{} machine.openshift.io/v1beta1, Resource=machines:{} machine.openshift.io/v1beta1, Resource=machinesets:{} managed.openshift.io/v1alpha1, Resource=mustgathers:{} managed.openshift.io/v1alpha1, Resource=subjectpermissions:{} managed.openshift.io/v1alpha2, Resource=veleroinstalls:{} metal3.io/v1alpha1, Resource=baremetalhosts:{} metal3.io/v1alpha1, Resource=bmceventsubscriptions:{} metal3.io/v1alpha1, Resource=firmwareschemas:{} metal3.io/v1alpha1, Resource=hostfirmwaresettings:{} metal3.io/v1alpha1, Resource=preprovisioningimages:{} monitoring.coreos.com/v1, Resource=alertmanagers:{} monitoring.coreos.com/v1, Resource=podmonitors:{} monitoring.coreos.com/v1, Resource=probes:{} monitoring.coreos.com/v1, Resource=prometheuses:{} monitoring.coreos.com/v1, Resource=prometheusrules:{} monitoring.coreos.com/v1, Resource=servicemonitors:{} monitoring.coreos.com/v1, Resource=thanosrulers:{} monitoring.coreos.com/v1alpha1, Resource=alertmanagerconfigs:{} monitoring.openshift.io/v1alpha1, Resource=clusterurlmonitors:{} monitoring.openshift.io/v1alpha1, Resource=routemonitors:{} network.openshift.io/v1, Resource=egressnetworkpolicies:{} network.operator.openshift.io/v1, Resource=egressrouters:{} network.operator.openshift.io/v1, Resource=operatorpkis:{} networking.k8s.io/v1, Resource=ingresses:{} networking.k8s.io/v1, Resource=networkpolicies:{} noobaa.io/v1alpha1, Resource=backingstores:{} noobaa.io/v1alpha1, Resource=bucketclasses:{} noobaa.io/v1alpha1, Resource=namespacestores:{} noobaa.io/v1alpha1, Resource=noobaaaccounts:{} noobaa.io/v1alpha1, Resource=noobaas:{} objectbucket.io/v1alpha1, Resource=objectbucketclaims:{} ocmagent.managed.openshift.io/v1alpha1, Resource=managednotifications:{} ocmagent.managed.openshift.io/v1alpha1, Resource=ocmagents:{} ocs.openshift.io/v1, Resource=ocsinitializations:{} ocs.openshift.io/v1, Resource=storageclusters:{} ocs.openshift.io/v1alpha1, Resource=managedocs:{} ocs.openshift.io/v1alpha1, Resource=storageconsumers:{} odf.openshift.io/v1alpha1, Resource=storagesystems:{} operator.openshift.io/v1, Resource=ingresscontrollers:{} operators.coreos.com/v1, Resource=operatorgroups:{} operators.coreos.com/v1alpha1, Resource=catalogsources:{} operators.coreos.com/v1alpha1, Resource=clusterserviceversions:{} operators.coreos.com/v1alpha1, Resource=installplans:{} operators.coreos.com/v1alpha1, Resource=subscriptions:{} operators.coreos.com/v2, Resource=operatorconditions:{} policy/v1, Resource=poddisruptionbudgets:{} rbac.authorization.k8s.io/v1, Resource=rolebindings:{} rbac.authorization.k8s.io/v1, Resource=roles:{} replication.storage.openshift.io/v1alpha1, Resource=volumereplications:{} route.openshift.io/v1, Resource=routes:{} snapshot.storage.k8s.io/v1, Resource=volumesnapshots:{} splunkforwarder.managed.openshift.io/v1alpha1, Resource=splunkforwarders:{} storage.k8s.io/v1beta1, Resource=csistoragecapacities:{} template.openshift.io/v1, Resource=templateinstances:{} template.openshift.io/v1, Resource=templates:{} tuned.openshift.io/v1, Resource=profiles:{} tuned.openshift.io/v1, Resource=tuneds:{} upgrade.managed.openshift.io/v1alpha1, Resource=upgradeconfigs:{} velero.io/v1, Resource=backups:{} velero.io/v1, Resource=backupstoragelocations:{} velero.io/v1, Resource=deletebackuprequests:{} velero.io/v1, Resource=downloadrequests:{} velero.io/v1, Resource=podvolumebackups:{} velero.io/v1, Resource=podvolumerestores:{} velero.io/v1, Resource=resticrepositories:{} velero.io/v1, Resource=restores:{} velero.io/v1, Resource=schedules:{} velero.io/v1, Resource=serverstatusrequests:{} velero.io/v1, Resource=volumesnapshotlocations:{} whereabouts.cni.cncf.io/v1alpha1, Resource=ippools:{} whereabouts.cni.cncf.io/v1alpha1, Resource=overlappingrangeipreservations:{}]
2022-05-24T18:44:18.153792037Z E0524 18:44:18.153744       1 reconciliation_controller.go:197] failed to sync resource monitors: [couldn't start monitor for resource "metal3.io/v1alpha1, Resource=preprovisioningimages": unable to monitor quota for resource "metal3.io/v1alpha1, Resource=preprovisioningimages", couldn't start monitor for resource "network.operator.openshift.io/v1, Resource=operatorpkis": unable to monitor quota for resource "network.operator.openshift.io/v1, Resource=operatorpkis", couldn't start monitor for resource "metal3.io/v1alpha1, Resource=hostfirmwaresettings": unable to monitor quota for resource "metal3.io/v1alpha1, Resource=hostfirmwaresettings", couldn't start monitor for resource "velero.io/v1, Resource=podvolumerestores": unable to monitor quota for resource "velero.io/v1, Resource=podvolumerestores", couldn't start monitor for resource "monitoring.openshift.io/v1alpha1, Resource=clusterurlmonitors": unable to monitor quota for resource "monitoring.openshift.io/v1alpha1, Resource=clusterurlmonitors", couldn't start monitor for resource "splunkforwarder.managed.openshift.io/v1alpha1, Resource=splunkforwarders": unable to monitor quota for resource "splunkforwarder.managed.openshift.io/v1alpha1, Resource=splunkforwarders", couldn't start monitor for resource "ceph.rook.io/v1, Resource=cephbuckettopics": unable to monitor quota for resource "ceph.rook.io/v1, Resource=cephbuckettopics", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=alertmanagers": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=alertmanagers", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=probes": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=probes", couldn't start monitor for resource "ingress.operator.openshift.io/v1, Resource=dnsrecords": unable to monitor quota for resource "ingress.operator.openshift.io/v1, Resource=dnsrecords", couldn't start monitor for resource "network.openshift.io/v1, Resource=egressnetworkpolicies": unable to monitor quota for resource "network.openshift.io/v1, Resource=egressnetworkpolicies", couldn't start monitor for resource "ocs.openshift.io/v1, Resource=storageclusters": unable to monitor quota for resource "ocs.openshift.io/v1, Resource=storageclusters", couldn't start monitor for resource "velero.io/v1, Resource=podvolumebackups": unable to monitor quota for resource "velero.io/v1, Resource=podvolumebackups", couldn't start monitor for resource "managed.openshift.io/v1alpha1, Resource=mustgathers": unable to monitor quota for resource "managed.openshift.io/v1alpha1, Resource=mustgathers", couldn't start monitor for resource "noobaa.io/v1alpha1, Resource=backingstores": unable to monitor quota for resource "noobaa.io/v1alpha1, Resource=backingstores", couldn't start monitor for resource "velero.io/v1, Resource=schedules": unable to monitor quota for resource "velero.io/v1, Resource=schedules", couldn't start monitor for resource "noobaa.io/v1alpha1, Resource=bucketclasses": unable to monitor quota for resource "noobaa.io/v1alpha1, Resource=bucketclasses", couldn't start monitor for resource "authorization.openshift.io/v1, Resource=rolebindingrestrictions": unable to monitor quota for resource "authorization.openshift.io/v1, Resource=rolebindingrestrictions", couldn't start monitor for resource "operators.coreos.com/v1alpha1, Resource=catalogsources": unable to monitor quota for resource "operators.coreos.com/v1alpha1, Resource=catalogsources", couldn't start monitor for resource "velero.io/v1, Resource=volumesnapshotlocations": unable to monitor quota for resource "velero.io/v1, Resource=volumesnapshotlocations", couldn't start monitor for resource "ceph.rook.io/v1, Resource=cephobjectstoreusers": unable to monitor quota for resource "ceph.rook.io/v1, Resource=cephobjectstoreusers", couldn't start monitor for resource "metal3.io/v1alpha1, Resource=firmwareschemas": unable to monitor quota for resource "metal3.io/v1alpha1, Resource=firmwareschemas", couldn't start monitor for resource "ceph.rook.io/v1, Resource=cephrbdmirrors": unable to monitor quota for resource "ceph.rook.io/v1, Resource=cephrbdmirrors", couldn't start monitor for resource "tuned.openshift.io/v1, Resource=profiles": unable to monitor quota for resource "tuned.openshift.io/v1, Resource=profiles", couldn't start monitor for resource "ceph.rook.io/v1, Resource=cephclusters": unable to monitor quota for resource "ceph.rook.io/v1, Resource=cephclusters", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=thanosrulers": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=thanosrulers", couldn't start monitor for resource "ceph.rook.io/v1, Resource=cephfilesystemsubvolumegroups": unable to monitor quota for resource "ceph.rook.io/v1, Resource=cephfilesystemsubvolumegroups", couldn't start monitor for resource "ocs.openshift.io/v1alpha1, Resource=managedocs": unable to monitor quota for resource "ocs.openshift.io/v1alpha1, Resource=managedocs", couldn't start monitor for resource "template.openshift.io/v1, Resource=templates": unable to monitor quota for resource "template.openshift.io/v1, Resource=templates", couldn't start monitor for resource "ceph.rook.io/v1, Resource=cephobjectzones": unable to monitor quota for resource "ceph.rook.io/v1, Resource=cephobjectzones", couldn't start monitor for resource "objectbucket.io/v1alpha1, Resource=objectbucketclaims": unable to monitor quota for resource "objectbucket.io/v1alpha1, Resource=objectbucketclaims", couldn't start monitor for resource "operators.coreos.com/v1alpha1, Resource=subscriptions": unable to monitor quota for resource "operators.coreos.com/v1alpha1, Resource=subscriptions", couldn't start monitor for resource "snapshot.storage.k8s.io/v1, Resource=volumesnapshots": unable to monitor quota for resource "snapshot.storage.k8s.io/v1, Resource=volumesnapshots", couldn't start monitor for resource "helm.openshift.io/v1beta1, Resource=projecthelmchartrepositories": unable to monitor quota for resource "helm.openshift.io/v1beta1, Resource=projecthelmchartrepositories", couldn't start monitor for resource "monitoring.coreos.com/v1alpha1, Resource=alertmanagerconfigs": unable to monitor quota for resource "monitoring.coreos.com/v1alpha1, Resource=alertmanagerconfigs", couldn't start monitor for resource "odf.openshift.io/v1alpha1, Resource=storagesystems": unable to monitor quota for resource "odf.openshift.io/v1alpha1, Resource=storagesystems", couldn't start monitor for resource "velero.io/v1, Resource=downloadrequests": unable to monitor quota for resource "velero.io/v1, Resource=downloadrequests", couldn't start monitor for resource "noobaa.io/v1alpha1, Resource=noobaaaccounts": unable to monitor quota for resource "noobaa.io/v1alpha1, Resource=noobaaaccounts", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=prometheuses": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=prometheuses", couldn't start monitor for resource "tuned.openshift.io/v1, Resource=tuneds": unable to monitor quota for resource "tuned.openshift.io/v1, Resource=tuneds", couldn't start monitor for resource "operator.openshift.io/v1, Resource=ingresscontrollers": unable to monitor quota for resource "operator.openshift.io/v1, Resource=ingresscontrollers", couldn't start monitor for resource "autoscaling.openshift.io/v1beta1, Resource=machineautoscalers": unable to monitor quota for resource "autoscaling.openshift.io/v1beta1, Resource=machineautoscalers", couldn't start monitor for resource "operators.coreos.com/v1alpha1, Resource=installplans": unable to monitor quota for resource "operators.coreos.com/v1alpha1, Resource=installplans", couldn't start monitor for resource "velero.io/v1, Resource=backups": unable to monitor quota for resource "velero.io/v1, Resource=backups", couldn't start monitor for resource "ceph.rook.io/v1, Resource=cephobjectstores": unable to monitor quota for resource "ceph.rook.io/v1, Resource=cephobjectstores", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=prometheusrules": unable to monitor quota for resource "moni2022-05-24T18:44:18.153827751Z toring.coreos.com/v1, Resource=prometheusrules", couldn't start monitor for resource "operators.coreos.com/v2, Resource=operatorconditions": unable to monitor quota for resource "operators.coreos.com/v2, Resource=operatorconditions", couldn't start monitor for resource "ocmagent.managed.openshift.io/v1alpha1, Resource=managednotifications": unable to monitor quota for resource "ocmagent.managed.openshift.io/v1alpha1, Resource=managednotifications", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=podmonitors": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=podmonitors", couldn't start monitor for resource "ocs.openshift.io/v1, Resource=ocsinitializations": unable to monitor quota for resource "ocs.openshift.io/v1, Resource=ocsinitializations", couldn't start monitor for resource "velero.io/v1, Resource=deletebackuprequests": unable to monitor quota for resource "velero.io/v1, Resource=deletebackuprequests", couldn't start monitor for resource "cloudcredential.openshift.io/v1, Resource=credentialsrequests": unable to monitor quota for resource "cloudcredential.openshift.io/v1, Resource=credentialsrequests", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=servicemonitors": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=servicemonitors", couldn't start monitor for resource "operators.coreos.com/v1, Resource=operatorgroups": unable to monitor quota for resource "operators.coreos.com/v1, Resource=operatorgroups", couldn't start monitor for resource "managed.openshift.io/v1alpha2, Resource=veleroinstalls": unable to monitor quota for resource "managed.openshift.io/v1alpha2, Resource=veleroinstalls", couldn't start monitor for resource "machine.openshift.io/v1beta1, Resource=machinesets": unable to monitor quota for resource "machine.openshift.io/v1beta1, Resource=machinesets", couldn't start monitor for resource "ceph.rook.io/v1, Resource=cephbucketnotifications": unable to monitor quota for resource "ceph.rook.io/v1, Resource=cephbucketnotifications", couldn't start monitor for resource "csiaddons.openshift.io/v1alpha1, Resource=csiaddonsnodes": unable to monitor quota for resource "csiaddons.openshift.io/v1alpha1, Resource=csiaddonsnodes", couldn't start monitor for resource "csiaddons.openshift.io/v1alpha1, Resource=reclaimspacejobs": unable to monitor quota for resource "csiaddons.openshift.io/v1alpha1, Resource=reclaimspacejobs", couldn't start monitor for resource "velero.io/v1, Resource=resticrepositories": unable to monitor quota for resource "velero.io/v1, Resource=resticrepositories", couldn't start monitor for resource "addons.managed.openshift.io/v1alpha1, Resource=addoninstances": unable to monitor quota for resource "addons.managed.openshift.io/v1alpha1, Resource=addoninstances", couldn't start monitor for resource "noobaa.io/v1alpha1, Resource=noobaas": unable to monitor quota for resource "noobaa.io/v1alpha1, Resource=noobaas", couldn't start monitor for resource "noobaa.io/v1alpha1, Resource=namespacestores": unable to monitor quota for resource "noobaa.io/v1alpha1, Resource=namespacestores", couldn't start monitor for resource "whereabouts.cni.cncf.io/v1alpha1, Resource=ippools": unable to monitor quota for resource "whereabouts.cni.cncf.io/v1alpha1, Resource=ippools", couldn't start monitor for resource "whereabouts.cni.cncf.io/v1alpha1, Resource=overlappingrangeipreservations": unable to monitor quota for resource "whereabouts.cni.cncf.io/v1alpha1, Resource=overlappingrangeipreservations", couldn't start monitor for resource "metal3.io/v1alpha1, Resource=baremetalhosts": unable to monitor quota for resource "metal3.io/v1alpha1, Resource=baremetalhosts", couldn't start monitor for resource "velero.io/v1, Resource=backupstoragelocations": unable to monitor quota for resource "velero.io/v1, Resource=backupstoragelocations", couldn't start monitor for resource "ceph.rook.io/v1, Resource=cephnfses": unable to monitor quota for resource "ceph.rook.io/v1, Resource=cephnfses", couldn't start monitor for resource "controlplane.operator.openshift.io/v1alpha1, Resource=podnetworkconnectivitychecks": unable to monitor quota for resource "controlplane.operator.openshift.io/v1alpha1, Resource=podnetworkconnectivitychecks", couldn't start monitor for resource "machine.openshift.io/v1beta1, Resource=machines": unable to monitor quota for resource "machine.openshift.io/v1beta1, Resource=machines", couldn't start monitor for resource "ceph.rook.io/v1, Resource=cephclients": unable to monitor quota for resource "ceph.rook.io/v1, Resource=cephclients", couldn't start monitor for resource "ceph.rook.io/v1, Resource=cephfilesystemmirrors": unable to monitor quota for resource "ceph.rook.io/v1, Resource=cephfilesystemmirrors", couldn't start monitor for resource "operators.coreos.com/v1alpha1, Resource=clusterserviceversions": unable to monitor quota for resource "operators.coreos.com/v1alpha1, Resource=clusterserviceversions", couldn't start monitor for resource "metal3.io/v1alpha1, Resource=bmceventsubscriptions": unable to monitor quota for resource "metal3.io/v1alpha1, Resource=bmceventsubscriptions", couldn't start monitor for resource "ocmagent.managed.openshift.io/v1alpha1, Resource=ocmagents": unable to monitor quota for resource "ocmagent.managed.openshift.io/v1alpha1, Resource=ocmagents", couldn't start monitor for resource "replication.storage.openshift.io/v1alpha1, Resource=volumereplications": unable to monitor quota for resource "replication.storage.openshift.io/v1alpha1, Resource=volumereplications", couldn't start monitor for resource "ceph.rook.io/v1, Resource=cephblockpools": unable to monitor quota for resource "ceph.rook.io/v1, Resource=cephblockpools", couldn't start monitor for resource "k8s.cni.cncf.io/v1, Resource=network-attachment-definitions": unable to monitor quota for resource "k8s.cni.cncf.io/v1, Resource=network-attachment-definitions", couldn't start monitor for resource "csiaddons.openshift.io/v1alpha1, Resource=reclaimspacecronjobs": unable to monitor quota for resource "csiaddons.openshift.io/v1alpha1, Resource=reclaimspacecronjobs", couldn't start monitor for resource "template.openshift.io/v1, Resource=templateinstances": unable to monitor quota for resource "template.openshift.io/v1, Resource=templateinstances", couldn't start monitor for resource "monitoring.openshift.io/v1alpha1, Resource=routemonitors": unable to monitor quota for resource "monitoring.openshift.io/v1alpha1, Resource=routemonitors", couldn't start monitor for resource "machine.openshift.io/v1beta1, Resource=machinehealthchecks": unable to monitor quota for resource "machine.openshift.io/v1beta1, Resource=machinehealthchecks", couldn't start monitor for resource "ceph.rook.io/v1, Resource=cephfilesystems": unable to monitor quota for resource "ceph.rook.io/v1, Resource=cephfilesystems", couldn't start monitor for resource "velero.io/v1, Resource=serverstatusrequests": unable to monitor quota for resource "velero.io/v1, Resource=serverstatusrequests", couldn't start monitor for resource "managed.openshift.io/v1alpha1, Resource=subjectpermissions": unable to monitor quota for resource "managed.openshift.io/v1alpha1, Resource=subjectpermissions", couldn't start monitor for resource "upgrade.managed.openshift.io/v1alpha1, Resource=upgradeconfigs": unable to monitor quota for resource "upgrade.managed.openshift.io/v1alpha1, Resource=upgradeconfigs", couldn't start monitor for resource "ocs.openshift.io/v1alpha1, Resource=storageconsumers": unable to monitor quota for resource "ocs.openshift.io/v1alpha1, Resource=storageconsumers", couldn't start monitor for resource "ceph.rook.io/v1, Resource=cephobjectrealms": unable to monitor quota for resource "ceph.rook.io/v1, Resource=cephobjectrealms", couldn't start monitor for resource "ceph.rook.io/v1, Resource=cephobjectzonegroups": unable to monitor quota for resource "ceph.rook.io/v1, Resource=cephobjectzonegroups", couldn't start monitor for resource "network.operator.openshift.io/v1, Resource=egressrouters": unable to monitor quota for resource "network.operator.openshift.io/v1, Resource=egressroute2022-05-24T18:44:18.153851085Z rs", couldn't start monitor for resource "velero.io/v1, Resource=restores": unable to monitor quota for resource "velero.io/v1, Resource=restores"]
2022-05-24T19:11:03.845665400Z I0524 19:11:03.845615       1 event.go:285] Event(v1.ObjectReference{Kind:"Pod", Namespace:"openshift-kube-controller-manager", Name:"kube-controller-manager-ip-10-0-169-205.ec2.internal", UID:"", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'CreatedSCCRanges' created SCC ranges for openshift-build-test-27556991-qmtcb namespace
2022-05-24T20:11:04.064494260Z I0524 20:11:04.064441       1 event.go:285] Event(v1.ObjectReference{Kind:"Pod", Namespace:"openshift-kube-controller-manager", Name:"kube-controller-manager-ip-10-0-169-205.ec2.internal", UID:"", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'CreatedSCCRanges' created SCC ranges for openshift-build-test-27557051-95tsl namespace
2022-05-24T21:11:03.422341867Z I0524 21:11:03.422287       1 event.go:285] Event(v1.ObjectReference{Kind:"Pod", Namespace:"openshift-kube-controller-manager", Name:"kube-controller-manager-ip-10-0-169-205.ec2.internal", UID:"", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'CreatedSCCRanges' created SCC ranges for openshift-build-test-27557111-gmnzb namespace
2022-05-24T22:11:03.552342886Z I0524 22:11:03.552223       1 event.go:285] Event(v1.ObjectReference{Kind:"Pod", Namespace:"openshift-kube-controller-manager", Name:"kube-controller-manager-ip-10-0-169-205.ec2.internal", UID:"", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'CreatedSCCRanges' created SCC ranges for openshift-build-test-27557171-8rq7d namespace
2022-05-24T23:11:03.288947604Z I0524 23:11:03.288899       1 event.go:285] Event(v1.ObjectReference{Kind:"Pod", Namespace:"openshift-kube-controller-manager", Name:"kube-controller-manager-ip-10-0-169-205.ec2.internal", UID:"", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'CreatedSCCRanges' created SCC ranges for openshift-build-test-27557231-k7kpt namespace
