2022-05-24T18:36:37.743907307Z I0524 18:36:37.743795       1 cmd.go:209] Using service-serving-cert provided certificates
2022-05-24T18:36:37.745520401Z I0524 18:36:37.745475       1 observer_polling.go:159] Starting file observer
2022-05-24T18:36:38.793551284Z I0524 18:36:38.793508       1 builder.go:262] kube-apiserver-operator version 4.10.0-202204211158.p0.g78da169.assembly.stream-78da169-78da169541f6a2d13b1e2cdba9fd0564f6fc1e3a
2022-05-24T18:36:39.503146938Z W0524 18:36:39.503100       1 secure_serving.go:69] Use of insecure cipher 'TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256' detected.
2022-05-24T18:36:39.503629760Z W0524 18:36:39.503587       1 secure_serving.go:69] Use of insecure cipher 'TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' detected.
2022-05-24T18:36:39.506992596Z I0524 18:36:39.506967       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
2022-05-24T18:36:39.507027229Z I0524 18:36:39.506999       1 shared_informer.go:240] Waiting for caches to sync for RequestHeaderAuthRequestController
2022-05-24T18:36:39.507090891Z I0524 18:36:39.507043       1 configmap_cafile_content.go:201] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
2022-05-24T18:36:39.507104435Z I0524 18:36:39.507090       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
2022-05-24T18:36:39.507441558Z I0524 18:36:39.507395       1 dynamic_serving_content.go:131] "Starting controller" name="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key"
2022-05-24T18:36:39.507551547Z I0524 18:36:39.507529       1 secure_serving.go:266] Serving securely on [::]:8443
2022-05-24T18:36:39.507551547Z I0524 18:36:39.507542       1 configmap_cafile_content.go:201] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
2022-05-24T18:36:39.507565484Z I0524 18:36:39.507552       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
2022-05-24T18:36:39.507575524Z I0524 18:36:39.507568       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
2022-05-24T18:36:39.607218787Z I0524 18:36:39.607138       1 shared_informer.go:247] Caches are synced for RequestHeaderAuthRequestController 
2022-05-24T18:36:39.607296000Z I0524 18:36:39.607149       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file 
2022-05-24T18:36:39.607751217Z I0524 18:36:39.607730       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file 
2022-05-24T18:36:39.644647058Z I0524 18:36:39.644610       1 leaderelection.go:248] attempting to acquire leader lease openshift-kube-apiserver-operator/kube-apiserver-operator-lock...
2022-05-24T18:36:39.931627237Z I0524 18:36:39.931591       1 leaderelection.go:258] successfully acquired lease openshift-kube-apiserver-operator/kube-apiserver-operator-lock
2022-05-24T18:36:39.931773986Z I0524 18:36:39.931704       1 event.go:285] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator-lock", UID:"47fdac4e-7048-4d42-91e5-d48079d0de25", APIVersion:"v1", ResourceVersion:"63576", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' kube-apiserver-operator-78b5477d97-zcntl_48ea7137-5c16-4dfa-989c-e1fc036609ec became leader
2022-05-24T18:36:39.931792233Z I0524 18:36:39.931768       1 event.go:285] Event(v1.ObjectReference{Kind:"Lease", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator-lock", UID:"2b57a708-83f0-47a1-ba11-5b9e77e6ed3c", APIVersion:"coordination.k8s.io/v1", ResourceVersion:"63594", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' kube-apiserver-operator-78b5477d97-zcntl_48ea7137-5c16-4dfa-989c-e1fc036609ec became leader
2022-05-24T18:36:39.933916705Z I0524 18:36:39.933880       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "EventWatchController" resync interval is set to 0s which might lead to client request throttling
2022-05-24T18:36:39.968322898Z E0524 18:36:39.968290       1 static_resource_controller.go:193] cannot decode "assets/kube-apiserver/apiserver.openshift.io_apirequestcount.yaml": no kind "CustomResourceDefinition" is registered for version "apiextensions.k8s.io/v1" in scheme "pkg/runtime/scheme.go:100"
2022-05-24T18:36:39.968820174Z E0524 18:36:39.968800       1 static_resource_controller.go:193] cannot decode "assets/alerts/api-usage.yaml": no kind "PrometheusRule" is registered for version "monitoring.coreos.com/v1" in scheme "pkg/runtime/scheme.go:100"
2022-05-24T18:36:39.970053252Z E0524 18:36:39.970026       1 static_resource_controller.go:193] cannot decode "assets/alerts/audit-errors.yaml": no kind "PrometheusRule" is registered for version "monitoring.coreos.com/v1" in scheme "pkg/runtime/scheme.go:100"
2022-05-24T18:36:39.971553780Z E0524 18:36:39.971531       1 static_resource_controller.go:193] cannot decode "assets/alerts/cpu-utilization.yaml": no kind "PrometheusRule" is registered for version "monitoring.coreos.com/v1" in scheme "pkg/runtime/scheme.go:100"
2022-05-24T18:36:39.972737740Z E0524 18:36:39.972721       1 static_resource_controller.go:193] cannot decode "assets/alerts/kube-apiserver-requests.yaml": no kind "PrometheusRule" is registered for version "monitoring.coreos.com/v1" in scheme "pkg/runtime/scheme.go:100"
2022-05-24T18:36:39.975695316Z E0524 18:36:39.975668       1 static_resource_controller.go:193] cannot decode "assets/alerts/kube-apiserver-slos-basic.yaml": no kind "PrometheusRule" is registered for version "monitoring.coreos.com/v1" in scheme "pkg/runtime/scheme.go:100"
2022-05-24T18:36:39.977643047Z E0524 18:36:39.977605       1 static_resource_controller.go:193] cannot decode "assets/alerts/kube-apiserver-slos-extended.yaml": no kind "PrometheusRule" is registered for version "monitoring.coreos.com/v1" in scheme "pkg/runtime/scheme.go:100"
2022-05-24T18:36:39.978613072Z I0524 18:36:39.978561       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "ConnectivityCheckController" resync interval is set to 0s which might lead to client request throttling
2022-05-24T18:36:40.062344705Z I0524 18:36:40.062293       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "RevisionController" resync interval is set to 0s which might lead to client request throttling
2022-05-24T18:36:40.062886228Z I0524 18:36:40.062832       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "PruneController" resync interval is set to 0s which might lead to client request throttling
2022-05-24T18:36:40.063012312Z I0524 18:36:40.062987       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "NodeController" resync interval is set to 0s which might lead to client request throttling
2022-05-24T18:36:40.063759933Z I0524 18:36:40.063718       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "UnsupportedConfigOverridesController" resync interval is set to 0s which might lead to client request throttling
2022-05-24T18:36:40.063945075Z I0524 18:36:40.063922       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "GuardController" resync interval is set to 0s which might lead to client request throttling
2022-05-24T18:36:40.063962678Z I0524 18:36:40.063943       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "LoggingSyncer" resync interval is set to 0s which might lead to client request throttling
2022-05-24T18:36:40.121316676Z I0524 18:36:40.121258       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "FeatureUpgradeableController" resync interval is set to 0s which might lead to client request throttling
2022-05-24T18:36:40.122913563Z I0524 18:36:40.122877       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "auditPolicyController" resync interval is set to 10s which might lead to client request throttling
2022-05-24T18:36:40.123078169Z I0524 18:36:40.123044       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "KubeletVersionSkewController" resync interval is set to 0s which might lead to client request throttling
2022-05-24T18:36:40.123242292Z I0524 18:36:40.123198       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "webhookSupportabilityController" resync interval is set to 0s which might lead to client request throttling
2022-05-24T18:36:40.123303095Z I0524 18:36:40.123278       1 base_controller.go:67] Waiting for caches to sync for webhookSupportabilityController
2022-05-24T18:36:40.124022677Z I0524 18:36:40.123994       1 base_controller.go:67] Waiting for caches to sync for KubeletVersionSkewController
2022-05-24T18:36:40.124471810Z I0524 18:36:40.124449       1 base_controller.go:67] Waiting for caches to sync for CertRotationTimeUpgradeableController
2022-05-24T18:36:40.124493428Z I0524 18:36:40.124480       1 termination_observer.go:145] Starting TerminationObserver
2022-05-24T18:36:40.124504441Z I0524 18:36:40.124496       1 base_controller.go:67] Waiting for caches to sync for EventWatchController
2022-05-24T18:36:40.125041133Z I0524 18:36:40.125002       1 base_controller.go:67] Waiting for caches to sync for FeatureUpgradeableController
2022-05-24T18:36:40.125450071Z I0524 18:36:40.125421       1 base_controller.go:67] Waiting for caches to sync for MissingStaticPodController
2022-05-24T18:36:40.125498264Z I0524 18:36:40.125474       1 base_controller.go:67] Waiting for caches to sync for ResourceSyncController
2022-05-24T18:36:40.126159250Z I0524 18:36:40.126139       1 base_controller.go:67] Waiting for caches to sync for StaticResourceController
2022-05-24T18:36:40.126191170Z I0524 18:36:40.126161       1 base_controller.go:67] Waiting for caches to sync for TargetConfigController
2022-05-24T18:36:40.126382709Z I0524 18:36:40.126361       1 base_controller.go:67] Waiting for caches to sync for NodeKubeconfigController
2022-05-24T18:36:40.126402190Z I0524 18:36:40.126387       1 base_controller.go:67] Waiting for caches to sync for ConfigObserver
2022-05-24T18:36:40.126707943Z I0524 18:36:40.126686       1 base_controller.go:67] Waiting for caches to sync for BoundSATokenSignerController
2022-05-24T18:36:40.126761052Z I0524 18:36:40.126747       1 base_controller.go:67] Waiting for caches to sync for auditPolicyController
2022-05-24T18:36:40.126831752Z I0524 18:36:40.126816       1 base_controller.go:67] Waiting for caches to sync for RemoveStaleConditionsController
2022-05-24T18:36:40.126845374Z I0524 18:36:40.126839       1 base_controller.go:67] Waiting for caches to sync for ConnectivityCheckController
2022-05-24T18:36:40.127549536Z I0524 18:36:40.127524       1 certrotationcontroller.go:654] Starting CertRotation
2022-05-24T18:36:40.127592521Z I0524 18:36:40.127580       1 certrotationcontroller.go:619] Waiting for CertRotation
2022-05-24T18:36:40.127648440Z I0524 18:36:40.127636       1 base_controller.go:67] Waiting for caches to sync for EncryptionConditionController
2022-05-24T18:36:40.127705004Z I0524 18:36:40.127693       1 base_controller.go:67] Waiting for caches to sync for RevisionController
2022-05-24T18:36:40.128318935Z I0524 18:36:40.128297       1 base_controller.go:67] Waiting for caches to sync for InstallerController
2022-05-24T18:36:40.128398521Z I0524 18:36:40.128373       1 base_controller.go:67] Waiting for caches to sync for EncryptionStateController
2022-05-24T18:36:40.128419205Z I0524 18:36:40.128402       1 base_controller.go:67] Waiting for caches to sync for EncryptionPruneController
2022-05-24T18:36:40.128430719Z I0524 18:36:40.128422       1 base_controller.go:67] Waiting for caches to sync for EncryptionMigrationController
2022-05-24T18:36:40.128587772Z I0524 18:36:40.128570       1 base_controller.go:67] Waiting for caches to sync for UnsupportedConfigOverridesController
2022-05-24T18:36:40.128603645Z I0524 18:36:40.128583       1 base_controller.go:67] Waiting for caches to sync for EncryptionKeyController
2022-05-24T18:36:40.128603645Z I0524 18:36:40.128592       1 base_controller.go:67] Waiting for caches to sync for InstallerStateController
2022-05-24T18:36:40.128715124Z I0524 18:36:40.128692       1 base_controller.go:67] Waiting for caches to sync for StaticPodStateController
2022-05-24T18:36:40.128730210Z I0524 18:36:40.128723       1 base_controller.go:67] Waiting for caches to sync for PruneController
2022-05-24T18:36:40.128749030Z I0524 18:36:40.128303       1 base_controller.go:67] Waiting for caches to sync for StatusSyncer_kube-apiserver
2022-05-24T18:36:40.128792833Z I0524 18:36:40.128768       1 base_controller.go:67] Waiting for caches to sync for StartupMonitorPodCondition
2022-05-24T18:36:40.128857069Z I0524 18:36:40.128819       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "InstallerController" resync interval is set to 0s which might lead to client request throttling
2022-05-24T18:36:40.128857069Z I0524 18:36:40.128836       1 base_controller.go:67] Waiting for caches to sync for StaticPodStateFallback
2022-05-24T18:36:40.128873963Z I0524 18:36:40.128863       1 base_controller.go:67] Waiting for caches to sync for LoggingSyncer
2022-05-24T18:36:40.129045085Z I0524 18:36:40.129005       1 base_controller.go:67] Waiting for caches to sync for GuardController
2022-05-24T18:36:40.129143431Z I0524 18:36:40.129117       1 base_controller.go:67] Waiting for caches to sync for StaticResourceController
2022-05-24T18:36:40.129250737Z I0524 18:36:40.129026       1 base_controller.go:67] Waiting for caches to sync for NodeController
2022-05-24T18:36:40.224129264Z I0524 18:36:40.224092       1 base_controller.go:73] Caches are synced for KubeletVersionSkewController 
2022-05-24T18:36:40.224246361Z I0524 18:36:40.224228       1 base_controller.go:110] Starting #1 worker of KubeletVersionSkewController controller ...
2022-05-24T18:36:40.224705886Z E0524 18:36:40.224357       1 base_controller.go:272] KubeletVersionSkewController reconciliation failed: kubeapiservers.operator.openshift.io "cluster" not found
2022-05-24T18:36:40.227774074Z I0524 18:36:40.227746       1 certrotationcontroller.go:637] Finished waiting for CertRotation
2022-05-24T18:36:40.227810042Z I0524 18:36:40.227791       1 base_controller.go:67] Waiting for caches to sync for CertRotationController
2022-05-24T18:36:40.227889435Z I0524 18:36:40.227872       1 base_controller.go:67] Waiting for caches to sync for CertRotationController
2022-05-24T18:36:40.227989079Z I0524 18:36:40.227929       1 base_controller.go:67] Waiting for caches to sync for CertRotationController
2022-05-24T18:36:40.227989079Z I0524 18:36:40.227951       1 base_controller.go:67] Waiting for caches to sync for CertRotationController
2022-05-24T18:36:40.227989079Z I0524 18:36:40.227967       1 base_controller.go:67] Waiting for caches to sync for CertRotationController
2022-05-24T18:36:40.227989079Z I0524 18:36:40.227983       1 base_controller.go:67] Waiting for caches to sync for CertRotationController
2022-05-24T18:36:40.228007133Z I0524 18:36:40.227999       1 base_controller.go:67] Waiting for caches to sync for CertRotationController
2022-05-24T18:36:40.228020075Z I0524 18:36:40.228014       1 base_controller.go:67] Waiting for caches to sync for CertRotationController
2022-05-24T18:36:40.228074063Z I0524 18:36:40.228029       1 base_controller.go:67] Waiting for caches to sync for CertRotationController
2022-05-24T18:36:40.228074063Z I0524 18:36:40.228047       1 base_controller.go:67] Waiting for caches to sync for CertRotationController
2022-05-24T18:36:40.228074063Z I0524 18:36:40.228063       1 base_controller.go:67] Waiting for caches to sync for CertRotationController
2022-05-24T18:36:40.228085846Z I0524 18:36:40.228078       1 base_controller.go:67] Waiting for caches to sync for CertRotationController
2022-05-24T18:36:40.229904224Z E0524 18:36:40.229879       1 base_controller.go:272] KubeletVersionSkewController reconciliation failed: kubeapiservers.operator.openshift.io "cluster" not found
2022-05-24T18:36:40.240197984Z E0524 18:36:40.240154       1 base_controller.go:272] KubeletVersionSkewController reconciliation failed: kubeapiservers.operator.openshift.io "cluster" not found
2022-05-24T18:36:40.260499564Z E0524 18:36:40.260470       1 base_controller.go:272] KubeletVersionSkewController reconciliation failed: kubeapiservers.operator.openshift.io "cluster" not found
2022-05-24T18:36:40.301414627Z E0524 18:36:40.301134       1 base_controller.go:272] KubeletVersionSkewController reconciliation failed: kubeapiservers.operator.openshift.io "cluster" not found
2022-05-24T18:36:40.325275473Z I0524 18:36:40.325219       1 base_controller.go:73] Caches are synced for EventWatchController 
2022-05-24T18:36:40.325275473Z I0524 18:36:40.325235       1 base_controller.go:73] Caches are synced for FeatureUpgradeableController 
2022-05-24T18:36:40.325275473Z I0524 18:36:40.325258       1 base_controller.go:110] Starting #1 worker of FeatureUpgradeableController controller ...
2022-05-24T18:36:40.325359144Z I0524 18:36:40.325221       1 base_controller.go:73] Caches are synced for CertRotationTimeUpgradeableController 
2022-05-24T18:36:40.325359144Z I0524 18:36:40.325334       1 base_controller.go:110] Starting #1 worker of CertRotationTimeUpgradeableController controller ...
2022-05-24T18:36:40.325382741Z I0524 18:36:40.325248       1 base_controller.go:110] Starting #1 worker of EventWatchController controller ...
2022-05-24T18:36:40.327507798Z I0524 18:36:40.327479       1 base_controller.go:73] Caches are synced for RemoveStaleConditionsController 
2022-05-24T18:36:40.327507798Z I0524 18:36:40.327497       1 base_controller.go:110] Starting #1 worker of RemoveStaleConditionsController controller ...
2022-05-24T18:36:40.328619194Z I0524 18:36:40.328594       1 base_controller.go:73] Caches are synced for UnsupportedConfigOverridesController 
2022-05-24T18:36:40.328619194Z I0524 18:36:40.328611       1 base_controller.go:110] Starting #1 worker of UnsupportedConfigOverridesController controller ...
2022-05-24T18:36:40.329558628Z I0524 18:36:40.329534       1 base_controller.go:73] Caches are synced for PruneController 
2022-05-24T18:36:40.329558628Z I0524 18:36:40.329550       1 base_controller.go:110] Starting #1 worker of PruneController controller ...
2022-05-24T18:36:40.330922713Z E0524 18:36:40.330900       1 base_controller.go:272] PruneController reconciliation failed: unable to set pruner pod ownerrefs: configmap "revision-status-10" not found
2022-05-24T18:36:40.330970374Z I0524 18:36:40.330956       1 base_controller.go:73] Caches are synced for LoggingSyncer 
2022-05-24T18:36:40.330979584Z I0524 18:36:40.330969       1 base_controller.go:110] Starting #1 worker of LoggingSyncer controller ...
2022-05-24T18:36:40.331479503Z I0524 18:36:40.331461       1 base_controller.go:73] Caches are synced for NodeController 
2022-05-24T18:36:40.331479503Z I0524 18:36:40.331475       1 base_controller.go:110] Starting #1 worker of NodeController controller ...
2022-05-24T18:36:40.336903568Z E0524 18:36:40.336877       1 base_controller.go:272] PruneController reconciliation failed: unable to set pruner pod ownerrefs: configmap "revision-status-10" not found
2022-05-24T18:36:40.348625339Z E0524 18:36:40.348598       1 base_controller.go:272] PruneController reconciliation failed: unable to set pruner pod ownerrefs: configmap "revision-status-10" not found
2022-05-24T18:36:40.369530685Z E0524 18:36:40.369493       1 base_controller.go:272] PruneController reconciliation failed: unable to set pruner pod ownerrefs: configmap "revision-status-10" not found
2022-05-24T18:36:40.410862038Z E0524 18:36:40.410829       1 base_controller.go:272] PruneController reconciliation failed: unable to set pruner pod ownerrefs: configmap "revision-status-10" not found
2022-05-24T18:36:40.491941061Z E0524 18:36:40.491907       1 base_controller.go:272] PruneController reconciliation failed: unable to set pruner pod ownerrefs: configmap "revision-status-10" not found
2022-05-24T18:36:40.530289887Z I0524 18:36:40.530242       1 base_controller.go:73] Caches are synced for StaticResourceController 
2022-05-24T18:36:40.530289887Z I0524 18:36:40.530262       1 base_controller.go:110] Starting #1 worker of StaticResourceController controller ...
2022-05-24T18:36:40.629302612Z I0524 18:36:40.629261       1 base_controller.go:73] Caches are synced for StatusSyncer_kube-apiserver 
2022-05-24T18:36:40.629302612Z I0524 18:36:40.629284       1 base_controller.go:110] Starting #1 worker of StatusSyncer_kube-apiserver controller ...
2022-05-24T18:36:40.653151695Z E0524 18:36:40.653115       1 base_controller.go:272] PruneController reconciliation failed: unable to set pruner pod ownerrefs: configmap "revision-status-10" not found
2022-05-24T18:36:40.928983481Z I0524 18:36:40.928946       1 base_controller.go:73] Caches are synced for StaticPodStateFallback 
2022-05-24T18:36:40.928983481Z I0524 18:36:40.928968       1 base_controller.go:110] Starting #1 worker of StaticPodStateFallback controller ...
2022-05-24T18:36:40.929015869Z I0524 18:36:40.928986       1 base_controller.go:73] Caches are synced for InstallerController 
2022-05-24T18:36:40.929015869Z I0524 18:36:40.928993       1 base_controller.go:73] Caches are synced for InstallerStateController 
2022-05-24T18:36:40.929015869Z I0524 18:36:40.929000       1 base_controller.go:73] Caches are synced for StaticPodStateController 
2022-05-24T18:36:40.929015869Z I0524 18:36:40.929005       1 base_controller.go:110] Starting #1 worker of InstallerStateController controller ...
2022-05-24T18:36:40.929015869Z I0524 18:36:40.929011       1 base_controller.go:110] Starting #1 worker of StaticPodStateController controller ...
2022-05-24T18:36:40.929052761Z I0524 18:36:40.929002       1 base_controller.go:110] Starting #1 worker of InstallerController controller ...
2022-05-24T18:36:40.929404779Z I0524 18:36:40.929382       1 base_controller.go:73] Caches are synced for GuardController 
2022-05-24T18:36:40.929404779Z I0524 18:36:40.929395       1 base_controller.go:110] Starting #1 worker of GuardController controller ...
2022-05-24T18:36:40.929656423Z I0524 18:36:40.929626       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: aggregator-client-ca,check-endpoints-kubeconfig,client-ca,control-plane-node-kubeconfig, secrets: aggregator-client,bound-service-account-signing-key,check-endpoints-client-cert-key,control-plane-node-admin-client-cert-key,external-loadbalancer-serving-certkey,internal-loadbalancer-serving-certkey,kubelet-client,localhost-serving-cert-certkey,node-kubeconfigs,service-network-serving-certkey, configmaps: bound-sa-token-signing-certs-10,config-10,etcd-serving-ca-10,kube-apiserver-audit-policies-10,kube-apiserver-cert-syncer-kubeconfig-10,kube-apiserver-pod-10,kubelet-serving-ca-10,sa-token-signing-certs-10, secrets: etcd-client-10,localhost-recovery-client-token-10,localhost-recovery-serving-certkey-10
2022-05-24T18:36:40.930655019Z I0524 18:36:40.930622       1 base_controller.go:73] Caches are synced for StartupMonitorPodCondition 
2022-05-24T18:36:40.930655019Z I0524 18:36:40.930634       1 base_controller.go:110] Starting #1 worker of StartupMonitorPodCondition controller ...
2022-05-24T18:36:40.973891928Z E0524 18:36:40.973863       1 base_controller.go:272] PruneController reconciliation failed: unable to set pruner pod ownerrefs: configmap "revision-status-10" not found
2022-05-24T18:36:41.015413202Z I0524 18:36:41.015380       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:36:41.030894705Z E0524 18:36:41.030854       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: [configmaps: aggregator-client-ca,check-endpoints-kubeconfig,client-ca,control-plane-node-kubeconfig, secrets: aggregator-client,bound-service-account-signing-key,check-endpoints-client-cert-key,control-plane-node-admin-client-cert-key,external-loadbalancer-serving-certkey,internal-loadbalancer-serving-certkey,kubelet-client,localhost-serving-cert-certkey,node-kubeconfigs,service-network-serving-certkey, configmaps: bound-sa-token-signing-certs-10,config-10,etcd-serving-ca-10,kube-apiserver-audit-policies-10,kube-apiserver-cert-syncer-kubeconfig-10,kube-apiserver-pod-10,kubelet-serving-ca-10,sa-token-signing-certs-10, secrets: etcd-client-10,localhost-recovery-client-token-10,localhost-recovery-serving-certkey-10]
2022-05-24T18:36:41.038890997Z I0524 18:36:41.038851       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: aggregator-client-ca,check-endpoints-kubeconfig,client-ca,control-plane-node-kubeconfig, secrets: aggregator-client,bound-service-account-signing-key,check-endpoints-client-cert-key,control-plane-node-admin-client-cert-key,external-loadbalancer-serving-certkey,internal-loadbalancer-serving-certkey,kubelet-client,localhost-serving-cert-certkey,node-kubeconfigs,service-network-serving-certkey, configmaps: bound-sa-token-signing-certs-10,config-10,etcd-serving-ca-10,kube-apiserver-audit-policies-10,kube-apiserver-cert-syncer-kubeconfig-10,kube-apiserver-pod-10,kubelet-serving-ca-10,sa-token-signing-certs-10, secrets: etcd-client-10,localhost-recovery-client-token-10,localhost-recovery-serving-certkey-10
2022-05-24T18:36:41.127875514Z I0524 18:36:41.127835       1 base_controller.go:73] Caches are synced for ConnectivityCheckController 
2022-05-24T18:36:41.127875514Z I0524 18:36:41.127858       1 base_controller.go:110] Starting #1 worker of ConnectivityCheckController controller ...
2022-05-24T18:36:41.264622796Z I0524 18:36:41.264582       1 request.go:665] Waited for 1.140524716s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps?limit=500&resourceVersion=0
2022-05-24T18:36:41.326432599Z I0524 18:36:41.326385       1 base_controller.go:73] Caches are synced for MissingStaticPodController 
2022-05-24T18:36:41.326432599Z I0524 18:36:41.326412       1 base_controller.go:110] Starting #1 worker of MissingStaticPodController controller ...
2022-05-24T18:36:41.327016200Z I0524 18:36:41.326987       1 base_controller.go:73] Caches are synced for auditPolicyController 
2022-05-24T18:36:41.327016200Z I0524 18:36:41.327003       1 base_controller.go:110] Starting #1 worker of auditPolicyController controller ...
2022-05-24T18:36:41.510755155Z E0524 18:36:41.510714       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: [configmaps: aggregator-client-ca,check-endpoints-kubeconfig,client-ca,control-plane-node-kubeconfig, secrets: aggregator-client,bound-service-account-signing-key,check-endpoints-client-cert-key,control-plane-node-admin-client-cert-key,external-loadbalancer-serving-certkey,internal-loadbalancer-serving-certkey,kubelet-client,localhost-serving-cert-certkey,node-kubeconfigs,service-network-serving-certkey, configmaps: bound-sa-token-signing-certs-10,config-10,etcd-serving-ca-10,kube-apiserver-audit-policies-10,kube-apiserver-cert-syncer-kubeconfig-10,kube-apiserver-pod-10,kubelet-serving-ca-10,sa-token-signing-certs-10, secrets: etcd-client-10,localhost-recovery-client-token-10,localhost-recovery-serving-certkey-10]
2022-05-24T18:36:41.527286351Z I0524 18:36:41.527255       1 base_controller.go:73] Caches are synced for StaticResourceController 
2022-05-24T18:36:41.527286351Z I0524 18:36:41.527274       1 base_controller.go:110] Starting #1 worker of StaticResourceController controller ...
2022-05-24T18:36:41.527786571Z I0524 18:36:41.527757       1 base_controller.go:73] Caches are synced for EncryptionConditionController 
2022-05-24T18:36:41.527786571Z I0524 18:36:41.527773       1 base_controller.go:110] Starting #1 worker of EncryptionConditionController controller ...
2022-05-24T18:36:41.528615122Z I0524 18:36:41.528591       1 base_controller.go:73] Caches are synced for EncryptionMigrationController 
2022-05-24T18:36:41.528696575Z I0524 18:36:41.528671       1 base_controller.go:110] Starting #1 worker of EncryptionMigrationController controller ...
2022-05-24T18:36:41.528714600Z I0524 18:36:41.528615       1 base_controller.go:73] Caches are synced for EncryptionStateController 
2022-05-24T18:36:41.528771319Z I0524 18:36:41.528725       1 base_controller.go:110] Starting #1 worker of EncryptionStateController controller ...
2022-05-24T18:36:41.528837358Z I0524 18:36:41.528636       1 base_controller.go:73] Caches are synced for EncryptionPruneController 
2022-05-24T18:36:41.528883566Z I0524 18:36:41.528870       1 base_controller.go:110] Starting #1 worker of EncryptionPruneController controller ...
2022-05-24T18:36:41.528934076Z I0524 18:36:41.528653       1 base_controller.go:73] Caches are synced for EncryptionKeyController 
2022-05-24T18:36:41.528965195Z I0524 18:36:41.528955       1 base_controller.go:110] Starting #1 worker of EncryptionKeyController controller ...
2022-05-24T18:36:41.529895844Z I0524 18:36:41.529873       1 base_controller.go:73] Caches are synced for RevisionController 
2022-05-24T18:36:41.529937304Z I0524 18:36:41.529926       1 base_controller.go:110] Starting #1 worker of RevisionController controller ...
2022-05-24T18:36:41.658432101Z I0524 18:36:41.658389       1 status_controller.go:211] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:35:50Z","message":"NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [configmaps: aggregator-client-ca,check-endpoints-kubeconfig,client-ca,control-plane-node-kubeconfig, secrets: aggregator-client,bound-service-account-signing-key,check-endpoints-client-cert-key,control-plane-node-admin-client-cert-key,external-loadbalancer-serving-certkey,internal-loadbalancer-serving-certkey,kubelet-client,localhost-serving-cert-certkey,node-kubeconfigs,service-network-serving-certkey, configmaps: bound-sa-token-signing-certs-10,config-10,etcd-serving-ca-10,kube-apiserver-audit-policies-10,kube-apiserver-cert-syncer-kubeconfig-10,kube-apiserver-pod-10,kubelet-serving-ca-10,sa-token-signing-certs-10, secrets: etcd-client-10,localhost-recovery-client-token-10,localhost-recovery-serving-certkey-10]","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:26:37Z","message":"NodeInstallerProgressing: 2 nodes are at revision 6; 1 nodes are at revision 10","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:11:43Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 10","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:42Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2022-05-24T18:36:41.710120964Z I0524 18:36:41.710053       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-apiserver changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready" to "NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [configmaps: aggregator-client-ca,check-endpoints-kubeconfig,client-ca,control-plane-node-kubeconfig, secrets: aggregator-client,bound-service-account-signing-key,check-endpoints-client-cert-key,control-plane-node-admin-client-cert-key,external-loadbalancer-serving-certkey,internal-loadbalancer-serving-certkey,kubelet-client,localhost-serving-cert-certkey,node-kubeconfigs,service-network-serving-certkey, configmaps: bound-sa-token-signing-certs-10,config-10,etcd-serving-ca-10,kube-apiserver-audit-policies-10,kube-apiserver-cert-syncer-kubeconfig-10,kube-apiserver-pod-10,kubelet-serving-ca-10,sa-token-signing-certs-10, secrets: etcd-client-10,localhost-recovery-client-token-10,localhost-recovery-serving-certkey-10]"
2022-05-24T18:36:42.127417220Z I0524 18:36:42.127375       1 base_controller.go:73] Caches are synced for BoundSATokenSignerController 
2022-05-24T18:36:42.127417220Z I0524 18:36:42.127398       1 base_controller.go:110] Starting #1 worker of BoundSATokenSignerController controller ...
2022-05-24T18:36:42.127460162Z I0524 18:36:42.127376       1 base_controller.go:73] Caches are synced for NodeKubeconfigController 
2022-05-24T18:36:42.127460162Z I0524 18:36:42.127428       1 base_controller.go:110] Starting #1 worker of NodeKubeconfigController controller ...
2022-05-24T18:36:42.127996434Z I0524 18:36:42.127976       1 base_controller.go:73] Caches are synced for CertRotationController 
2022-05-24T18:36:42.128007877Z I0524 18:36:42.127993       1 base_controller.go:110] Starting #1 worker of CertRotationController controller ...
2022-05-24T18:36:42.128007877Z I0524 18:36:42.127998       1 base_controller.go:73] Caches are synced for CertRotationController 
2022-05-24T18:36:42.128018378Z I0524 18:36:42.127983       1 base_controller.go:73] Caches are synced for CertRotationController 
2022-05-24T18:36:42.128018378Z I0524 18:36:42.128008       1 base_controller.go:110] Starting #1 worker of CertRotationController controller ...
2022-05-24T18:36:42.128018378Z I0524 18:36:42.128013       1 base_controller.go:110] Starting #1 worker of CertRotationController controller ...
2022-05-24T18:36:42.128032329Z I0524 18:36:42.128025       1 base_controller.go:73] Caches are synced for CertRotationController 
2022-05-24T18:36:42.128042174Z I0524 18:36:42.128031       1 base_controller.go:110] Starting #1 worker of CertRotationController controller ...
2022-05-24T18:36:42.128064501Z I0524 18:36:42.128050       1 base_controller.go:73] Caches are synced for CertRotationController 
2022-05-24T18:36:42.128064501Z I0524 18:36:42.128059       1 base_controller.go:110] Starting #1 worker of CertRotationController controller ...
2022-05-24T18:36:42.128104321Z I0524 18:36:42.128076       1 base_controller.go:73] Caches are synced for CertRotationController 
2022-05-24T18:36:42.128104321Z I0524 18:36:42.128088       1 base_controller.go:110] Starting #1 worker of CertRotationController controller ...
2022-05-24T18:36:42.128142732Z I0524 18:36:42.128105       1 base_controller.go:73] Caches are synced for CertRotationController 
2022-05-24T18:36:42.128142732Z I0524 18:36:42.128112       1 base_controller.go:110] Starting #1 worker of CertRotationController controller ...
2022-05-24T18:36:42.128879218Z I0524 18:36:42.128853       1 base_controller.go:73] Caches are synced for CertRotationController 
2022-05-24T18:36:42.128927964Z I0524 18:36:42.128907       1 base_controller.go:110] Starting #1 worker of CertRotationController controller ...
2022-05-24T18:36:42.130151793Z I0524 18:36:42.127999       1 base_controller.go:73] Caches are synced for CertRotationController 
2022-05-24T18:36:42.130217514Z I0524 18:36:42.130203       1 base_controller.go:110] Starting #1 worker of CertRotationController controller ...
2022-05-24T18:36:42.130267512Z I0524 18:36:42.130243       1 base_controller.go:73] Caches are synced for CertRotationController 
2022-05-24T18:36:42.132081577Z I0524 18:36:42.132056       1 base_controller.go:110] Starting #1 worker of CertRotationController controller ...
2022-05-24T18:36:42.132105413Z I0524 18:36:42.131147       1 base_controller.go:73] Caches are synced for CertRotationController 
2022-05-24T18:36:42.132105413Z I0524 18:36:42.132100       1 base_controller.go:110] Starting #1 worker of CertRotationController controller ...
2022-05-24T18:36:42.264798060Z I0524 18:36:42.264766       1 request.go:665] Waited for 2.13825502s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-config-managed/configmaps?limit=500&resourceVersion=0
2022-05-24T18:36:42.326935792Z I0524 18:36:42.326894       1 base_controller.go:73] Caches are synced for TargetConfigController 
2022-05-24T18:36:42.326935792Z I0524 18:36:42.326918       1 base_controller.go:110] Starting #1 worker of TargetConfigController controller ...
2022-05-24T18:36:42.326980123Z I0524 18:36:42.326894       1 base_controller.go:73] Caches are synced for ConfigObserver 
2022-05-24T18:36:42.326980123Z I0524 18:36:42.326955       1 base_controller.go:110] Starting #1 worker of ConfigObserver controller ...
2022-05-24T18:36:42.328070239Z I0524 18:36:42.328034       1 base_controller.go:73] Caches are synced for CertRotationController 
2022-05-24T18:36:42.328070239Z I0524 18:36:42.328053       1 base_controller.go:110] Starting #1 worker of CertRotationController controller ...
2022-05-24T18:36:42.723791784Z I0524 18:36:42.723754       1 base_controller.go:73] Caches are synced for webhookSupportabilityController 
2022-05-24T18:36:42.723791784Z I0524 18:36:42.723776       1 base_controller.go:110] Starting #1 worker of webhookSupportabilityController controller ...
2022-05-24T18:36:42.925642366Z I0524 18:36:42.925595       1 base_controller.go:73] Caches are synced for ResourceSyncController 
2022-05-24T18:36:42.925642366Z I0524 18:36:42.925618       1 base_controller.go:110] Starting #1 worker of ResourceSyncController controller ...
2022-05-24T18:36:43.465817567Z I0524 18:36:43.465403       1 request.go:665] Waited for 2.536271249s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller
2022-05-24T18:36:44.664470841Z I0524 18:36:44.664411       1 request.go:665] Waited for 3.048959009s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/revision-pruner-10-ip-10-0-149-121.ec2.internal
2022-05-24T18:36:45.665391885Z I0524 18:36:45.665355       1 request.go:665] Waited for 2.010209431s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dopenshift-kube-apiserver
2022-05-24T18:36:46.864102726Z I0524 18:36:46.864068       1 request.go:665] Waited for 2.194430382s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods
2022-05-24T18:36:46.875768016Z I0524 18:36:46.875733       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-10-ip-10-0-149-121.ec2.internal -n openshift-kube-apiserver because it was missing
2022-05-24T18:36:47.864686567Z I0524 18:36:47.864648       1 request.go:665] Waited for 1.797670186s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ip-10-0-138-197.ec2.internal
2022-05-24T18:36:49.064768471Z I0524 18:36:49.064732       1 request.go:665] Waited for 1.996669177s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/client-ca
2022-05-24T18:36:49.870225264Z I0524 18:36:49.870169       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:36:49.917564029Z I0524 18:36:49.917480       1 termination_observer.go:236] Observed event "TerminationPreShutdownHooksFinished" for API server pod "kube-apiserver-ip-10-0-138-197.ec2.internal" (last termination at 2022-05-24 18:17:00 +0000 UTC) at 0001-01-01 00:00:00 +0000 UTC
2022-05-24T18:36:50.072224244Z I0524 18:36:50.071955       1 request.go:665] Waited for 1.603292424s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/installer-10-ip-10-0-138-197.ec2.internal
2022-05-24T18:36:51.264302945Z I0524 18:36:51.264253       1 request.go:665] Waited for 1.597116609s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-138-197.ec2.internal
2022-05-24T18:36:51.342602936Z E0524 18:36:51.342557       1 degraded_webhook.go:128] x509: certificate signed by unknown authority
2022-05-24T18:36:51.667310780Z I0524 18:36:51.667274       1 installer_controller.go:512] "ip-10-0-138-197.ec2.internal" is in transition to 10, but has not made progress because installer is not finished, but in Running phase
2022-05-24T18:36:51.683633110Z I0524 18:36:51.683594       1 status_controller.go:211] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:35:50Z","message":"NodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:26:37Z","message":"NodeInstallerProgressing: 2 nodes are at revision 6; 1 nodes are at revision 10","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:11:43Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 10","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:42Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2022-05-24T18:36:51.693830792Z I0524 18:36:51.693785       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-apiserver changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [configmaps: aggregator-client-ca,check-endpoints-kubeconfig,client-ca,control-plane-node-kubeconfig, secrets: aggregator-client,bound-service-account-signing-key,check-endpoints-client-cert-key,control-plane-node-admin-client-cert-key,external-loadbalancer-serving-certkey,internal-loadbalancer-serving-certkey,kubelet-client,localhost-serving-cert-certkey,node-kubeconfigs,service-network-serving-certkey, configmaps: bound-sa-token-signing-certs-10,config-10,etcd-serving-ca-10,kube-apiserver-audit-policies-10,kube-apiserver-cert-syncer-kubeconfig-10,kube-apiserver-pod-10,kubelet-serving-ca-10,sa-token-signing-certs-10, secrets: etcd-client-10,localhost-recovery-client-token-10,localhost-recovery-serving-certkey-10]" to "NodeControllerDegraded: All master nodes are ready"
2022-05-24T18:36:52.349259249Z E0524 18:36:52.349225       1 degraded_webhook.go:128] x509: certificate signed by unknown authority
2022-05-24T18:36:52.463887630Z I0524 18:36:52.463853       1 request.go:665] Waited for 1.59440965s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller
2022-05-24T18:36:53.463898247Z I0524 18:36:53.463865       1 request.go:665] Waited for 1.78227902s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-149-121.ec2.internal
2022-05-24T18:36:54.464400660Z I0524 18:36:54.464368       1 request.go:665] Waited for 2.189594897s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client
2022-05-24T18:36:55.664255128Z I0524 18:36:55.664215       1 request.go:665] Waited for 2.196621901s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-138-197.ec2.internal
2022-05-24T18:36:56.863814058Z I0524 18:36:56.863771       1 request.go:665] Waited for 2.194879406s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller
2022-05-24T18:36:57.670739562Z I0524 18:36:57.670707       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:36:57.864412950Z I0524 18:36:57.864378       1 request.go:665] Waited for 2.197116373s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-169-205.ec2.internal
2022-05-24T18:36:58.864510557Z I0524 18:36:58.864473       1 request.go:665] Waited for 1.193402395s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ip-10-0-169-205.ec2.internal
2022-05-24T18:36:59.864523856Z I0524 18:36:59.864490       1 request.go:665] Waited for 1.195580166s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-149-121.ec2.internal
2022-05-24T18:37:01.063822886Z I0524 18:37:01.063786       1 request.go:665] Waited for 1.194406118s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-138-197.ec2.internal
2022-05-24T18:37:01.467722956Z I0524 18:37:01.467688       1 installer_controller.go:512] "ip-10-0-138-197.ec2.internal" is in transition to 10, but has not made progress because waiting for static pod of revision 10, found 6
2022-05-24T18:37:02.272811490Z I0524 18:37:02.272777       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:37:02.464587098Z I0524 18:37:02.464545       1 request.go:665] Waited for 1.1334995s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T18:37:05.269343151Z I0524 18:37:05.269299       1 installer_controller.go:512] "ip-10-0-138-197.ec2.internal" is in transition to 10, but has not made progress because waiting for static pod of revision 10, found 6
2022-05-24T18:37:22.857520034Z I0524 18:37:22.856474       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:37:27.340297626Z I0524 18:37:27.340250       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:37:28.185788131Z I0524 18:37:28.185749       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:37:28.456199615Z I0524 18:37:28.456144       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:37:38.460151874Z I0524 18:37:38.460071       1 termination_observer.go:236] Observed event "TerminationPreShutdownHooksFinished" for API server pod "kube-apiserver-ip-10-0-149-121.ec2.internal" (last termination at 2022-05-24 18:32:52 +0000 UTC) at 0001-01-01 00:00:00 +0000 UTC
2022-05-24T18:37:58.545917177Z I0524 18:37:58.545868       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:37:59.271220279Z I0524 18:37:59.270319       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 11 triggered by "secret/localhost-recovery-client-token has changed"
2022-05-24T18:38:00.154212758Z I0524 18:38:00.147500       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/revision-status-11 -n openshift-kube-apiserver because it was missing
2022-05-24T18:38:01.146919386Z I0524 18:38:01.146864       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-apiserver-pod-11 -n openshift-kube-apiserver because it was missing
2022-05-24T18:38:01.332713922Z I0524 18:38:01.332673       1 request.go:665] Waited for 1.182709152s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T18:38:02.140614654Z I0524 18:38:02.140553       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/config-11 -n openshift-kube-apiserver because it was missing
2022-05-24T18:38:03.141214676Z I0524 18:38:03.141152       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-apiserver-cert-syncer-kubeconfig-11 -n openshift-kube-apiserver because it was missing
2022-05-24T18:38:04.146231879Z I0524 18:38:04.146156       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/oauth-metadata-11 -n openshift-kube-apiserver because it was missing
2022-05-24T18:38:06.142290626Z I0524 18:38:06.142170       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/bound-sa-token-signing-certs-11 -n openshift-kube-apiserver because it was missing
2022-05-24T18:38:07.143083871Z I0524 18:38:07.143033       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-serving-ca-11 -n openshift-kube-apiserver because it was missing
2022-05-24T18:38:08.026880043Z I0524 18:38:08.026826       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:38:08.139836406Z I0524 18:38:08.139786       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-apiserver-server-ca-11 -n openshift-kube-apiserver because it was missing
2022-05-24T18:38:09.132873445Z I0524 18:38:09.132820       1 request.go:665] Waited for 1.103567119s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ip-10-0-169-205.ec2.internal
2022-05-24T18:38:09.340119124Z I0524 18:38:09.340071       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kubelet-serving-ca-11 -n openshift-kube-apiserver because it was missing
2022-05-24T18:38:10.332369750Z I0524 18:38:10.332329       1 request.go:665] Waited for 1.19373515s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ip-10-0-169-205.ec2.internal
2022-05-24T18:38:10.541748023Z I0524 18:38:10.541701       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/sa-token-signing-certs-11 -n openshift-kube-apiserver because it was missing
2022-05-24T18:38:11.333104289Z I0524 18:38:11.333067       1 request.go:665] Waited for 1.196733386s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/client-ca
2022-05-24T18:38:11.739039760Z I0524 18:38:11.738970       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-apiserver-audit-policies-11 -n openshift-kube-apiserver because it was missing
2022-05-24T18:38:12.532819208Z I0524 18:38:12.532784       1 request.go:665] Waited for 1.194144722s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-server-ca
2022-05-24T18:38:12.940408172Z I0524 18:38:12.940362       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/etcd-client-11 -n openshift-kube-apiserver because it was missing
2022-05-24T18:38:13.533104147Z I0524 18:38:13.533070       1 request.go:665] Waited for 1.189092932s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2022-05-24T18:38:14.744632684Z I0524 18:38:14.744588       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/localhost-recovery-serving-certkey-11 -n openshift-kube-apiserver because it was missing
2022-05-24T18:38:15.342980879Z I0524 18:38:15.341752       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/localhost-recovery-client-token-11 -n openshift-kube-apiserver because it was missing
2022-05-24T18:38:16.150898267Z I0524 18:38:16.150857       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/webhook-authenticator-11 -n openshift-kube-apiserver because it was missing
2022-05-24T18:38:16.170101442Z I0524 18:38:16.170045       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionCreate' Revision 10 created because secret/localhost-recovery-client-token has changed
2022-05-24T18:38:17.333167266Z I0524 18:38:17.333129       1 request.go:665] Waited for 1.165892144s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-149-121.ec2.internal
2022-05-24T18:38:18.532646695Z I0524 18:38:18.532609       1 request.go:665] Waited for 1.194527477s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-138-197.ec2.internal
2022-05-24T18:38:18.747397170Z I0524 18:38:18.747344       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-11-ip-10-0-149-121.ec2.internal -n openshift-kube-apiserver because it was missing
2022-05-24T18:38:18.749858998Z I0524 18:38:18.749834       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:38:19.932955965Z I0524 18:38:19.932910       1 request.go:665] Waited for 1.185914895s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/revision-pruner-11-ip-10-0-138-197.ec2.internal
2022-05-24T18:38:21.132341178Z I0524 18:38:21.132301       1 request.go:665] Waited for 1.594188968s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/installer-10-ip-10-0-138-197.ec2.internal
2022-05-24T18:38:21.135788036Z I0524 18:38:21.135758       1 installer_controller.go:500] "ip-10-0-138-197.ec2.internal" moving to (v1.NodeStatus) {
2022-05-24T18:38:21.135788036Z  NodeName: (string) (len=28) "ip-10-0-138-197.ec2.internal",
2022-05-24T18:38:21.135788036Z  CurrentRevision: (int32) 6,
2022-05-24T18:38:21.135788036Z  TargetRevision: (int32) 11,
2022-05-24T18:38:21.135788036Z  LastFailedRevision: (int32) 0,
2022-05-24T18:38:21.135788036Z  LastFailedTime: (*v1.Time)(<nil>),
2022-05-24T18:38:21.135788036Z  LastFailedReason: (string) "",
2022-05-24T18:38:21.135788036Z  LastFailedCount: (int) 0,
2022-05-24T18:38:21.135788036Z  LastFallbackCount: (int) 0,
2022-05-24T18:38:21.135788036Z  LastFailedRevisionErrors: ([]string) <nil>
2022-05-24T18:38:21.135788036Z }
2022-05-24T18:38:21.135788036Z  because new revision pending
2022-05-24T18:38:21.151082421Z I0524 18:38:21.151053       1 status_controller.go:211] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:35:50Z","message":"NodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:26:37Z","message":"NodeInstallerProgressing: 2 nodes are at revision 6; 1 nodes are at revision 10; 0 nodes have achieved new revision 11","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:11:43Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 10; 0 nodes have achieved new revision 11","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:42Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2022-05-24T18:38:21.161726852Z I0524 18:38:21.161689       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-apiserver changed: Progressing message changed from "NodeInstallerProgressing: 2 nodes are at revision 6; 1 nodes are at revision 10" to "NodeInstallerProgressing: 2 nodes are at revision 6; 1 nodes are at revision 10; 0 nodes have achieved new revision 11",Available message changed from "StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 10" to "StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 10; 0 nodes have achieved new revision 11"
2022-05-24T18:38:21.340250016Z I0524 18:38:21.340199       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-11-ip-10-0-138-197.ec2.internal -n openshift-kube-apiserver because it was missing
2022-05-24T18:38:22.132859137Z I0524 18:38:22.132805       1 request.go:665] Waited for 1.16314301s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client
2022-05-24T18:38:23.332541529Z I0524 18:38:23.332486       1 request.go:665] Waited for 1.99248906s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/revision-pruner-11-ip-10-0-169-205.ec2.internal
2022-05-24T18:38:24.532639737Z I0524 18:38:24.532600       1 request.go:665] Waited for 1.794345142s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T18:38:25.215846980Z I0524 18:38:25.215779       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-11-ip-10-0-169-205.ec2.internal -n openshift-kube-apiserver because it was missing
2022-05-24T18:38:25.533075519Z I0524 18:38:25.533036       1 request.go:665] Waited for 1.796827809s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ip-10-0-138-197.ec2.internal
2022-05-24T18:38:26.732992785Z I0524 18:38:26.732949       1 request.go:665] Waited for 1.795112807s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller
2022-05-24T18:38:27.932416864Z I0524 18:38:27.932379       1 request.go:665] Waited for 1.792399286s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-pod
2022-05-24T18:38:28.932423885Z I0524 18:38:28.932385       1 request.go:665] Waited for 1.595485013s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ip-10-0-138-197.ec2.internal
2022-05-24T18:38:28.935904946Z I0524 18:38:28.935868       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:38:29.745127629Z I0524 18:38:29.745053       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-11-ip-10-0-138-197.ec2.internal -n openshift-kube-apiserver because it was missing
2022-05-24T18:38:29.933076237Z I0524 18:38:29.933030       1 request.go:665] Waited for 1.595728575s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller
2022-05-24T18:38:31.133115826Z I0524 18:38:31.133079       1 request.go:665] Waited for 1.597029537s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-server-ca
2022-05-24T18:38:31.336721180Z I0524 18:38:31.336687       1 installer_controller.go:512] "ip-10-0-138-197.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Pending phase
2022-05-24T18:38:32.332640251Z I0524 18:38:32.332609       1 request.go:665] Waited for 1.716514819s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2022-05-24T18:38:33.332880535Z I0524 18:38:33.332839       1 request.go:665] Waited for 1.793974866s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller
2022-05-24T18:38:34.333118918Z I0524 18:38:34.333075       1 request.go:665] Waited for 1.594817624s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver
2022-05-24T18:38:35.333235523Z I0524 18:38:35.333172       1 request.go:665] Waited for 1.59527546s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ip-10-0-138-197.ec2.internal
2022-05-24T18:38:35.337300868Z I0524 18:38:35.337266       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:38:36.456019458Z I0524 18:38:36.453952       1 status_controller.go:211] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:35:50Z","message":"NodeControllerDegraded: The master nodes not ready: node \"ip-10-0-149-121.ec2.internal\" not ready since 2022-05-24 18:38:36 +0000 UTC because NodeStatusUnknown (Kubelet stopped posting node status.)","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:26:37Z","message":"NodeInstallerProgressing: 2 nodes are at revision 6; 1 nodes are at revision 10; 0 nodes have achieved new revision 11","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:11:43Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 10; 0 nodes have achieved new revision 11","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:42Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2022-05-24T18:38:36.473627493Z I0524 18:38:36.473577       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-apiserver changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready" to "NodeControllerDegraded: The master nodes not ready: node \"ip-10-0-149-121.ec2.internal\" not ready since 2022-05-24 18:38:36 +0000 UTC because NodeStatusUnknown (Kubelet stopped posting node status.)"
2022-05-24T18:38:37.140883805Z I0524 18:38:37.140845       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:38:37.532407942Z I0524 18:38:37.532369       1 request.go:665] Waited for 1.089041272s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/revision-pruner-11-ip-10-0-149-121.ec2.internal
2022-05-24T18:38:38.532979018Z I0524 18:38:38.532916       1 request.go:665] Waited for 1.952806404s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller
2022-05-24T18:38:39.135859441Z I0524 18:38:39.135823       1 installer_controller.go:512] "ip-10-0-138-197.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2022-05-24T18:38:39.532989430Z I0524 18:38:39.532953       1 request.go:665] Waited for 1.99552811s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/revision-pruner-11-ip-10-0-138-197.ec2.internal
2022-05-24T18:38:40.732344369Z I0524 18:38:40.732305       1 request.go:665] Waited for 1.195480167s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/revision-pruner-11-ip-10-0-169-205.ec2.internal
2022-05-24T18:38:41.933250563Z I0524 18:38:41.933214       1 request.go:665] Waited for 1.396487474s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ip-10-0-138-197.ec2.internal
2022-05-24T18:38:43.132155881Z I0524 18:38:43.132116       1 request.go:665] Waited for 1.595552691s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-169-205.ec2.internal
2022-05-24T18:38:43.340966859Z I0524 18:38:43.340935       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:38:45.136102453Z I0524 18:38:45.136069       1 installer_controller.go:512] "ip-10-0-138-197.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2022-05-24T18:38:47.336793072Z I0524 18:38:47.336758       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:38:48.736728723Z I0524 18:38:48.736690       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:39:00.307204619Z I0524 18:39:00.307134       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:39:00.336721208Z I0524 18:39:00.336666       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:39:00.898877223Z I0524 18:39:00.898800       1 termination_observer.go:236] Observed event "TerminationGracefulTerminationFinished" for API server pod "kube-apiserver-ip-10-0-138-197.ec2.internal" (last termination at 2022-05-24 18:17:00 +0000 UTC) at 0001-01-01 00:00:00 +0000 UTC
2022-05-24T18:39:08.678028387Z E0524 18:39:08.677979       1 degraded_webhook.go:128] x509: certificate signed by unknown authority
2022-05-24T18:39:09.288918386Z I0524 18:39:09.288863       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:39:09.686138889Z E0524 18:39:09.686096       1 degraded_webhook.go:128] x509: certificate signed by unknown authority
2022-05-24T18:39:10.123121840Z I0524 18:39:10.123074       1 termination_observer.go:130] Observed termination of API server pod "kube-apiserver-ip-10-0-138-197.ec2.internal" at 2022-05-24 18:39:09 +0000 UTC
2022-05-24T18:39:12.318207773Z I0524 18:39:12.318162       1 installer_controller.go:512] "ip-10-0-138-197.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2022-05-24T18:39:12.512960257Z I0524 18:39:12.512924       1 request.go:665] Waited for 1.178086337s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T18:39:12.672538135Z I0524 18:39:12.672492       1 termination_observer.go:236] Observed event "TerminationPreShutdownHooksFinished" for API server pod "kube-apiserver-ip-10-0-138-197.ec2.internal" (last termination at 2022-05-24 18:39:09 +0000 UTC) at 0001-01-01 00:00:00 +0000 UTC
2022-05-24T18:39:12.803156173Z E0524 18:39:12.803083       1 degraded_webhook.go:128] dial tcp 172.30.73.174:443: i/o timeout
2022-05-24T18:39:13.815456842Z E0524 18:39:13.815407       1 degraded_webhook.go:128] x509: certificate signed by unknown authority
2022-05-24T18:39:14.715860983Z I0524 18:39:14.715827       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:39:14.824656187Z E0524 18:39:14.824624       1 degraded_webhook.go:128] x509: certificate signed by unknown authority
2022-05-24T18:39:16.321474774Z I0524 18:39:16.321438       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:39:17.722121149Z I0524 18:39:17.722084       1 installer_controller.go:512] "ip-10-0-138-197.ec2.internal" is in transition to 11, but has not made progress because waiting for static pod of revision 11, found 10
2022-05-24T18:39:18.512846131Z I0524 18:39:18.512808       1 request.go:665] Waited for 1.086684069s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dopenshift-kube-apiserver
2022-05-24T18:39:21.517553486Z I0524 18:39:21.517480       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:39:22.716074880Z I0524 18:39:22.716036       1 installer_controller.go:512] "ip-10-0-138-197.ec2.internal" is in transition to 11, but has not made progress because waiting for static pod of revision 11, found 10
2022-05-24T18:39:23.712436609Z I0524 18:39:23.712401       1 request.go:665] Waited for 1.050623979s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/config
2022-05-24T18:39:25.717298328Z I0524 18:39:25.717260       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:39:33.058895228Z E0524 18:39:33.058857       1 degraded_webhook.go:128] dial tcp: i/o timeout
2022-05-24T18:39:35.060503443Z E0524 18:39:35.060465       1 degraded_webhook.go:128] dial tcp: i/o timeout
2022-05-24T18:39:38.074264975Z E0524 18:39:38.074223       1 degraded_webhook.go:128] dial tcp: i/o timeout
2022-05-24T18:39:40.075400838Z E0524 18:39:40.075358       1 degraded_webhook.go:128] dial tcp: i/o timeout
2022-05-24T18:39:43.076441826Z E0524 18:39:43.076393       1 degraded_webhook.go:56] prometheusrules.openshift.io: dial tcp: i/o timeout
2022-05-24T18:39:43.978574042Z I0524 18:39:43.978477       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:39:44.089278614Z E0524 18:39:44.089238       1 degraded_webhook.go:128] dial tcp: i/o timeout
2022-05-24T18:39:46.097340388Z E0524 18:39:46.097301       1 degraded_webhook.go:128] dial tcp: i/o timeout
2022-05-24T18:39:46.861425854Z I0524 18:39:46.861378       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:39:46.906216549Z I0524 18:39:46.906160       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:39:46.985213345Z I0524 18:39:46.985135       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:39:48.104175090Z E0524 18:39:48.104132       1 degraded_webhook.go:128] dial tcp: i/o timeout
2022-05-24T18:39:49.779815996Z I0524 18:39:49.779740       1 termination_observer.go:236] Observed event "TerminationGracefulTerminationFinished" for API server pod "kube-apiserver-ip-10-0-149-121.ec2.internal" (last termination at 2022-05-24 18:32:52 +0000 UTC) at 0001-01-01 00:00:00 +0000 UTC
2022-05-24T18:39:50.118288005Z E0524 18:39:50.118241       1 degraded_webhook.go:128] dial tcp: i/o timeout
2022-05-24T18:39:53.126528644Z E0524 18:39:53.126491       1 degraded_webhook.go:128] dial tcp: i/o timeout
2022-05-24T18:39:55.127420576Z E0524 18:39:55.127378       1 degraded_webhook.go:128] dial tcp: i/o timeout
2022-05-24T18:39:58.143917608Z E0524 18:39:58.143879       1 degraded_webhook.go:128] dial tcp: i/o timeout
2022-05-24T18:40:00.144905329Z E0524 18:40:00.144867       1 degraded_webhook.go:128] dial tcp: i/o timeout
2022-05-24T18:40:00.286078084Z I0524 18:40:00.285752       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:00.308561261Z I0524 18:40:00.308513       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:01.889570596Z I0524 18:40:01.889522       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:03.267993602Z I0524 18:40:03.267957       1 request.go:665] Waited for 1.038993937s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-138-197.ec2.internal
2022-05-24T18:40:05.476332845Z I0524 18:40:05.476297       1 installer_controller.go:512] "ip-10-0-138-197.ec2.internal" is in transition to 11, but has not made progress because waiting for static pod of revision 11, found 10
2022-05-24T18:40:08.633520812Z I0524 18:40:08.633481       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:08.915275138Z I0524 18:40:08.912453       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:10.357999625Z I0524 18:40:10.357964       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:11.770475597Z I0524 18:40:11.770431       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:12.886909966Z I0524 18:40:12.886858       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:13.003784906Z I0524 18:40:13.003745       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:13.655064629Z I0524 18:40:13.654291       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:14.680291501Z I0524 18:40:14.680242       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:16.304076126Z I0524 18:40:16.304038       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:17.508901556Z I0524 18:40:17.508867       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:19.508076901Z I0524 18:40:19.508044       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:21.800503625Z I0524 18:40:21.800466       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:25.870385573Z I0524 18:40:25.870348       1 status_controller.go:211] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:35:50Z","message":"NodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:26:37Z","message":"NodeInstallerProgressing: 2 nodes are at revision 6; 1 nodes are at revision 10; 0 nodes have achieved new revision 11","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:11:43Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 10; 0 nodes have achieved new revision 11","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:42Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2022-05-24T18:40:25.896222899Z I0524 18:40:25.892447       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:25.919816821Z I0524 18:40:25.917851       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-apiserver changed: Degraded message changed from "NodeControllerDegraded: The master nodes not ready: node \"ip-10-0-149-121.ec2.internal\" not ready since 2022-05-24 18:38:36 +0000 UTC because NodeStatusUnknown (Kubelet stopped posting node status.)" to "NodeControllerDegraded: All master nodes are ready"
2022-05-24T18:40:25.921995385Z I0524 18:40:25.921958       1 status_controller.go:211] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:35:50Z","message":"NodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:26:37Z","message":"NodeInstallerProgressing: 2 nodes are at revision 6; 1 nodes are at revision 10; 0 nodes have achieved new revision 11","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:11:43Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 10; 0 nodes have achieved new revision 11","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:42Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2022-05-24T18:40:25.938807410Z I0524 18:40:25.938759       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-apiserver changed: Degraded message changed from "NodeControllerDegraded: The master nodes not ready: node \"ip-10-0-149-121.ec2.internal\" not ready since 2022-05-24 18:38:36 +0000 UTC because NodeStatusUnknown (Kubelet stopped posting node status.)" to "NodeControllerDegraded: All master nodes are ready"
2022-05-24T18:40:27.024122215Z I0524 18:40:27.023885       1 request.go:665] Waited for 1.100527699s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/revision-pruner-11-ip-10-0-138-197.ec2.internal
2022-05-24T18:40:28.222423995Z I0524 18:40:28.222379       1 request.go:665] Waited for 1.194772706s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/revision-pruner-11-ip-10-0-169-205.ec2.internal
2022-05-24T18:40:29.421889216Z I0524 18:40:29.421850       1 request.go:665] Waited for 1.393223221s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ip-10-0-138-197.ec2.internal
2022-05-24T18:40:30.422638884Z I0524 18:40:30.422599       1 request.go:665] Waited for 1.197969971s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/installer-11-ip-10-0-138-197.ec2.internal
2022-05-24T18:40:30.627429246Z I0524 18:40:30.627387       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:31.226141175Z I0524 18:40:31.226099       1 installer_controller.go:512] "ip-10-0-138-197.ec2.internal" is in transition to 11, but has not made progress because waiting for static pod of revision 11, found 10
2022-05-24T18:40:33.026709260Z I0524 18:40:33.026672       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:34.025857701Z I0524 18:40:34.025818       1 installer_controller.go:512] "ip-10-0-138-197.ec2.internal" is in transition to 11, but has not made progress because waiting for static pod of revision 11, found 10
2022-05-24T18:40:34.481047939Z I0524 18:40:34.481005       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:35.621878384Z I0524 18:40:35.621825       1 request.go:665] Waited for 1.084242436s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/config
2022-05-24T18:40:37.632560179Z I0524 18:40:37.632520       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:38.026455088Z I0524 18:40:38.026414       1 installer_controller.go:512] "ip-10-0-138-197.ec2.internal" is in transition to 11, but has not made progress because waiting for static pod of revision 11, found 10
2022-05-24T18:40:40.285802130Z I0524 18:40:40.285745       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:42.022662082Z I0524 18:40:42.022631       1 request.go:665] Waited for 1.089539112s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-149-121.ec2.internal
2022-05-24T18:40:43.222643550Z I0524 18:40:43.222594       1 request.go:665] Waited for 1.595614783s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ip-10-0-138-197.ec2.internal
2022-05-24T18:40:43.225282026Z I0524 18:40:43.225248       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:44.222652248Z I0524 18:40:44.222619       1 request.go:665] Waited for 1.396858333s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-138-197.ec2.internal
2022-05-24T18:40:45.425134562Z I0524 18:40:45.421788       1 request.go:665] Waited for 1.394100139s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-server-ca
2022-05-24T18:40:46.422454592Z I0524 18:40:46.422408       1 request.go:665] Waited for 1.397335324s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-149-121.ec2.internal
2022-05-24T18:40:47.621812555Z I0524 18:40:47.621774       1 request.go:665] Waited for 1.395180057s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller
2022-05-24T18:40:48.435025124Z I0524 18:40:48.434984       1 guard_controller.go:234] Node ip-10-0-149-121.ec2.internal not schedulable, skipping reconciling the guard pod
2022-05-24T18:40:49.027026983Z I0524 18:40:49.026989       1 installer_controller.go:512] "ip-10-0-138-197.ec2.internal" is in transition to 11, but has not made progress because waiting for static pod of revision 11, found 10
2022-05-24T18:40:50.821870752Z I0524 18:40:50.821838       1 request.go:665] Waited for 1.152089799s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dopenshift-kube-apiserver
2022-05-24T18:40:54.831295277Z I0524 18:40:54.831239       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/kube-apiserver-guard-ip-10-0-149-121.ec2.internal -n openshift-kube-apiserver because it was missing
2022-05-24T18:40:56.021849995Z I0524 18:40:56.021814       1 request.go:665] Waited for 1.190706934s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-149-121.ec2.internal
2022-05-24T18:40:57.022602739Z I0524 18:40:57.022551       1 request.go:665] Waited for 1.197789254s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ip-10-0-169-205.ec2.internal
2022-05-24T18:40:58.023299975Z I0524 18:40:58.023259       1 request.go:665] Waited for 1.193770649s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller
2022-05-24T18:40:59.221927162Z I0524 18:40:59.221892       1 request.go:665] Waited for 1.17968603s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller
2022-05-24T18:41:00.222532913Z I0524 18:41:00.222490       1 request.go:665] Waited for 1.197026632s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-138-197.ec2.internal
2022-05-24T18:41:01.622720928Z I0524 18:41:01.622680       1 request.go:665] Waited for 1.066379004s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/config
2022-05-24T18:41:16.819953832Z I0524 18:41:16.819917       1 installer_controller.go:512] "ip-10-0-138-197.ec2.internal" is in transition to 11, but has not made progress because waiting for static pod of revision 11, found 10
2022-05-24T18:41:23.626136650Z I0524 18:41:23.626071       1 termination_observer.go:236] Observed event "TerminationGracefulTerminationFinished" for API server pod "kube-apiserver-ip-10-0-138-197.ec2.internal" (last termination at 2022-05-24 18:39:09 +0000 UTC) at 0001-01-01 00:00:00 +0000 UTC
2022-05-24T18:41:30.286769447Z I0524 18:41:30.286723       1 termination_observer.go:130] Observed termination of API server pod "kube-apiserver-ip-10-0-138-197.ec2.internal" at 2022-05-24 18:41:29 +0000 UTC
2022-05-24T18:41:31.485017016Z I0524 18:41:31.484968       1 installer_controller.go:512] "ip-10-0-138-197.ec2.internal" is in transition to 11, but has not made progress because static pod is pending
2022-05-24T18:41:32.482305505Z I0524 18:41:32.482268       1 request.go:665] Waited for 1.142219691s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T18:41:33.882601447Z I0524 18:41:33.882568       1 request.go:665] Waited for 1.026546791s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver
2022-05-24T18:41:35.082203837Z I0524 18:41:35.082139       1 request.go:665] Waited for 1.39594949s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-169-205.ec2.internal
2022-05-24T18:41:38.084749779Z I0524 18:41:38.084714       1 installer_controller.go:512] "ip-10-0-138-197.ec2.internal" is in transition to 11, but has not made progress because static pod is pending
2022-05-24T18:41:38.482580784Z I0524 18:41:38.482529       1 request.go:665] Waited for 1.053979799s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller
2022-05-24T18:41:39.682057706Z I0524 18:41:39.681985       1 request.go:665] Waited for 1.370233567s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client
2022-05-24T18:41:42.081938684Z I0524 18:41:42.081897       1 request.go:665] Waited for 1.148321818s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller
2022-05-24T18:41:43.082408891Z I0524 18:41:43.082371       1 request.go:665] Waited for 1.553322494s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver
2022-05-24T18:41:44.282446283Z I0524 18:41:44.282397       1 request.go:665] Waited for 1.796221224s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-149-121.ec2.internal
2022-05-24T18:41:45.485320565Z I0524 18:41:45.485284       1 installer_controller.go:500] "ip-10-0-138-197.ec2.internal" moving to (v1.NodeStatus) {
2022-05-24T18:41:45.485320565Z  NodeName: (string) (len=28) "ip-10-0-138-197.ec2.internal",
2022-05-24T18:41:45.485320565Z  CurrentRevision: (int32) 11,
2022-05-24T18:41:45.485320565Z  TargetRevision: (int32) 0,
2022-05-24T18:41:45.485320565Z  LastFailedRevision: (int32) 0,
2022-05-24T18:41:45.485320565Z  LastFailedTime: (*v1.Time)(<nil>),
2022-05-24T18:41:45.485320565Z  LastFailedReason: (string) "",
2022-05-24T18:41:45.485320565Z  LastFailedCount: (int) 0,
2022-05-24T18:41:45.485320565Z  LastFallbackCount: (int) 0,
2022-05-24T18:41:45.485320565Z  LastFailedRevisionErrors: ([]string) <nil>
2022-05-24T18:41:45.485320565Z }
2022-05-24T18:41:45.485320565Z  because static pod is ready
2022-05-24T18:41:45.498667831Z I0524 18:41:45.498620       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "ip-10-0-138-197.ec2.internal" from revision 6 to 11 because static pod is ready
2022-05-24T18:41:45.508137325Z I0524 18:41:45.508103       1 status_controller.go:211] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:35:50Z","message":"NodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:26:37Z","message":"NodeInstallerProgressing: 1 nodes are at revision 6; 1 nodes are at revision 10; 1 nodes are at revision 11","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:11:43Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 1 nodes are at revision 10; 1 nodes are at revision 11","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:42Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2022-05-24T18:41:45.518418182Z I0524 18:41:45.517616       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-apiserver changed: Progressing message changed from "NodeInstallerProgressing: 2 nodes are at revision 6; 1 nodes are at revision 10; 0 nodes have achieved new revision 11" to "NodeInstallerProgressing: 1 nodes are at revision 6; 1 nodes are at revision 10; 1 nodes are at revision 11",Available message changed from "StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 10; 0 nodes have achieved new revision 11" to "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 1 nodes are at revision 10; 1 nodes are at revision 11"
2022-05-24T18:41:46.681900595Z I0524 18:41:46.681864       1 request.go:665] Waited for 1.182135738s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa
2022-05-24T18:41:47.682427178Z I0524 18:41:47.682388       1 request.go:665] Waited for 1.797108924s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/trusted-ca-bundle
2022-05-24T18:41:48.881948350Z I0524 18:41:48.881907       1 request.go:665] Waited for 1.188374899s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client
2022-05-24T18:41:49.882060179Z I0524 18:41:49.882019       1 request.go:665] Waited for 1.166673812s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client
2022-05-24T18:42:11.430543539Z I0524 18:42:11.430489       1 installer_controller.go:524] node ip-10-0-169-205.ec2.internal with revision 6 is the oldest and needs new revision 11
2022-05-24T18:42:11.430543539Z I0524 18:42:11.430529       1 installer_controller.go:532] "ip-10-0-169-205.ec2.internal" moving to (v1.NodeStatus) {
2022-05-24T18:42:11.430543539Z  NodeName: (string) (len=28) "ip-10-0-169-205.ec2.internal",
2022-05-24T18:42:11.430543539Z  CurrentRevision: (int32) 6,
2022-05-24T18:42:11.430543539Z  TargetRevision: (int32) 11,
2022-05-24T18:42:11.430543539Z  LastFailedRevision: (int32) 0,
2022-05-24T18:42:11.430543539Z  LastFailedTime: (*v1.Time)(<nil>),
2022-05-24T18:42:11.430543539Z  LastFailedReason: (string) "",
2022-05-24T18:42:11.430543539Z  LastFailedCount: (int) 0,
2022-05-24T18:42:11.430543539Z  LastFallbackCount: (int) 0,
2022-05-24T18:42:11.430543539Z  LastFailedRevisionErrors: ([]string) <nil>
2022-05-24T18:42:11.430543539Z }
2022-05-24T18:42:11.447913660Z I0524 18:42:11.447868       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "ip-10-0-169-205.ec2.internal" from revision 6 to 11 because node ip-10-0-169-205.ec2.internal with revision 6 is the oldest
2022-05-24T18:42:12.543123345Z I0524 18:42:12.543086       1 request.go:665] Waited for 1.08868005s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/installer-sa
2022-05-24T18:42:13.742711299Z I0524 18:42:13.742673       1 request.go:665] Waited for 1.095933966s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client
2022-05-24T18:42:14.943030281Z I0524 18:42:14.942993       1 request.go:665] Waited for 1.160609542s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver
2022-05-24T18:42:16.355300588Z I0524 18:42:16.355240       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-11-ip-10-0-169-205.ec2.internal -n openshift-kube-apiserver because it was missing
2022-05-24T18:42:17.160025157Z I0524 18:42:17.159983       1 installer_controller.go:512] "ip-10-0-169-205.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Pending phase
2022-05-24T18:42:17.542830146Z I0524 18:42:17.542792       1 request.go:665] Waited for 1.183586411s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dopenshift-kube-apiserver
2022-05-24T18:42:18.542998507Z I0524 18:42:18.542958       1 request.go:665] Waited for 1.193436479s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller
2022-05-24T18:42:19.742353604Z I0524 18:42:19.742313       1 request.go:665] Waited for 1.107891552s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2022-05-24T18:42:20.742540884Z I0524 18:42:20.742503       1 request.go:665] Waited for 1.395848924s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ip-10-0-138-197.ec2.internal
2022-05-24T18:42:21.942400622Z I0524 18:42:21.942361       1 request.go:665] Waited for 1.195161321s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ip-10-0-149-121.ec2.internal
2022-05-24T18:42:23.147591331Z I0524 18:42:23.147554       1 installer_controller.go:512] "ip-10-0-169-205.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2022-05-24T18:42:25.942524301Z I0524 18:42:25.942485       1 request.go:665] Waited for 1.037521143s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T18:42:27.144943100Z I0524 18:42:27.144902       1 installer_controller.go:512] "ip-10-0-169-205.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2022-05-24T18:42:32.525349703Z E0524 18:42:32.525243       1 degraded_webhook.go:128] dial tcp 172.30.229.159:443: i/o timeout
2022-05-24T18:42:57.664174281Z I0524 18:42:57.664081       1 termination_observer.go:236] Observed event "TerminationPreShutdownHooksFinished" for API server pod "kube-apiserver-ip-10-0-169-205.ec2.internal" (last termination at 2022-05-24 18:18:36 +0000 UTC) at 0001-01-01 00:00:00 +0000 UTC
2022-05-24T18:42:59.923986708Z I0524 18:42:59.923951       1 request.go:665] Waited for 1.126769589s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ip-10-0-138-197.ec2.internal
2022-05-24T18:43:01.924537499Z I0524 18:43:01.924501       1 request.go:665] Waited for 1.130154586s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller
2022-05-24T18:43:02.924756935Z I0524 18:43:02.924723       1 request.go:665] Waited for 1.578671067s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T18:43:04.728001855Z I0524 18:43:04.727961       1 request.go:665] Waited for 1.000818862s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/client-ca
2022-05-24T18:43:04.928678149Z I0524 18:43:04.928637       1 installer_controller.go:512] "ip-10-0-169-205.ec2.internal" is in transition to 11, but has not made progress because waiting for static pod of revision 11, found 6
2022-05-24T18:43:08.329151608Z I0524 18:43:08.329111       1 installer_controller.go:512] "ip-10-0-169-205.ec2.internal" is in transition to 11, but has not made progress because waiting for static pod of revision 11, found 6
2022-05-24T18:45:08.655001938Z I0524 18:45:08.654942       1 termination_observer.go:236] Observed event "TerminationGracefulTerminationFinished" for API server pod "kube-apiserver-ip-10-0-169-205.ec2.internal" (last termination at 2022-05-24 18:18:36 +0000 UTC) at 0001-01-01 00:00:00 +0000 UTC
2022-05-24T18:45:14.669645090Z I0524 18:45:14.669595       1 termination_observer.go:130] Observed termination of API server pod "kube-apiserver-ip-10-0-169-205.ec2.internal" at 2022-05-24 18:45:14 +0000 UTC
2022-05-24T18:45:17.469841047Z I0524 18:45:17.469800       1 installer_controller.go:512] "ip-10-0-169-205.ec2.internal" is in transition to 11, but has not made progress because static pod is pending
2022-05-24T18:45:22.268581075Z I0524 18:45:22.268542       1 installer_controller.go:512] "ip-10-0-169-205.ec2.internal" is in transition to 11, but has not made progress because static pod is pending
2022-05-24T18:45:22.464305700Z I0524 18:45:22.464258       1 request.go:665] Waited for 1.10972089s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T18:45:25.664231416Z I0524 18:45:25.664190       1 request.go:665] Waited for 1.096874665s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dopenshift-kube-apiserver
2022-05-24T18:45:27.069480090Z I0524 18:45:27.069441       1 installer_controller.go:512] "ip-10-0-169-205.ec2.internal" is in transition to 11, but has not made progress because static pod is pending
2022-05-24T18:45:29.064730434Z I0524 18:45:29.064692       1 request.go:665] Waited for 1.190628312s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2022-05-24T18:45:30.064890841Z I0524 18:45:30.064852       1 request.go:665] Waited for 1.795536518s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-138-197.ec2.internal
2022-05-24T18:45:31.064934391Z I0524 18:45:31.064897       1 request.go:665] Waited for 1.3951294s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-pod
2022-05-24T18:45:32.464381241Z I0524 18:45:32.464346       1 request.go:665] Waited for 1.109331805s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T18:45:33.464683186Z I0524 18:45:33.464643       1 request.go:665] Waited for 1.195902935s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-138-197.ec2.internal
2022-05-24T18:45:57.496113915Z I0524 18:45:57.496074       1 installer_controller.go:500] "ip-10-0-169-205.ec2.internal" moving to (v1.NodeStatus) {
2022-05-24T18:45:57.496113915Z  NodeName: (string) (len=28) "ip-10-0-169-205.ec2.internal",
2022-05-24T18:45:57.496113915Z  CurrentRevision: (int32) 11,
2022-05-24T18:45:57.496113915Z  TargetRevision: (int32) 0,
2022-05-24T18:45:57.496113915Z  LastFailedRevision: (int32) 0,
2022-05-24T18:45:57.496113915Z  LastFailedTime: (*v1.Time)(<nil>),
2022-05-24T18:45:57.496113915Z  LastFailedReason: (string) "",
2022-05-24T18:45:57.496113915Z  LastFailedCount: (int) 0,
2022-05-24T18:45:57.496113915Z  LastFallbackCount: (int) 0,
2022-05-24T18:45:57.496113915Z  LastFailedRevisionErrors: ([]string) <nil>
2022-05-24T18:45:57.496113915Z }
2022-05-24T18:45:57.496113915Z  because static pod is ready
2022-05-24T18:45:57.510169948Z I0524 18:45:57.510124       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "ip-10-0-169-205.ec2.internal" from revision 6 to 11 because static pod is ready
2022-05-24T18:45:57.511589079Z I0524 18:45:57.511554       1 status_controller.go:211] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:35:50Z","message":"NodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:26:37Z","message":"NodeInstallerProgressing: 1 nodes are at revision 10; 2 nodes are at revision 11","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:11:43Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 10; 2 nodes are at revision 11","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:42Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2022-05-24T18:45:57.524143915Z I0524 18:45:57.524081       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-apiserver changed: Progressing message changed from "NodeInstallerProgressing: 1 nodes are at revision 6; 1 nodes are at revision 10; 1 nodes are at revision 11" to "NodeInstallerProgressing: 1 nodes are at revision 10; 2 nodes are at revision 11",Available message changed from "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 1 nodes are at revision 10; 1 nodes are at revision 11" to "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 10; 2 nodes are at revision 11"
2022-05-24T18:45:58.691396218Z I0524 18:45:58.691362       1 request.go:665] Waited for 1.168483543s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/revision-pruner-11-ip-10-0-149-121.ec2.internal
2022-05-24T18:46:03.294897905Z I0524 18:46:03.294858       1 installer_controller.go:524] node ip-10-0-149-121.ec2.internal with revision 10 is the oldest and needs new revision 11
2022-05-24T18:46:03.294951999Z I0524 18:46:03.294937       1 installer_controller.go:532] "ip-10-0-149-121.ec2.internal" moving to (v1.NodeStatus) {
2022-05-24T18:46:03.294951999Z  NodeName: (string) (len=28) "ip-10-0-149-121.ec2.internal",
2022-05-24T18:46:03.294951999Z  CurrentRevision: (int32) 10,
2022-05-24T18:46:03.294951999Z  TargetRevision: (int32) 11,
2022-05-24T18:46:03.294951999Z  LastFailedRevision: (int32) 6,
2022-05-24T18:46:03.294951999Z  LastFailedTime: (*v1.Time)(0xc0063e6bd0)(2022-05-24 18:21:05 +0000 UTC),
2022-05-24T18:46:03.294951999Z  LastFailedReason: (string) (len=15) "InstallerFailed",
2022-05-24T18:46:03.294951999Z  LastFailedCount: (int) 1,
2022-05-24T18:46:03.294951999Z  LastFallbackCount: (int) 0,
2022-05-24T18:46:03.294951999Z  LastFailedRevisionErrors: ([]string) (len=1 cap=1) {
2022-05-24T18:46:03.294951999Z   (string) (len=2059) "installer: -apiserver-server-ca\"\n },\n CertSecretNames: ([]string) (len=10 cap=16) {\n  (string) (len=17) \"aggregator-client\",\n  (string) (len=30) \"localhost-serving-cert-certkey\",\n  (string) (len=31) \"service-network-serving-certkey\",\n  (string) (len=37) \"external-loadbalancer-serving-certkey\",\n  (string) (len=37) \"internal-loadbalancer-serving-certkey\",\n  (string) (len=33) \"bound-service-account-signing-key\",\n  (string) (len=40) \"control-plane-node-admin-client-cert-key\",\n  (string) (len=31) \"check-endpoints-client-cert-key\",\n  (string) (len=14) \"kubelet-client\",\n  (string) (len=16) \"node-kubeconfigs\"\n },\n OptionalCertSecretNamePrefixes: ([]string) (len=11 cap=16) {\n  (string) (len=17) \"user-serving-cert\",\n  (string) (len=21) \"user-serving-cert-000\",\n  (string) (len=21) \"user-serving-cert-001\",\n  (string) (len=21) \"user-serving-cert-002\",\n  (string) (len=21) \"user-serving-cert-003\",\n  (string) (len=21) \"user-serving-cert-004\",\n  (string) (len=21) \"user-serving-cert-005\",\n  (string) (len=21) \"user-serving-cert-006\",\n  (string) (len=21) \"user-serving-cert-007\",\n  (string) (len=21) \"user-serving-cert-008\",\n  (string) (len=21) \"user-serving-cert-009\"\n },\n CertConfigMapNamePrefixes: ([]string) (len=4 cap=4) {\n  (string) (len=20) \"aggregator-client-ca\",\n  (string) (len=9) \"client-ca\",\n  (string) (len=29) \"control-plane-node-kubeconfig\",\n  (string) (len=26) \"check-endpoints-kubeconfig\"\n },\n OptionalCertConfigMapNamePrefixes: ([]string) (len=1 cap=1) {\n  (string) (len=17) \"trusted-ca-bundle\"\n },\n CertDir: (string) (len=57) \"/etc/kubernetes/static-pod-resources/kube-apiserver-certs\",\n ResourceDir: (string) (len=36) \"/etc/kubernetes/static-pod-resources\",\n PodManifestDir: (string) (len=25) \"/etc/kubernetes/manifests\",\n Timeout: (time.Duration) 2m0s,\n StaticPodManifestsLockFile: (string) \"\",\n PodMutationFns: ([]installerpod.PodMutationFunc) <nil>,\n KubeletVersion: (string) \"\"\n})\nF0524 18:21:01.380244       1 cmd.go:106] the server was unable to return a response in the time allotted, but may still be processing the request (get pods)\n"
2022-05-24T18:46:03.294951999Z  }
2022-05-24T18:46:03.294951999Z }
2022-05-24T18:46:03.309111632Z I0524 18:46:03.309054       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "ip-10-0-149-121.ec2.internal" from revision 10 to 11 because node ip-10-0-149-121.ec2.internal with revision 10 is the oldest
2022-05-24T18:46:04.491959574Z I0524 18:46:04.491912       1 request.go:665] Waited for 1.179534297s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/revision-pruner-11-ip-10-0-149-121.ec2.internal
2022-05-24T18:46:05.691282233Z I0524 18:46:05.691243       1 request.go:665] Waited for 1.195593309s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/revision-pruner-11-ip-10-0-138-197.ec2.internal
2022-05-24T18:46:07.507640423Z I0524 18:46:07.507591       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-11-ip-10-0-149-121.ec2.internal -n openshift-kube-apiserver because it was missing
2022-05-24T18:46:08.691334351Z I0524 18:46:08.691297       1 request.go:665] Waited for 1.183746471s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/installer-11-ip-10-0-149-121.ec2.internal
2022-05-24T18:46:08.695195151Z I0524 18:46:08.695163       1 installer_controller.go:512] "ip-10-0-149-121.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Pending phase
2022-05-24T18:46:09.891347784Z I0524 18:46:09.891306       1 request.go:665] Waited for 1.195335263s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-149-121.ec2.internal
2022-05-24T18:46:10.891978323Z I0524 18:46:10.891929       1 request.go:665] Waited for 1.195206136s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller
2022-05-24T18:46:12.491884410Z I0524 18:46:12.491852       1 request.go:665] Waited for 1.134544168s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T18:46:13.894659763Z I0524 18:46:13.894622       1 installer_controller.go:512] "ip-10-0-149-121.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2022-05-24T18:46:16.896397567Z I0524 18:46:16.896363       1 installer_controller.go:512] "ip-10-0-149-121.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2022-05-24T18:46:40.128812179Z I0524 18:46:40.128772       1 externalloadbalancer.go:27] syncing external loadbalancer hostnames: api.odf-service.dif5.p1.openshiftapps.com
2022-05-24T18:46:40.128875339Z I0524 18:46:40.128782       1 servicehostname.go:40] syncing servicenetwork hostnames: [172.30.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local openshift openshift.default openshift.default.svc openshift.default.svc.cluster.local]
2022-05-24T18:46:41.879146969Z I0524 18:46:41.879107       1 request.go:665] Waited for 1.010479209s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-149-121.ec2.internal
2022-05-24T18:46:43.078969816Z I0524 18:46:43.078934       1 request.go:665] Waited for 1.798311421s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T18:46:44.079515754Z I0524 18:46:44.079481       1 request.go:665] Waited for 1.995467348s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-138-197.ec2.internal
2022-05-24T18:46:45.279339126Z I0524 18:46:45.279297       1 request.go:665] Waited for 1.795777011s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ip-10-0-149-121.ec2.internal
2022-05-24T18:46:47.078720136Z I0524 18:46:47.078679       1 request.go:665] Waited for 1.143421294s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T18:46:48.079334110Z I0524 18:46:48.079298       1 request.go:665] Waited for 1.396471647s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/installer-11-ip-10-0-149-121.ec2.internal
2022-05-24T18:46:48.082828321Z I0524 18:46:48.082802       1 installer_controller.go:512] "ip-10-0-149-121.ec2.internal" is in transition to 11, but has not made progress because installer is not finished, but in Running phase
2022-05-24T18:46:48.498347400Z I0524 18:46:48.498281       1 termination_observer.go:236] Observed event "TerminationPreShutdownHooksFinished" for API server pod "kube-apiserver-ip-10-0-149-121.ec2.internal" (last termination at 2022-05-24 18:32:52 +0000 UTC) at 0001-01-01 00:00:00 +0000 UTC
2022-05-24T18:46:49.479367088Z I0524 18:46:49.479327       1 request.go:665] Waited for 1.007223407s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T18:46:50.679188433Z I0524 18:46:50.679141       1 request.go:665] Waited for 1.159794632s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller
2022-05-24T18:46:51.679340207Z I0524 18:46:51.679308       1 request.go:665] Waited for 1.396173847s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-169-205.ec2.internal
2022-05-24T18:46:52.679679726Z I0524 18:46:52.679636       1 request.go:665] Waited for 1.197077268s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ip-10-0-169-205.ec2.internal
2022-05-24T18:46:53.878870310Z I0524 18:46:53.878832       1 request.go:665] Waited for 1.59551232s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-server-ca
2022-05-24T18:46:55.082671630Z I0524 18:46:55.082636       1 installer_controller.go:512] "ip-10-0-149-121.ec2.internal" is in transition to 11, but has not made progress because waiting for static pod of revision 11, found 10
2022-05-24T18:46:57.679152077Z I0524 18:46:57.679116       1 request.go:665] Waited for 1.077596621s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/config
2022-05-24T18:47:00.291965263Z I0524 18:47:00.291928       1 installer_controller.go:512] "ip-10-0-149-121.ec2.internal" is in transition to 11, but has not made progress because waiting for static pod of revision 11, found 10
2022-05-24T18:48:59.715425969Z I0524 18:48:59.715360       1 termination_observer.go:236] Observed event "TerminationGracefulTerminationFinished" for API server pod "kube-apiserver-ip-10-0-149-121.ec2.internal" (last termination at 2022-05-24 18:32:52 +0000 UTC) at 0001-01-01 00:00:00 +0000 UTC
2022-05-24T18:49:11.376155563Z I0524 18:49:11.376104       1 termination_observer.go:130] Observed termination of API server pod "kube-apiserver-ip-10-0-149-121.ec2.internal" at 2022-05-24 18:49:10 +0000 UTC
2022-05-24T18:49:12.369651495Z I0524 18:49:12.369604       1 request.go:665] Waited for 1.000609815s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T18:49:13.369876426Z I0524 18:49:13.369835       1 request.go:665] Waited for 1.194790027s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller
2022-05-24T18:49:14.374719059Z I0524 18:49:14.374679       1 installer_controller.go:512] "ip-10-0-149-121.ec2.internal" is in transition to 11, but has not made progress because static pod is pending
2022-05-24T18:49:16.170441480Z I0524 18:49:16.170405       1 request.go:665] Waited for 1.137526101s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2022-05-24T18:49:19.373713526Z I0524 18:49:19.373676       1 installer_controller.go:512] "ip-10-0-149-121.ec2.internal" is in transition to 11, but has not made progress because static pod is pending
2022-05-24T18:49:22.574036134Z I0524 18:49:22.573998       1 installer_controller.go:512] "ip-10-0-149-121.ec2.internal" is in transition to 11, but has not made progress because static pod is pending
2022-05-24T18:49:58.716302685Z I0524 18:49:58.716264       1 installer_controller.go:500] "ip-10-0-149-121.ec2.internal" moving to (v1.NodeStatus) {
2022-05-24T18:49:58.716302685Z  NodeName: (string) (len=28) "ip-10-0-149-121.ec2.internal",
2022-05-24T18:49:58.716302685Z  CurrentRevision: (int32) 11,
2022-05-24T18:49:58.716302685Z  TargetRevision: (int32) 0,
2022-05-24T18:49:58.716302685Z  LastFailedRevision: (int32) 6,
2022-05-24T18:49:58.716302685Z  LastFailedTime: (*v1.Time)(0xc005b0aff0)(2022-05-24 18:21:05 +0000 UTC),
2022-05-24T18:49:58.716302685Z  LastFailedReason: (string) (len=15) "InstallerFailed",
2022-05-24T18:49:58.716302685Z  LastFailedCount: (int) 1,
2022-05-24T18:49:58.716302685Z  LastFallbackCount: (int) 0,
2022-05-24T18:49:58.716302685Z  LastFailedRevisionErrors: ([]string) (len=1 cap=1) {
2022-05-24T18:49:58.716302685Z   (string) (len=2059) "installer: -apiserver-server-ca\"\n },\n CertSecretNames: ([]string) (len=10 cap=16) {\n  (string) (len=17) \"aggregator-client\",\n  (string) (len=30) \"localhost-serving-cert-certkey\",\n  (string) (len=31) \"service-network-serving-certkey\",\n  (string) (len=37) \"external-loadbalancer-serving-certkey\",\n  (string) (len=37) \"internal-loadbalancer-serving-certkey\",\n  (string) (len=33) \"bound-service-account-signing-key\",\n  (string) (len=40) \"control-plane-node-admin-client-cert-key\",\n  (string) (len=31) \"check-endpoints-client-cert-key\",\n  (string) (len=14) \"kubelet-client\",\n  (string) (len=16) \"node-kubeconfigs\"\n },\n OptionalCertSecretNamePrefixes: ([]string) (len=11 cap=16) {\n  (string) (len=17) \"user-serving-cert\",\n  (string) (len=21) \"user-serving-cert-000\",\n  (string) (len=21) \"user-serving-cert-001\",\n  (string) (len=21) \"user-serving-cert-002\",\n  (string) (len=21) \"user-serving-cert-003\",\n  (string) (len=21) \"user-serving-cert-004\",\n  (string) (len=21) \"user-serving-cert-005\",\n  (string) (len=21) \"user-serving-cert-006\",\n  (string) (len=21) \"user-serving-cert-007\",\n  (string) (len=21) \"user-serving-cert-008\",\n  (string) (len=21) \"user-serving-cert-009\"\n },\n CertConfigMapNamePrefixes: ([]string) (len=4 cap=4) {\n  (string) (len=20) \"aggregator-client-ca\",\n  (string) (len=9) \"client-ca\",\n  (string) (len=29) \"control-plane-node-kubeconfig\",\n  (string) (len=26) \"check-endpoints-kubeconfig\"\n },\n OptionalCertConfigMapNamePrefixes: ([]string) (len=1 cap=1) {\n  (string) (len=17) \"trusted-ca-bundle\"\n },\n CertDir: (string) (len=57) \"/etc/kubernetes/static-pod-resources/kube-apiserver-certs\",\n ResourceDir: (string) (len=36) \"/etc/kubernetes/static-pod-resources\",\n PodManifestDir: (string) (len=25) \"/etc/kubernetes/manifests\",\n Timeout: (time.Duration) 2m0s,\n StaticPodManifestsLockFile: (string) \"\",\n PodMutationFns: ([]installerpod.PodMutationFunc) <nil>,\n KubeletVersion: (string) \"\"\n})\nF0524 18:21:01.380244       1 cmd.go:106] the server was unable to return a response in the time allotted, but may still be processing the request (get pods)\n"
2022-05-24T18:49:58.716302685Z  }
2022-05-24T18:49:58.716302685Z }
2022-05-24T18:49:58.716302685Z  because static pod is ready
2022-05-24T18:49:58.728827482Z I0524 18:49:58.728769       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "ip-10-0-149-121.ec2.internal" from revision 10 to 11 because static pod is ready
2022-05-24T18:49:58.739295833Z I0524 18:49:58.739260       1 status_controller.go:211] clusteroperator/kube-apiserver diff {"status":{"conditions":[{"lastTransitionTime":"2022-05-24T18:35:50Z","message":"NodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2022-05-24T18:49:58Z","message":"NodeInstallerProgressing: 3 nodes are at revision 11","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2022-05-24T18:11:43Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 11","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2022-05-24T18:07:42Z","message":"KubeletMinorVersionUpgradeable: Kubelet and API server minor versions are synced.","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2022-05-24T18:49:58.746601711Z I0524 18:49:58.746552       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-apiserver-operator", Name:"kube-apiserver-operator", UID:"f93b6045-6e50-49a6-9e4f-5d1a1f853d0f", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-apiserver changed: Progressing changed from True to False ("NodeInstallerProgressing: 3 nodes are at revision 11"),Available message changed from "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 10; 2 nodes are at revision 11" to "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 11"
2022-05-24T18:49:59.899016474Z I0524 18:49:59.898979       1 request.go:665] Waited for 1.090432263s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2022-05-24T18:50:00.899521023Z I0524 18:50:00.899473       1 request.go:665] Waited for 1.396123708s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/client-ca
2022-05-24T18:50:02.499202540Z I0524 18:50:02.499136       1 request.go:665] Waited for 1.127769084s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T18:56:40.128869484Z I0524 18:56:40.128834       1 externalloadbalancer.go:27] syncing external loadbalancer hostnames: api.odf-service.dif5.p1.openshiftapps.com
2022-05-24T18:56:40.128977011Z I0524 18:56:40.128929       1 servicehostname.go:40] syncing servicenetwork hostnames: [172.30.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local openshift openshift.default openshift.default.svc openshift.default.svc.cluster.local]
2022-05-24T18:56:41.930508223Z I0524 18:56:41.930466       1 request.go:665] Waited for 1.060623687s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller
2022-05-24T18:56:42.930627050Z I0524 18:56:42.930590       1 request.go:665] Waited for 1.649040144s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2022-05-24T18:56:44.130115384Z I0524 18:56:44.130078       1 request.go:665] Waited for 1.996745897s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-138-197.ec2.internal
2022-05-24T18:56:45.130206337Z I0524 18:56:45.130157       1 request.go:665] Waited for 1.79590062s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/trusted-ca-bundle
2022-05-24T19:06:40.128998068Z I0524 19:06:40.128930       1 servicehostname.go:40] syncing servicenetwork hostnames: [172.30.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local openshift openshift.default openshift.default.svc openshift.default.svc.cluster.local]
2022-05-24T19:06:40.128998068Z I0524 19:06:40.128964       1 externalloadbalancer.go:27] syncing external loadbalancer hostnames: api.odf-service.dif5.p1.openshiftapps.com
2022-05-24T19:06:41.930336568Z I0524 19:06:41.930299       1 request.go:665] Waited for 1.059474626s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-149-121.ec2.internal
2022-05-24T19:06:42.930397374Z I0524 19:06:42.930359       1 request.go:665] Waited for 1.648029801s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2022-05-24T19:06:44.129924940Z I0524 19:06:44.129886       1 request.go:665] Waited for 1.792988679s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-138-197.ec2.internal
2022-05-24T19:06:45.130330389Z I0524 19:06:45.130293       1 request.go:665] Waited for 1.788380861s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client
2022-05-24T19:16:40.129149908Z I0524 19:16:40.129108       1 externalloadbalancer.go:27] syncing external loadbalancer hostnames: api.odf-service.dif5.p1.openshiftapps.com
2022-05-24T19:16:40.129233877Z I0524 19:16:40.129097       1 servicehostname.go:40] syncing servicenetwork hostnames: [172.30.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local openshift openshift.default openshift.default.svc openshift.default.svc.cluster.local]
2022-05-24T19:16:42.004596056Z I0524 19:16:42.004555       1 request.go:665] Waited for 1.133293181s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller
2022-05-24T19:16:43.004828066Z I0524 19:16:43.004794       1 request.go:665] Waited for 1.721417666s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2022-05-24T19:16:44.205238167Z I0524 19:16:44.205204       1 request.go:665] Waited for 1.796312695s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-138-197.ec2.internal
2022-05-24T19:16:45.205476265Z I0524 19:16:45.205436       1 request.go:665] Waited for 1.797913308s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-server-ca
2022-05-24T19:26:40.129531810Z I0524 19:26:40.129459       1 servicehostname.go:40] syncing servicenetwork hostnames: [172.30.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local openshift openshift.default openshift.default.svc openshift.default.svc.cluster.local]
2022-05-24T19:26:40.129636023Z I0524 19:26:40.129521       1 externalloadbalancer.go:27] syncing external loadbalancer hostnames: api.odf-service.dif5.p1.openshiftapps.com
2022-05-24T19:26:41.930952498Z I0524 19:26:41.930912       1 request.go:665] Waited for 1.058847573s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-149-121.ec2.internal
2022-05-24T19:26:42.930995592Z I0524 19:26:42.930960       1 request.go:665] Waited for 1.647223959s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2022-05-24T19:26:44.131429246Z I0524 19:26:44.131398       1 request.go:665] Waited for 1.796113947s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-138-197.ec2.internal
2022-05-24T19:26:45.331609327Z I0524 19:26:45.331560       1 request.go:665] Waited for 1.597212474s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-169-205.ec2.internal
2022-05-24T19:26:47.735615055Z I0524 19:26:47.735576       1 request.go:665] Waited for 1.000829737s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ip-10-0-169-205.ec2.internal
2022-05-24T19:36:40.130244781Z I0524 19:36:40.130199       1 externalloadbalancer.go:27] syncing external loadbalancer hostnames: api.odf-service.dif5.p1.openshiftapps.com
2022-05-24T19:36:40.130330224Z I0524 19:36:40.130224       1 servicehostname.go:40] syncing servicenetwork hostnames: [172.30.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local openshift openshift.default openshift.default.svc openshift.default.svc.cluster.local]
2022-05-24T19:36:41.932087894Z I0524 19:36:41.932050       1 request.go:665] Waited for 1.059802011s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dopenshift-kube-apiserver
2022-05-24T19:36:43.131952802Z I0524 19:36:43.131912       1 request.go:665] Waited for 1.847632014s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T19:36:44.331702129Z I0524 19:36:44.331663       1 request.go:665] Waited for 1.996608994s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-138-197.ec2.internal
2022-05-24T19:36:45.331745618Z I0524 19:36:45.331707       1 request.go:665] Waited for 1.795261647s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ip-10-0-169-205.ec2.internal
2022-05-24T19:36:50.332059465Z I0524 19:36:50.332023       1 request.go:665] Waited for 1.117468086s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2022-05-24T19:46:40.130724139Z I0524 19:46:40.130679       1 externalloadbalancer.go:27] syncing external loadbalancer hostnames: api.odf-service.dif5.p1.openshiftapps.com
2022-05-24T19:46:40.130804985Z I0524 19:46:40.130662       1 servicehostname.go:40] syncing servicenetwork hostnames: [172.30.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local openshift openshift.default openshift.default.svc openshift.default.svc.cluster.local]
2022-05-24T19:46:41.931772409Z I0524 19:46:41.931739       1 request.go:665] Waited for 1.058680396s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-149-121.ec2.internal
2022-05-24T19:46:42.932039477Z I0524 19:46:42.931998       1 request.go:665] Waited for 1.646821078s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2022-05-24T19:46:43.932135312Z I0524 19:46:43.932096       1 request.go:665] Waited for 1.996633581s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-138-197.ec2.internal
2022-05-24T19:46:44.932480240Z I0524 19:46:44.932443       1 request.go:665] Waited for 1.796996839s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T19:46:48.932414975Z I0524 19:46:48.932376       1 request.go:665] Waited for 1.110082757s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T19:56:40.131821754Z I0524 19:56:40.131779       1 externalloadbalancer.go:27] syncing external loadbalancer hostnames: api.odf-service.dif5.p1.openshiftapps.com
2022-05-24T19:56:40.131881443Z I0524 19:56:40.131783       1 servicehostname.go:40] syncing servicenetwork hostnames: [172.30.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local openshift openshift.default openshift.default.svc openshift.default.svc.cluster.local]
2022-05-24T19:56:41.933668757Z I0524 19:56:41.933631       1 request.go:665] Waited for 1.060194519s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-149-121.ec2.internal
2022-05-24T19:56:43.133362857Z I0524 19:56:43.133322       1 request.go:665] Waited for 1.84729041s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T19:56:44.333481819Z I0524 19:56:44.333062       1 request.go:665] Waited for 1.796904239s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver
2022-05-24T19:56:45.333328473Z I0524 19:56:45.333294       1 request.go:665] Waited for 1.596526705s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-169-205.ec2.internal
2022-05-24T19:56:47.333411053Z I0524 19:56:47.333372       1 request.go:665] Waited for 1.012873682s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2022-05-24T19:56:48.333708771Z I0524 19:56:48.333672       1 request.go:665] Waited for 1.397500213s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ip-10-0-138-197.ec2.internal
2022-05-24T20:06:40.132500690Z I0524 20:06:40.132456       1 externalloadbalancer.go:27] syncing external loadbalancer hostnames: api.odf-service.dif5.p1.openshiftapps.com
2022-05-24T20:06:40.132561033Z I0524 20:06:40.132499       1 servicehostname.go:40] syncing servicenetwork hostnames: [172.30.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local openshift openshift.default openshift.default.svc openshift.default.svc.cluster.local]
2022-05-24T20:06:41.933870812Z I0524 20:06:41.933821       1 request.go:665] Waited for 1.060299036s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dopenshift-kube-apiserver
2022-05-24T20:06:43.134417582Z I0524 20:06:43.134376       1 request.go:665] Waited for 1.844365613s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T20:06:44.134438583Z I0524 20:06:44.134403       1 request.go:665] Waited for 1.796050129s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-138-197.ec2.internal
2022-05-24T20:06:45.334061588Z I0524 20:06:45.334021       1 request.go:665] Waited for 1.396205106s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-169-205.ec2.internal
2022-05-24T20:06:46.533884130Z I0524 20:06:46.533844       1 request.go:665] Waited for 1.393348709s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/config
2022-05-24T20:16:40.132873298Z I0524 20:16:40.132832       1 externalloadbalancer.go:27] syncing external loadbalancer hostnames: api.odf-service.dif5.p1.openshiftapps.com
2022-05-24T20:16:40.132937789Z I0524 20:16:40.132871       1 servicehostname.go:40] syncing servicenetwork hostnames: [172.30.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local openshift openshift.default openshift.default.svc openshift.default.svc.cluster.local]
2022-05-24T20:16:41.934086806Z I0524 20:16:41.934045       1 request.go:665] Waited for 1.059337972s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller
2022-05-24T20:16:42.934168561Z I0524 20:16:42.934129       1 request.go:665] Waited for 1.646182332s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2022-05-24T20:16:43.934569464Z I0524 20:16:43.934530       1 request.go:665] Waited for 1.995248859s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller
2022-05-24T20:16:45.134279207Z I0524 20:16:45.134238       1 request.go:665] Waited for 1.7923066s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ip-10-0-149-121.ec2.internal
2022-05-24T20:16:46.334903822Z I0524 20:16:46.334865       1 request.go:665] Waited for 1.397449928s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T20:26:40.133297795Z I0524 20:26:40.133258       1 externalloadbalancer.go:27] syncing external loadbalancer hostnames: api.odf-service.dif5.p1.openshiftapps.com
2022-05-24T20:26:40.133350072Z I0524 20:26:40.133290       1 servicehostname.go:40] syncing servicenetwork hostnames: [172.30.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local openshift openshift.default openshift.default.svc openshift.default.svc.cluster.local]
2022-05-24T20:26:41.934499175Z I0524 20:26:41.934464       1 request.go:665] Waited for 1.058814202s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dopenshift-kube-apiserver
2022-05-24T20:26:42.935271735Z I0524 20:26:42.935231       1 request.go:665] Waited for 1.644634228s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T20:26:43.935476082Z I0524 20:26:43.935437       1 request.go:665] Waited for 1.796326466s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-138-197.ec2.internal
2022-05-24T20:26:45.134766860Z I0524 20:26:45.134728       1 request.go:665] Waited for 1.795028934s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/localhost-recovery-client-token
2022-05-24T20:33:42.014506775Z I0524 20:33:42.014468       1 request.go:665] Waited for 1.013560544s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-149-121.ec2.internal
2022-05-24T20:36:40.136520584Z I0524 20:36:40.135103       1 servicehostname.go:40] syncing servicenetwork hostnames: [172.30.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local openshift openshift.default openshift.default.svc openshift.default.svc.cluster.local]
2022-05-24T20:36:40.152015082Z I0524 20:36:40.151980       1 externalloadbalancer.go:27] syncing external loadbalancer hostnames: api.odf-service.dif5.p1.openshiftapps.com
2022-05-24T20:36:41.720461765Z I0524 20:36:41.720423       1 request.go:665] Waited for 1.14181107s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2022-05-24T20:36:42.920027024Z I0524 20:36:42.919988       1 request.go:665] Waited for 1.995072517s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-server-ca
2022-05-24T20:36:43.920465938Z I0524 20:36:43.920434       1 request.go:665] Waited for 1.995952316s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ip-10-0-149-121.ec2.internal
2022-05-24T20:36:44.920476510Z I0524 20:36:44.920437       1 request.go:665] Waited for 1.997261213s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/trusted-ca-bundle
2022-05-24T20:36:46.120872458Z I0524 20:36:46.120834       1 request.go:665] Waited for 1.39555764s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-169-205.ec2.internal
2022-05-24T20:46:40.135277359Z I0524 20:46:40.135192       1 servicehostname.go:40] syncing servicenetwork hostnames: [172.30.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local openshift openshift.default openshift.default.svc openshift.default.svc.cluster.local]
2022-05-24T20:46:40.152387063Z I0524 20:46:40.152350       1 externalloadbalancer.go:27] syncing external loadbalancer hostnames: api.odf-service.dif5.p1.openshiftapps.com
2022-05-24T20:46:41.887012110Z I0524 20:46:41.886977       1 request.go:665] Waited for 1.009485446s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dopenshift-kube-apiserver
2022-05-24T20:46:43.086276975Z I0524 20:46:43.086237       1 request.go:665] Waited for 1.795899155s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T20:46:44.086829428Z I0524 20:46:44.086797       1 request.go:665] Waited for 1.991710947s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller
2022-05-24T20:46:45.286416199Z I0524 20:46:45.286380       1 request.go:665] Waited for 1.796482014s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ip-10-0-149-121.ec2.internal
2022-05-24T20:56:40.135575898Z I0524 20:56:40.135511       1 servicehostname.go:40] syncing servicenetwork hostnames: [172.30.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local openshift openshift.default openshift.default.svc openshift.default.svc.cluster.local]
2022-05-24T20:56:40.152724314Z I0524 20:56:40.152683       1 externalloadbalancer.go:27] syncing external loadbalancer hostnames: api.odf-service.dif5.p1.openshiftapps.com
2022-05-24T20:56:42.056028778Z I0524 20:56:42.055993       1 request.go:665] Waited for 1.178051908s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dopenshift-kube-apiserver
2022-05-24T20:56:43.256039113Z I0524 20:56:43.255996       1 request.go:665] Waited for 1.964554819s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T20:56:44.456256874Z I0524 20:56:44.456212       1 request.go:665] Waited for 1.996476389s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-138-197.ec2.internal
2022-05-24T20:56:45.655856097Z I0524 20:56:45.655817       1 request.go:665] Waited for 1.762723427s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver
2022-05-24T20:56:48.655383851Z I0524 20:56:48.655333       1 request.go:665] Waited for 1.030108883s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2022-05-24T21:06:40.136113167Z I0524 21:06:40.136042       1 servicehostname.go:40] syncing servicenetwork hostnames: [172.30.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local openshift openshift.default openshift.default.svc openshift.default.svc.cluster.local]
2022-05-24T21:06:40.152835434Z I0524 21:06:40.152800       1 externalloadbalancer.go:27] syncing external loadbalancer hostnames: api.odf-service.dif5.p1.openshiftapps.com
2022-05-24T21:06:41.970966456Z I0524 21:06:41.970923       1 request.go:665] Waited for 1.092646841s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-149-121.ec2.internal
2022-05-24T21:06:43.170599470Z I0524 21:06:43.170556       1 request.go:665] Waited for 1.878061579s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T21:06:44.370583722Z I0524 21:06:44.370548       1 request.go:665] Waited for 1.996861445s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-138-197.ec2.internal
2022-05-24T21:06:45.570883408Z I0524 21:06:45.570840       1 request.go:665] Waited for 1.597513631s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-169-205.ec2.internal
2022-05-24T21:06:47.370454365Z I0524 21:06:47.370417       1 request.go:665] Waited for 1.104975609s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2022-05-24T21:06:48.370624061Z I0524 21:06:48.370591       1 request.go:665] Waited for 1.196910222s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/localhost-recovery-client-token
2022-05-24T21:16:40.136493978Z I0524 21:16:40.136417       1 servicehostname.go:40] syncing servicenetwork hostnames: [172.30.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local openshift openshift.default openshift.default.svc openshift.default.svc.cluster.local]
2022-05-24T21:16:40.153362923Z I0524 21:16:40.153321       1 externalloadbalancer.go:27] syncing external loadbalancer hostnames: api.odf-service.dif5.p1.openshiftapps.com
2022-05-24T21:16:41.943549238Z I0524 21:16:41.943514       1 request.go:665] Waited for 1.064749122s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-149-121.ec2.internal
2022-05-24T21:16:43.143115095Z I0524 21:16:43.143080       1 request.go:665] Waited for 1.850242463s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T21:16:44.143338228Z I0524 21:16:44.143304       1 request.go:665] Waited for 1.794173751s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller
2022-05-24T21:16:45.143456687Z I0524 21:16:45.143418       1 request.go:665] Waited for 1.796636564s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ip-10-0-149-121.ec2.internal
2022-05-24T21:16:46.343217938Z I0524 21:16:46.343160       1 request.go:665] Waited for 1.196426297s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ip-10-0-169-205.ec2.internal
2022-05-24T21:26:40.137212966Z I0524 21:26:40.137124       1 servicehostname.go:40] syncing servicenetwork hostnames: [172.30.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local openshift openshift.default openshift.default.svc openshift.default.svc.cluster.local]
2022-05-24T21:26:40.153487889Z I0524 21:26:40.153441       1 externalloadbalancer.go:27] syncing external loadbalancer hostnames: api.odf-service.dif5.p1.openshiftapps.com
2022-05-24T21:26:41.953409069Z I0524 21:26:41.953374       1 request.go:665] Waited for 1.064719429s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dopenshift-kube-apiserver
2022-05-24T21:26:43.152845681Z I0524 21:26:43.152806       1 request.go:665] Waited for 1.859172511s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T21:26:44.353054229Z I0524 21:26:44.353013       1 request.go:665] Waited for 1.99669717s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-138-197.ec2.internal
2022-05-24T21:26:45.353251192Z I0524 21:26:45.353211       1 request.go:665] Waited for 1.797218164s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ip-10-0-138-197.ec2.internal
2022-05-24T21:26:46.552561168Z I0524 21:26:46.552520       1 request.go:665] Waited for 1.194579553s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ip-10-0-149-121.ec2.internal
2022-05-24T21:36:40.138033214Z I0524 21:36:40.137961       1 servicehostname.go:40] syncing servicenetwork hostnames: [172.30.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local openshift openshift.default openshift.default.svc openshift.default.svc.cluster.local]
2022-05-24T21:36:40.153694256Z I0524 21:36:40.153657       1 externalloadbalancer.go:27] syncing external loadbalancer hostnames: api.odf-service.dif5.p1.openshiftapps.com
2022-05-24T21:36:41.953014682Z I0524 21:36:41.952978       1 request.go:665] Waited for 1.063492617s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dopenshift-kube-apiserver
2022-05-24T21:36:42.953635734Z I0524 21:36:42.953601       1 request.go:665] Waited for 1.659676712s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2022-05-24T21:36:43.953718085Z I0524 21:36:43.953682       1 request.go:665] Waited for 1.99422905s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dopenshift-kube-apiserver
2022-05-24T21:36:44.953824002Z I0524 21:36:44.953783       1 request.go:665] Waited for 1.795159614s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T21:46:40.138711696Z I0524 21:46:40.138645       1 servicehostname.go:40] syncing servicenetwork hostnames: [172.30.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local openshift openshift.default openshift.default.svc openshift.default.svc.cluster.local]
2022-05-24T21:46:40.154152805Z I0524 21:46:40.154119       1 externalloadbalancer.go:27] syncing external loadbalancer hostnames: api.odf-service.dif5.p1.openshiftapps.com
2022-05-24T21:46:41.947400374Z I0524 21:46:41.947364       1 request.go:665] Waited for 1.05715737s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-149-121.ec2.internal
2022-05-24T21:46:43.147321649Z I0524 21:46:43.147287       1 request.go:665] Waited for 1.85278076s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T21:46:44.347355004Z I0524 21:46:44.347311       1 request.go:665] Waited for 1.995824066s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dopenshift-kube-apiserver
2022-05-24T21:46:45.546719841Z I0524 21:46:45.546680       1 request.go:665] Waited for 1.795379179s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ip-10-0-169-205.ec2.internal
2022-05-24T21:56:40.139345421Z I0524 21:56:40.139257       1 servicehostname.go:40] syncing servicenetwork hostnames: [172.30.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local openshift openshift.default openshift.default.svc openshift.default.svc.cluster.local]
2022-05-24T21:56:40.154700913Z I0524 21:56:40.154634       1 externalloadbalancer.go:27] syncing external loadbalancer hostnames: api.odf-service.dif5.p1.openshiftapps.com
2022-05-24T21:56:42.055763383Z I0524 21:56:42.055726       1 request.go:665] Waited for 1.165328292s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dopenshift-kube-apiserver
2022-05-24T21:56:43.055958144Z I0524 21:56:43.055922       1 request.go:665] Waited for 1.761731256s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T21:56:44.255062836Z I0524 21:56:44.255021       1 request.go:665] Waited for 1.994698683s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-138-197.ec2.internal
2022-05-24T21:56:45.255968240Z I0524 21:56:45.255928       1 request.go:665] Waited for 1.798119605s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ip-10-0-138-197.ec2.internal
2022-05-24T22:06:40.139499315Z I0524 22:06:40.139415       1 servicehostname.go:40] syncing servicenetwork hostnames: [172.30.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local openshift openshift.default openshift.default.svc openshift.default.svc.cluster.local]
2022-05-24T22:06:40.155345318Z I0524 22:06:40.155289       1 externalloadbalancer.go:27] syncing external loadbalancer hostnames: api.odf-service.dif5.p1.openshiftapps.com
2022-05-24T22:06:41.944069506Z I0524 22:06:41.944038       1 request.go:665] Waited for 1.052738057s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-149-121.ec2.internal
2022-05-24T22:06:43.144334959Z I0524 22:06:43.144294       1 request.go:665] Waited for 1.848961636s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T22:06:44.344408081Z I0524 22:06:44.344373       1 request.go:665] Waited for 1.797464072s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver
2022-05-24T22:06:45.543578892Z I0524 22:06:45.543539       1 request.go:665] Waited for 1.596524357s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-169-205.ec2.internal
2022-05-24T22:16:40.140543293Z I0524 22:16:40.140472       1 servicehostname.go:40] syncing servicenetwork hostnames: [172.30.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local openshift openshift.default openshift.default.svc openshift.default.svc.cluster.local]
2022-05-24T22:16:40.155388826Z I0524 22:16:40.155353       1 externalloadbalancer.go:27] syncing external loadbalancer hostnames: api.odf-service.dif5.p1.openshiftapps.com
2022-05-24T22:16:41.951986640Z I0524 22:16:41.951945       1 request.go:665] Waited for 1.060064223s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-149-121.ec2.internal
2022-05-24T22:16:43.152048895Z I0524 22:16:43.152009       1 request.go:665] Waited for 1.849746711s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T22:16:44.351481245Z I0524 22:16:44.351439       1 request.go:665] Waited for 1.796600059s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver
2022-05-24T22:16:45.352357313Z I0524 22:16:45.352310       1 request.go:665] Waited for 1.597411723s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-169-205.ec2.internal
2022-05-24T22:26:40.141104529Z I0524 22:26:40.141031       1 servicehostname.go:40] syncing servicenetwork hostnames: [172.30.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local openshift openshift.default openshift.default.svc openshift.default.svc.cluster.local]
2022-05-24T22:26:40.155786020Z I0524 22:26:40.155398       1 externalloadbalancer.go:27] syncing external loadbalancer hostnames: api.odf-service.dif5.p1.openshiftapps.com
2022-05-24T22:26:41.954042072Z I0524 22:26:41.954008       1 request.go:665] Waited for 1.061420005s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-149-121.ec2.internal
2022-05-24T22:26:43.153803830Z I0524 22:26:43.153751       1 request.go:665] Waited for 1.856893803s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T22:26:44.154172895Z I0524 22:26:44.154138       1 request.go:665] Waited for 1.793074728s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dopenshift-kube-apiserver
2022-05-24T22:26:45.353212978Z I0524 22:26:45.353160       1 request.go:665] Waited for 1.59658016s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-169-205.ec2.internal
2022-05-24T22:36:40.142247175Z I0524 22:36:40.142124       1 servicehostname.go:40] syncing servicenetwork hostnames: [172.30.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local openshift openshift.default openshift.default.svc openshift.default.svc.cluster.local]
2022-05-24T22:36:40.155668711Z I0524 22:36:40.155631       1 externalloadbalancer.go:27] syncing external loadbalancer hostnames: api.odf-service.dif5.p1.openshiftapps.com
2022-05-24T22:36:41.955611903Z I0524 22:36:41.955569       1 request.go:665] Waited for 1.061903018s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-149-121.ec2.internal
2022-05-24T22:36:43.154836911Z I0524 22:36:43.154802       1 request.go:665] Waited for 1.85696337s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2022-05-24T22:36:44.155415238Z I0524 22:36:44.155382       1 request.go:665] Waited for 1.995562942s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-138-197.ec2.internal
2022-05-24T22:36:45.355537162Z I0524 22:36:45.355495       1 request.go:665] Waited for 1.794678017s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ip-10-0-149-121.ec2.internal
2022-05-24T22:36:50.155329279Z I0524 22:36:50.155294       1 request.go:665] Waited for 1.100955793s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver
2022-05-24T22:46:40.142504812Z I0524 22:46:40.142435       1 servicehostname.go:40] syncing servicenetwork hostnames: [172.30.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local openshift openshift.default openshift.default.svc openshift.default.svc.cluster.local]
2022-05-24T22:46:40.155832506Z I0524 22:46:40.155797       1 externalloadbalancer.go:27] syncing external loadbalancer hostnames: api.odf-service.dif5.p1.openshiftapps.com
2022-05-24T22:46:41.947845433Z I0524 22:46:41.947812       1 request.go:665] Waited for 1.053657753s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dopenshift-kube-apiserver
2022-05-24T22:46:43.147713596Z I0524 22:46:43.147675       1 request.go:665] Waited for 1.84933531s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T22:46:44.347088518Z I0524 22:46:44.347048       1 request.go:665] Waited for 1.796343026s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/services/apiserver
2022-05-24T22:46:45.347371084Z I0524 22:46:45.347319       1 request.go:665] Waited for 1.396201075s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-169-205.ec2.internal
2022-05-24T22:46:48.747461556Z I0524 22:46:48.747425       1 request.go:665] Waited for 1.194985279s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T22:46:49.747477320Z I0524 22:46:49.747429       1 request.go:665] Waited for 1.195502987s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-149-121.ec2.internal
2022-05-24T22:56:40.143367662Z I0524 22:56:40.143296       1 servicehostname.go:40] syncing servicenetwork hostnames: [172.30.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local openshift openshift.default openshift.default.svc openshift.default.svc.cluster.local]
2022-05-24T22:56:40.155928525Z I0524 22:56:40.155887       1 externalloadbalancer.go:27] syncing external loadbalancer hostnames: api.odf-service.dif5.p1.openshiftapps.com
2022-05-24T22:56:41.954842634Z I0524 22:56:41.954806       1 request.go:665] Waited for 1.055689326s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-149-121.ec2.internal
2022-05-24T22:56:42.955254886Z I0524 22:56:42.955209       1 request.go:665] Waited for 1.656777863s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2022-05-24T22:56:43.955503791Z I0524 22:56:43.955460       1 request.go:665] Waited for 1.794589175s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dopenshift-kube-apiserver
2022-05-24T22:56:45.155369315Z I0524 22:56:45.155311       1 request.go:665] Waited for 1.795868025s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ip-10-0-169-205.ec2.internal
2022-05-24T22:56:47.155131996Z I0524 22:56:47.155090       1 request.go:665] Waited for 1.068352501s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2022-05-24T22:56:48.354869698Z I0524 22:56:48.354831       1 request.go:665] Waited for 1.395527373s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/client-ca
2022-05-24T23:06:40.143646787Z I0524 23:06:40.143578       1 servicehostname.go:40] syncing servicenetwork hostnames: [172.30.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local openshift openshift.default openshift.default.svc openshift.default.svc.cluster.local]
2022-05-24T23:06:40.156994662Z I0524 23:06:40.156952       1 externalloadbalancer.go:27] syncing external loadbalancer hostnames: api.odf-service.dif5.p1.openshiftapps.com
2022-05-24T23:06:41.951462237Z I0524 23:06:41.951425       1 request.go:665] Waited for 1.05578728s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-149-121.ec2.internal
2022-05-24T23:06:42.951670871Z I0524 23:06:42.951632       1 request.go:665] Waited for 1.652422576s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2022-05-24T23:06:44.151571938Z I0524 23:06:44.151539       1 request.go:665] Waited for 1.797001611s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-138-197.ec2.internal
2022-05-24T23:06:45.351113620Z I0524 23:06:45.351073       1 request.go:665] Waited for 1.594820857s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-169-205.ec2.internal
2022-05-24T23:06:46.351686860Z I0524 23:06:46.351649       1 request.go:665] Waited for 1.397054613s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T23:16:40.143901581Z I0524 23:16:40.143832       1 servicehostname.go:40] syncing servicenetwork hostnames: [172.30.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local openshift openshift.default openshift.default.svc openshift.default.svc.cluster.local]
2022-05-24T23:16:40.158914319Z I0524 23:16:40.158868       1 externalloadbalancer.go:27] syncing external loadbalancer hostnames: api.odf-service.dif5.p1.openshiftapps.com
2022-05-24T23:16:41.956164551Z I0524 23:16:41.956128       1 request.go:665] Waited for 1.059512634s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-149-121.ec2.internal
2022-05-24T23:16:43.155640807Z I0524 23:16:43.155585       1 request.go:665] Waited for 1.855775154s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T23:16:44.156040772Z I0524 23:16:44.156008       1 request.go:665] Waited for 1.796136273s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-138-197.ec2.internal
2022-05-24T23:16:45.355169072Z I0524 23:16:45.355131       1 request.go:665] Waited for 1.596333066s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-169-205.ec2.internal
2022-05-24T23:16:46.355430117Z I0524 23:16:46.355392       1 request.go:665] Waited for 1.197439478s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-guard-ip-10-0-149-121.ec2.internal
2022-05-24T23:16:48.164294478Z I0524 23:16:48.164252       1 request.go:665] Waited for 1.004150101s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-pod
2022-05-24T23:26:40.144433297Z I0524 23:26:40.144355       1 servicehostname.go:40] syncing servicenetwork hostnames: [172.30.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local openshift openshift.default openshift.default.svc openshift.default.svc.cluster.local]
2022-05-24T23:26:40.159168228Z I0524 23:26:40.159131       1 externalloadbalancer.go:27] syncing external loadbalancer hostnames: api.odf-service.dif5.p1.openshiftapps.com
2022-05-24T23:26:41.958871881Z I0524 23:26:41.958820       1 request.go:665] Waited for 1.062154421s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-149-121.ec2.internal
2022-05-24T23:26:42.959208705Z I0524 23:26:42.959156       1 request.go:665] Waited for 1.658274533s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/secrets/node-kubeconfigs
2022-05-24T23:26:43.959238174Z I0524 23:26:43.959203       1 request.go:665] Waited for 1.794827699s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dinstaller
2022-05-24T23:26:45.159627551Z I0524 23:26:45.159596       1 request.go:665] Waited for 1.792085119s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/config
2022-05-24T23:36:40.145299726Z I0524 23:36:40.145233       1 servicehostname.go:40] syncing servicenetwork hostnames: [172.30.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local openshift openshift.default openshift.default.svc openshift.default.svc.cluster.local]
2022-05-24T23:36:40.159229777Z I0524 23:36:40.159197       1 externalloadbalancer.go:27] syncing external loadbalancer hostnames: api.odf-service.dif5.p1.openshiftapps.com
2022-05-24T23:36:41.442748818Z I0524 23:36:41.442705       1 request.go:665] Waited for 1.185699599s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/kube-apiserver-audit-policies
2022-05-24T23:36:42.443278057Z I0524 23:36:42.443243       1 request.go:665] Waited for 1.545901962s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-149-121.ec2.internal
2022-05-24T23:36:43.643116298Z I0524 23:36:43.643081       1 request.go:665] Waited for 1.996172592s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/configmaps/trusted-ca-bundle
2022-05-24T23:36:44.843061174Z I0524 23:36:44.843022       1 request.go:665] Waited for 1.995890823s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-138-197.ec2.internal
2022-05-24T23:36:45.843339092Z I0524 23:36:45.843304       1 request.go:665] Waited for 1.764429896s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/serviceaccounts/localhost-recovery-client
